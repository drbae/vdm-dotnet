//////////////////////////////////////////////////////////////////////////////
//
// Copyright (c) 2019, National Instruments Corp.

// Permission is hereby granted, free of charge, to any person obtaining
// a copy of this software and associated documentation files (the
// "Software"), to deal in the Software without restriction, including
// without limitation the rights to use, copy, modify, merge, publish,
// distribute, sublicense, and/or sell copies of the Software, and to
// permit persons to whom the Software is furnished to do so, subject to
// the following conditions:

// The above copyright notice and this permission notice shall be included
// in all copies or substantial portions of the Software.

// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
//
//////////////////////////////////////////////////////////////////////////////
using System;
using System.Collections.Generic;
using System.Collections.ObjectModel;
using System.Runtime.InteropServices;
using NationalInstruments.Vision.Analysis.Internal;
using NationalInstruments.Vision.Internal;
using NationalInstruments.Vision.WindowsForms;
using System.Diagnostics;
using System.ComponentModel;
using System.Security;
using System.Security.Permissions;
namespace NationalInstruments.Vision.Analysis
{

    /// <summary>
    /// Summary description for Algorithms.
    /// </summary>
    public static class Algorithms
    {
        #region Arithmetic functions
        //==========================================================================================
        /// <summary>
        /// Subtracts one image from another and returns the absolute value of the difference.
        /// </summary>
        /// <param name="sourceA">
        /// The first input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image. The destination image can be one of the source images.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Sgl, and Rgb32. The image type of <format type="italics">sourceB</format> depends on the image type of <format type="italics">sourceA</format>, as follows:
        /// <list type="bullet">
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is U8, <format type="italics">sourceB</format> must be U8, I16, Sgl, or Rgb.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is I16 or Sgl, <format type="italics">sourceB</format> must be U8, I16, or Sgl.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is Rgb, <format type="italics">sourceB</format> must be Rgb or U8.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// If <format type="italics">sourceB</format> is of an image type that contains more bits per pixel than <format type="italics">sourceA</format>, <format type="italics">sourceB</format> and <format type="italics">destination</format> must be the same type of image. Otherwise, <format type="italics">sourceA</format> and <format type="italics">destination</format> must be the same type of image.
        ///  This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Find the absolute difference between the image in i and the image
        /// ' in imageViewer1
        /// ' Do the operation inplace (store the result in i)
        /// Algorithms.AbsoluteDifference(imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // Find the absolute difference between the image in i and the image
        /// // in imageViewer1
        /// // Do the operation inplace (store the result in i)
        /// Algorithms.AbsoluteDifference(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void AbsoluteDifference(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqAbsoluteDifference(destination._image, sourceA._image, sourceB._image));
        }

        //==========================================================================================
        /// <summary>Subtracts a constant from an image and returns the absolute value of the difference.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="value">
        /// The pixel value.
        /// </param>
        /// <param name="destination">
        /// The resulting image. The destination image can be one of the source images.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Sgl, and Rgb32. The destination image and source image must be the same type of image. This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Add a constant to an image in imageViewer1.
        /// ' Store the result in i
        /// Algorithms.AbsoluteDifference(imageViewer1.Image, New PixelValue(50), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // Add a constant to an image in imageViewer1.
        /// // Store the result in i
        /// Algorithms.AbsoluteDifference(imageViewer1.Image, new PixelValue(50), i);
        /// </code>
        /// </example>

        public static void AbsoluteDifference(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqAbsoluteDifferenceConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Adds an image to an image.
        /// </summary>
        /// <param name="sourceA">
        /// The first input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image. This image can be one of the source images.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Sgl, and Rgb32. The image type of <format type="italics">sourceB</format> depends on the image type of <format type="italics">sourceA</format>, as follows:
        /// <list type="bullet">
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is U8, <format type="italics">sourceB</format> must be U8, I16, Sgl, or Rgb32.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is I16 or Sgl, <format type="italics">sourceB</format> must be U8, I16, or Sgl.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is Rgb32, <format type="italics">sourceB</format> must be Rgb32 or U8.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// If <format type="italics">sourceB</format> is of an image type that contains more bits per pixel than <format type="italics">sourceA</format>, <format type="italics">sourceB</format> and <format type="italics">destination</format> must be the same type of image. Otherwise, <format type="italics">sourceA</format> and <format type="italics">destination</format> must be the same type of image.
        ///  This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Add image i to the image in imageViewer1. 
        /// ' Do the addition inplace (store the result in i) 
        /// Algorithms.Add(imageViewer1.Image, i, i) 
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // Add image i to the image in imageViewer1. 
        /// // Do the addition inplace (store the result in i) 
        /// Algorithms.Add(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Add(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqAdd(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Adds a constant to an image.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="value">
        /// The pixel value to add to the image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Sgl, and Rgb32. The destination image and source image must be the same type of image. This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// ' Add a constant to an image in imageViewer1. 
        /// ' Store the result in i 
        /// Algorithms.Add(imageViewer1.Image, New PixelValue(50), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// // Add a constant to an image in imageViewer1. 
        /// // Store the result in i 
        /// Algorithms.Add(imageViewer1.Image, new PixelValue(50), i);
        /// </code>
        /// </example>

        public static void Add(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqAddConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the average of two source images.
        /// </summary>
        /// <param name="sourceA">
        /// The first input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image. It can be one of the source images.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, and Sgl. If <format type="italics">sourceB</format> is an image type that contains more bits per pixel than <format type="italics">sourceA</format>, <format type="italics">sourceB</format> and <format type="italics">destination</format> must be the same type of image. Otherwise <format type="italics">sourceA</format> and <format type="italics">destination</format> must be the same type of image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Average a constant and an image in Viewer1.
        /// ' Store the result in i
        /// Algorithms.Average (imageViewer1.Image, 50, i)
        ///  
        /// ' Average image i and the image in Viewer1.
        /// ' Do the averaging inplace (store the result in i)
        /// Algorithms.Average (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// // Average a constant and an image in Viewer1.
        /// // Store the result in i
        /// Algorithms.Average(imageViewer1.Image, 50, i);
        ///     
        /// // Average image i and the image in Viewer1.
        /// // Do the averaging inplace (store the result in i)
        /// Algorithms.Average(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Average(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqAverage(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the average of an image and a constant.
        /// </summary>
        /// <param name="source">
        /// The first input image.
        /// </param>
        /// <param name="value">
        /// The constant value. Constants are rounded down if <format type="italics">source</format> is encoded as an integer.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, and Sgl.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Average a constant and an image in Viewer1.
        /// ' Store the result in i
        /// Algorithms.Average (imageViewer1.Image, 50, i)
        ///  
        /// ' Average image i and the image in Viewer1.
        /// ' Do the averaging inplace (store the result in i)
        /// Algorithms.Average (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// // Average a constant and an image in Viewer1.
        /// // Store the result in i
        /// Algorithms.Average(imageViewer1.Image, 50, i);
        ///     
        /// // Average image i and the image in Viewer1.
        /// // Do the averaging inplace (store the result in i)
        /// Algorithms.Average(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Average(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqAverageConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// </summary>
        /// <param name="sourceA">The first input image.
        /// </param>
        /// <param name="sourceB">The second input image.
        /// </param>
        /// <param name="destination">The resulting image. It can be one of the source images.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, and Single. The image type of <format type="italics">sourceB</format> depends on the image type of <format type="italics">sourceA</format>, as follows:
        /// <list type="bullet">
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is a U8 image, <format type="italics">sourceB</format> must be a U8 image.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is an I16 image, <format type="italics">sourceB</format> must be a U8 or I16 image.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is an Single image, <format type="italics">sourceB</format> must be a U8, I16, or Single image.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// If <format type="italics">sourceB</format> is of an image type that contains more bits per pixel than <format type="italics">sourceA</format>, <format type="italics">sourceB</format> and <format type="italics">destination</format> must be the same type of image. Otherwise, <format type="italics">sourceA</format> and <format type="italics">destination</format> must be the same type of image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Divide the image in Viewer1 by a constant
        /// ' Store the result in i
        /// Algorithms.Divide (imageViewer1.Image, New PixelValue(50), i)
        ///  
        /// ' Divide the image in Viewer1 by the image in i
        /// ' Do the division inplace (store the result in i)
        /// Algorithms.Divide (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// // Divide the image in Viewer1 by a constant
        /// // Store the result in i
        /// Algorithms.Divide(imageViewer1.Image, new PixelValue(50), i);
        ///     
        /// // Divide the image in Viewer1 by the image in i
        /// // Do the division inplace (store the result in i)
        /// Algorithms.Divide(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Divide(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqDivide2(destination._image, sourceA._image, sourceB._image, CVI_RoundingMode.Optimize));
        }
        //==========================================================================================
        /// <summary>
        /// Divides an image by an image or a constant.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="value">
        /// The pixel value to divide the image by.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">source</format> and <format type="italics">destination</format> must be the same type of image. You cannot divide an image by zero. If you set <format type="italics">value</format> to 0, the method automatically replaces it with 1.
        /// </remarks>

        public static void Divide(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqDivideConstant2(destination._image, source._image, value.CVI_PixValue, CVI_RoundingMode.Optimize));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the maximum of the pixel values of two images and copies the larger pixel value into the 
        /// destination pixel.
        /// </summary>
        /// <param name="sourceA">
        /// The first source image. 
        /// </param>
        /// <param name="sourceB">
        /// The second source image, which must be the same type of image as <format type="italics">sourceA</format>.
        /// </param>
        /// <param name="destination">
        /// The destination image. 
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. 
        /// <para>
        /// This method copies the larger pixel value of the two 
        /// source pixels into the destination for each pixel. 
        /// </para>
        /// 	<para>
        /// If <format type="italics">sourceB</format> is of a type that contains more bits per 
        /// pixel than <format type="italics">sourceA</format>, <format type="italics">sourceB</format> and 
        /// <format type="italics">destination</format> must be the same type of image. Otherwise, 
        /// <format type="italics">sourceA</format> and <format type="italics">destination</format> must be the 
        /// same type of image.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Find the max of the pixels image i and the image in Viewer1.
        /// 'Store the result in i
        /// Algorithms.Max (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Find the max of the pixels image i and the image in Viewer1.
        /// //Store the result in i
        /// Algorithms.Max(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Max(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqMax(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the maximum of a pixel value and a constant and copies the larger pixel value into the 
        /// destination pixel.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="value">
        /// The pixel value to use in the computation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// <para>
        /// This method copies the source image to the destination in the following manner: If the source image pixel
        /// value is greater than the given constant, the method copies the source pixel to the destination. Otherwise, 
        /// the method copies the constant value to the destination.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Find the max of a constant and the image in Viewer1.
        /// 'Store the result in i
        /// Algorithms.Max (imageViewer1.Image, New PixelValue (50), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Find the max of a constant and the image in Viewer1.
        /// //Store the result in i
        /// Algorithms.Max(imageViewer1.Image, new PixelValue(50), i);
        /// </code>
        /// </example>

        public static void Max(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqMaxConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the minimum of the pixel values of two images and copies the smaller pixel value into the 
        /// destination pixel.
        /// </summary>
        /// <param name="sourceA">
        /// The first source image.
        /// </param>
        /// <param name="sourceB">
        /// The second source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. 
        /// <para>
        /// This method copies the smaller pixel value of the two source pixels into the destination for each pixel.
        /// </para>
        /// 	<para>
        /// If <format type="italics">sourceB</format> is of a type that contains more bits per 
        /// pixel than <format type="italics">sourceA</format>, <format type="italics">sourceB</format> and 
        /// <format type="italics">destination</format> must be the same type of image. Otherwise, 
        /// <format type="italics">sourceA</format> and <format type="italics">destination</format> must be the 
        /// same type of image.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Find the min of the pixels image i and the image in Viewer1.
        /// 'Store the result in i
        /// Algorithms.Min (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Find the min of the pixels image i and the image in Viewer1.
        /// //Store the result in i
        /// Algorithms.Min(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Min(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqMin(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the minimum between a pixel value and a constant and copies the value to the destination pixel. 
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="value">
        /// The pixel value to use in the computation.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. 
        /// <para>
        /// This method copies the source image to the destination 
        /// in the following manner: If the source image pixel value is less than the given constant, the function 
        /// copies the source pixel to the destination. Otherwise the function copies the constant value to the 
        /// destination.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Find the min of a constant and the image in Viewer1.
        /// 'Store the result in i
        /// Algorithms.Min (imageViewer1.Image, New PixelValue (50), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Find the min of a constant and the image in Viewer1.
        /// //Store the result in i
        /// Algorithms.Min(imageViewer1.Image, new PixelValue(50), i);
        /// </code>
        /// </example>

        public static void Min(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqMinConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Takes the modulo of an image by an image.
        /// </summary>
        /// <param name="sourceA">
        /// The first image to modulo divide. 
        /// </param>
        /// <param name="sourceB">
        /// The second image to modulo divide.
        /// </param>
        /// <param name="destination">
        /// THe destination image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images.
        /// <para>
        /// 		<format type="italics">sourceA</format> and <format type="italics">destination</format> must be the 
        /// same type of image. The image type of <format type="italics">sourceB</format> depends on the type 
        /// of <format type="italics">sourceA</format>, as follows:
        /// </para>
        /// 	<list type="bullet">
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is U8, <format type="italics">sourceB</format> must be U8.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is I16, <format type="italics">sourceB</format> must be I16 or U8.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is Single, <format type="italics">sourceB</format> must be Single, I16, or U8.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is Rgb32, <format type="italics">sourceB</format> must be Rgb32 or U8.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Modulo divide the image in Viewer1 by the image in i
        /// 'Store the result in i
        /// Algorithms.Modulo (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Modulo divide the image in Viewer1 by the image in i
        /// //Store the result in i
        /// Algorithms.Modulo(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Modulo(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqModulo(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Takes the modulo of an image by a constant.
        /// </summary>
        /// <param name="source">
        /// The image to be modulo divided by the scalar constant.
        /// </param>
        /// <param name="value">
        /// The pixel value to use as the divisor in the operation.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Modulo divide the image in Viewer1 by a constant
        /// 'Store the result in i
        /// Algorithms.Modulo (imageViewer1.Image, New PixelValue (50), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Modulo divide the image in Viewer1 by a constant
        /// //Store the result in i
        /// Algorithms.Modulo(imageViewer1.Image, new PixelValue(50), i);
        /// </code>
        /// </example>

        public static void Modulo(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqModuloConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Multiplies an image by an image.
        /// </summary>
        /// <param name="sourceA">
        /// The first image to multiply.
        /// </param>
        /// <param name="sourceB">
        /// The second image to multiply.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images.
        /// <para>
        /// 	The image type of <format type="italics">sourceB</format> depends on the type 
        /// of <format type="italics">sourceA</format>, as follows:
        /// </para>
        /// 	<list type="bullet">
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is U8, <format type="italics">sourceB</format> must be U8, I16, 
        /// Single, or Rgb32.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is I16 or Single, <format type="italics">sourceB</format> must be U8, 
        /// I16, or Single.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is Rgb32, <format type="italics">sourceB</format> must be Rgb32 or U8.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Multiply the image in Viewer1 by the image in i
        /// 'Store the result in i
        /// Algorithms.Multiply (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Multiply the image in Viewer1 by the image in i
        /// //Store the result in i
        /// Algorithms.Multiply(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Multiply(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqMultiply(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Multiplies each pixel in an image by a constant. 
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="value">
        /// The pixel value by which to multiply.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images.
        /// </remarks>

        public static void Multiply(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqMultiplyConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a ratio between two source images.
        /// </summary>
        /// <param name="sourceA">
        /// The first input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="value">
        /// The value by which the method multiplies the first image. The default is 255.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images. All images must be the same type.
        /// <para>
        /// The method finds the ratio by multiplying each pixel value in the first source image by a constant. 
        /// The method divides the result of the multiplication by the corresponding pixel in the second source 
        /// image. The final result is stored in the destination image.
        /// <para>
        /// You can use this method to correct a background if the background is lighter than the image. In a 
        /// background correction, the first source image is the acquired image and the second source image is 
        /// the background image.
        /// </para>
        /// 	</para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Perform a MultiplyDivide operation using the images on Viewer1 and Viewer2.
        /// 'Store the result in i
        /// Algorithms.MultiplyDivide (imageViewer1.Image, imageViewer2.Image, i, 30)
        ///  
        /// 'Perform a MultiplyDivide operation using the images on Viewer1 and i.
        /// 'Store the result in i
        /// Algorithms.MultiplyDivide (imageViewer1.Image, i, i, 10)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Perform a MultiplyDivide operation using the images on Viewer1 and Viewer2.
        /// //Store the result in i
        /// Algorithms.MultiplyDivide(imageViewer1.Image, imageViewer2.Image, i, 30);
        ///     
        /// //Perform a MultiplyDivide operation using the images on Viewer1 and i.
        /// //Store the result in i
        /// Algorithms.MultiplyDivide(imageViewer1.Image, i, i, 10);
        /// </code>
        /// </example>

        public static void MultiplyDivide(VisionImage sourceA, VisionImage sourceB, VisionImage destination, double value)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqMulDiv(destination._image, sourceA._image, sourceB._image, (float)value));
        }
        //==========================================================================================
        /// <summary>
        /// Subtracts an image from an image.
        /// </summary>
        /// <param name="sourceA">
        /// The first input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image.
        /// </param>
        /// <param name="destination">
        /// The destination image. 
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, or Rgb32 images.
        /// <para>
        /// 		<format type="italics">sourceA</format> and <format type="italics">destination</format> must of the 
        /// same type of image. The type of <format type="italics">sourceB</format> depends on the type of 
        /// <format type="italics">sourceA</format>, as follows:
        /// </para>
        /// 	<list type="bullet">
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is U8, <format type="italics">sourceB</format> must 
        /// be U8.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is I16, <format type="italics">sourceB</format> must 
        /// be I16 or U8.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is Single, <format type="italics">sourceB</format> must 
        /// be Single, I16, or U8.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// If <format type="italics">sourceA</format> is Rgb32, <format type="italics">sourceB</format> must 
        /// be Rgb32 or U8.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Subtract image i from the image in Viewer1
        /// 'Do the subtraction in-place (store the result in i)
        /// Algorithms.Subtract (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Subtract image i from the image in Viewer1
        /// //Do the subtraction in-place (store the result in i)
        /// Algorithms.Subtract(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Subtract(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqSubtract(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Subtracts each pixel in an image by a constant. 
        /// </summary>
        /// <param name="source">
        /// The image from which the function subtracts a scalar constant. 
        /// </param>
        /// <param name="value">
        /// The value to subtract from the source image pixels.
        /// </param>
        /// <param name="destination">
        /// The destination image. 
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, or Rgb32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Subtract a constant from an image in Viewer1
        /// 'Store the result in i
        /// Algorithms.Subtract (imageViewer1.Image, New PixelValue(50), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Subtract a constant from an image in Viewer1
        /// //Store the result in i
        /// Algorithms.Subtract(imageViewer1.Image, new PixelValue(50), i);
        /// </code>
        /// </example>

        public static void Subtract(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqSubtractConstant(destination._image, source._image, value.CVI_PixValue));
        }
        #endregion

        #region Caliper functions
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, AnnulusContour annulus)
        {
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            Roi roi = new Roi(new Shape[] { annulus });
            return ConcentricRake(image, roi, ConcentricRakeDirection.CounterClockwise, EdgeProcess.All, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, Roi annulus)
        {
            return ConcentricRake(image, annulus, ConcentricRakeDirection.CounterClockwise, EdgeProcess.All, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <param name="direction">Specifies the direction in which the method searches for edges along the concentric paths. You can search in either a clockwise or counterclockwise direction. The default is CounterClockwise.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, AnnulusContour annulus, ConcentricRakeDirection direction)
        {
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            Roi roi = new Roi(new Shape[] { annulus });
            return ConcentricRake(image, roi, direction, EdgeProcess.All, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <param name="direction">Specifies the direction in which the method searches for edges along the concentric paths. You can search in either a clockwise or counterclockwise direction. The default is CounterClockwise.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, Roi annulus, ConcentricRakeDirection direction)
        {
            return ConcentricRake(image, annulus, direction, EdgeProcess.All, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <param name="direction">Specifies the direction in which the method searches for edges along the concentric paths. You can search in either a clockwise or counterclockwise direction. The default is CounterClockwise.
        /// </param>
        /// <param name="process">Determines the type of search. The method can return the first edge, both the first and the last edge, or all edges found along each concentric path. The default is All.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, AnnulusContour annulus, ConcentricRakeDirection direction, EdgeProcess process)
        {
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            Roi roi = new Roi(new Shape[] { annulus });
            return ConcentricRake(image, roi, direction, process, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <param name="direction">Specifies the direction in which the method searches for edges along the concentric paths. You can search in either a clockwise or counterclockwise direction. The default is CounterClockwise.
        /// </param>
        /// <param name="process">Determines the type of search. The method can return the first edge, both the first and the last edge, or all edges found along each concentric path. The default is All.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, Roi annulus, ConcentricRakeDirection direction, EdgeProcess process)
        {
            return ConcentricRake(image, annulus, direction, process, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <param name="direction">Specifies the direction in which the method searches for edges along the concentric paths. You can search in either a clockwise or counterclockwise direction. The default is CounterClockwise.
        /// </param>
        /// <param name="process">Determines the type of search. The method can return the first edge, both the first and the last edge, or all edges found along each concentric path. The default is All.
        /// </param>
        /// <param name="stepSize">The radial distance in pixels between the concentric paths. The default is 5.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, AnnulusContour annulus, ConcentricRakeDirection direction, EdgeProcess process, Int32 stepSize)
        {
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            Roi roi = new Roi(new Shape[] { annulus });
            return ConcentricRake(image, roi, direction, process, stepSize, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <param name="direction">Specifies the direction in which the method searches for edges along the concentric paths. You can search in either a clockwise or counterclockwise direction. The default is CounterClockwise.
        /// </param>
        /// <param name="process">Determines the type of search. The method can return the first edge, both the first and the last edge, or all edges found along each concentric path. The default is All.
        /// </param>
        /// <param name="stepSize">The radial distance in pixels between the concentric paths. The default is 5.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, Roi annulus, ConcentricRakeDirection direction, EdgeProcess process, Int32 stepSize)
        {
            return ConcentricRake(image, annulus, direction, process, stepSize, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <param name="direction">Specifies the direction in which the method searches for edges along the concentric paths. You can search in either a clockwise or counterclockwise direction. The default is CounterClockwise.
        /// </param>
        /// <param name="process">Determines the type of search. The method can return the first edge, both the first and the last edge, or all edges found along each concentric path. The default is All.
        /// </param>
        /// <param name="stepSize">The radial distance in pixels between the concentric paths. The default is 5.
        /// </param>
        /// <param name="edgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel analysis of the edges.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This example assumes that you have an annulus selected on the viewer.
        /// 'Only look for the first rising edge.
        /// Dim EdgeOptions As New EdgeOptions(EdgePolaritySearchMode.Rising)
        /// Dim Report As ConcentricRakeReport
        /// Report = Algorithms.ConcentricRake(imageViewer1.Image, imageViewer1.Roi, ConcentricRakeDirection.Clockwise, EdgeProcess.First, 5, EdgeOptions)
        /// 'Overlay the search arcs
        /// For Each arcInfo As SearchArcInfo In Report.SearchArcs
        ///     imageViewer1.Image.Overlays.[Default].AddArc(arcInfo.Arc)
        /// Next
        /// 'Overlay the points found on the image.
        /// For Each EdgeInfo As EdgeInfo In Report.FirstEdges
        ///     imageViewer1.Image.Overlays.[Default].AddOval(New OvalContour(EdgeInfo.Position.X - 2, EdgeInfo.Position.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.PaintValue)
        /// Next
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //This example assumes that you have an annulus selected on the viewer.
        /// //Only look for the first rising edge.
        /// EdgeOptions edgeOptions = new EdgeOptions(EdgePolaritySearchMode.Rising);
        /// ConcentricRakeReport report;
        /// report = Algorithms.ConcentricRake(imageViewer1.Image, imageViewer1.Roi, ConcentricRakeDirection.Clockwise, EdgeProcess.First, 5, edgeOptions)
        /// //Overlay the search arcs
        /// foreach (SearchArcInfo arcInfo in report.SearchArcs) {
        ///     imageViewer1.Image.Overlays.Default.AddArc(arcInfo.Arc)
        /// }
        /// //Overlay the points found on the image.
        /// foreach (EdgeInfo edgeInfo in report.FirstEdges) {
        ///     imageViewer1.Image.Overlays.Default.AddOval(new OvalContour(edgeInfo.Position.X - 2, edgeInfo.Position.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.PaintValue)
        /// }
        /// </code>
        /// </example>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, AnnulusContour annulus, ConcentricRakeDirection direction, EdgeProcess process, Int32 stepSize, EdgeOptions edgeOptions)
        {
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            Roi roi = new Roi(new Shape[] { annulus });
            return ConcentricRake(image, roi, direction, process, stepSize, edgeOptions);
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along concentric circular or angular paths in the image. Edges are determined based on their contrast and slope.</summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="annulus">Describes the bounding annular region within which the concentric paths are defined.
        /// </param>
        /// <param name="direction">Specifies the direction in which the method searches for edges along the concentric paths. You can search in either a clockwise or counterclockwise direction. The default is CounterClockwise.
        /// </param>
        /// <param name="process">Determines the type of search. The method can return the first edge, both the first and the last edge, or all edges found along each concentric path. The default is All.
        /// </param>
        /// <param name="stepSize">The radial distance in pixels between the concentric paths. The default is 5.
        /// </param>
        /// <param name="edgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel analysis of the edges.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ConcentricRakeReport" crefType="Unqualified"/> object with the results of the concentric rake.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the concentric rake:
        /// <image src="conrake.gif"/>
        /// </remarks>

        public static ConcentricRakeReport ConcentricRake(VisionImage image, Roi annulus, ConcentricRakeDirection direction, EdgeProcess process, Int32 stepSize, EdgeOptions edgeOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            annulus.ThrowIfDisposed();
            if (edgeOptions == null) { throw new ArgumentNullException("edgeOptions"); }
            CVI_EdgeOptions2 cviEdgeOptions = new CVI_EdgeOptions2();
            cviEdgeOptions.ConvertFromExternal(edgeOptions);
            IntPtr report = VisionDll.imaqConcentricRake2(image._image, annulus._roi, direction, process, stepSize, ref cviEdgeOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ConcentricRakeReport, CVI_ConcentricRakeReport2>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Finds the location, amplitude, and second derivative of peaks or valleys in the input array.
        /// </summary>
        /// <param name="dataArray">
        /// The input array.
        /// </param>
        /// <param name="mode">
        /// Determines whether to search for peaks or valleys.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PeakValleyReportItem" crefType="Unqualified"/> objects about each peak or valley found. On failure, an exception is thrown.
        /// </returns>

        public static Collection<PeakValleyReportItem> DetectPeaksOrValleys(double[] dataArray, PeakOrValley mode)
        {
            return DetectPeaksOrValleys(dataArray, mode, new DetectPeaksOrValleysOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds the location, amplitude, and second derivative of peaks or valleys in the input array.
        /// </summary>
        /// <param name="dataArray">
        /// The input array.
        /// </param>
        /// <param name="mode">
        /// Determines whether to search for peaks or valleys.
        /// </param>
        /// <param name="options">Options the method uses for detecting peaks and valleys.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PeakValleyReportItem" crefType="Unqualified"/> objects about each peak or valley found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// The method is based on an algorithm that fits a quadratic polynomial to a sequential groups 
        /// of data points. Set the <see cref="NationalInstruments.Vision.Analysis.DetectPeaksOrValleysOptions.Width" crefType="Unqualified"/> 
        /// property in the <format type="italics">options</format> property to specify the number of data points to use in the fit. For each peak or valley, 
        /// the method tests the quadratic fit against the <see cref="NationalInstruments.Vision.Analysis.DetectPeaksOrValleysOptions.Threshold" crefType="Unqualified"/> 
        /// value in the <format type="italics">options</format> property. The method ignores peaks with values lower than the Threshold and valleys with 
        /// troughs higher than Threshold.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Private Sub Run_Click()
        ///     ' This example requires you to draw a region on Viewer1
        ///     ' before you execute this piece of code.
        ///     Dim ProfileReport As RoiProfileReport
        ///     Dim PeakValleyReport As Collection(Of PeakValleyReportItem)
        ///     Dim Index As Integer
        ///     ' Find the profile of the regions on Viewer1.
        ///     ProfileReport = Algorithms.RoiProfile (imageViewer1.Image, imageViewer1.Roi)
        ///     ' Extract the profile array
        ///     Dim ProfileData As Collection(Of Double) = ProfileReport.Report.ProfileData
        ///     Dim ProfileDataArray(ProfileData.Count) As Double
        ///     ProfileData.CopyTo (ProfileDataArray, 0)
        ///     ' Find the location, amplitude and, derivatives of the peaks
        ///     ' along this profile
        ///     PeakValleyReport = Algorithms.DetectPeaksOrValleys (ProfileDataArray, PeakOrValley.Peaks, New DetectPeaksOrValleysOptions (100, 3))
        ///     ' Overlay the points where peaks were found
        ///     For Each ReportItem As PeakValleyReportItem In PeakValleyReport
        ///         ' Find the index into the ProfileDataArray where the peak was found. This index corresponds
        ///         ' to the index of the points in ProfileReport.Pixels
        ///         Index = Math.Round (ReportItem.Position)
        ///         ' Draw a circular point at ProfileReport.Pixels(Index) -- the point nearest
        ///         ' to where the peak was found.
        ///         Dim Point As PointContour = ProfileReport.Pixels(Index)
        ///         imageViewer1.Image.Overlays.Default.AddOval (New OvalContour (Point.X - 2, Point.Y - 2, 5, 5), Rgb32Value.GreenColor, DrawingMode.PaintValue)
        ///     Next
        /// End Sub
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// private void run_Click(object sender, EventArgs e) {
        ///     // This example requires you to draw a region on Viewer1
        ///     // before you execute this piece of code.
        ///     RoiProfileReport profileReport;
        ///     Collection&lt;PeakValleyReportItem&gt; peakValleyReport;
        ///     // Find the profile of the regions on Viewer1.
        ///     profileReport = Algorithms.RoiProfile (imageViewer1.Image, imageViewer1.Roi);
        ///     // Extract the profile array
        ///     Collection&lt;double&gt; profileData = profileReport.Report.ProfileData;
        ///     double[] profileDataArray = new double[profileData.Count];
        ///     profileData.CopyTo (profileDataArray, 0);
        ///     // Find the location, amplitude and, derivatives of the peaks
        ///     // along this profile
        ///     peakValleyReport = Algorithms.DetectPeaksOrValleys (profileDataArray, PeakOrValley.Peaks, new DetectPeaksOrValleysOptions (100, 3));
        ///     // Overlay the points where peaks were found
        ///     foreach (PeakValleyReportItem reportItem in peakValleyReport) {
        ///         // Find the index into the profileDataArray where the peak was found. This index corresponds
        ///         // to the index of the points in profileReport.Pixels
        ///         int index = Math.Round (reportItem.Position);
        ///         // Draw a circular point at profileReport.Pixels(Index) -- the point nearest
        ///         // to where the peak was found.
        ///         PointContour point = profileReport.Pixels(Index);
        ///         imageViewer1.Image.Overlays.Default.AddOval (new OvalContour (point.X - 2, point.Y - 2, 5, 5), Rgb32Value.GreenColor, DrawingMode.PaintValue);
        ///     }
        /// }
        /// </code>
        /// </example>

        public static Collection<PeakValleyReportItem> DetectPeaksOrValleys(double[] dataArray, PeakOrValley mode, DetectPeaksOrValleysOptions options)
        {
            if (dataArray == null) { throw new ArgumentNullException("dataArray"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_DetectExtremesOptions cviOptions = new CVI_DetectExtremesOptions();
            cviOptions.ConvertFromExternal(options);
            int numExtremes;
            IntPtr report = VisionDll.imaqDetectExtremes(dataArray, dataArray.Length, mode, ref cviOptions, out numExtremes);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToCollection<PeakValleyReportItem, CVI_ExtremeReport>(report, numExtremes, true);
        }
        //==========================================================================================
        /// <summary>
        /// Detects the rotational shift between two images, usually a reference image containing a part at a known orientation and another image containing the part in an unknown orientation.
        /// </summary>
        /// <param name="reference">
        /// The reference image.
        /// </param>
        /// <param name="test">
        /// The test image.
        /// </param>
        /// <param name="referenceCenter">
        /// The center of the circle in the reference image.
        /// </param>
        /// <param name="testCenter">
        /// The center of the circle in the test image.
        /// </param>
        /// <param name="radius">
        /// The radius of the circle used to detect rotation.
        /// </param>
        /// <returns>
        /// A non-zero value that specifies the rotational shift between the reference image and the test image. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with image types U8, I16, and Single. The method extracts pixel values around a circular region in the reference image and compares these values to the same region in the test image. The algorithm looks for the rotational shift between those two samples.
        /// </remarks>

        public static double DetectRotation(VisionImage reference, VisionImage test, PointContour referenceCenter, PointContour testCenter, int radius)
        {
            return DetectRotation(reference, test, referenceCenter, testCenter, radius, 5);
        }
        //==========================================================================================
        /// <summary>
        /// Detects the rotational shift between two images, usually a reference image containing a part at a known orientation and another image containing the part in an unknown orientation.
        /// </summary>
        /// <param name="reference">
        /// The reference image.
        /// </param>
        /// <param name="test">
        /// The test image.
        /// </param>
        /// <param name="referenceCenter">
        /// The center of the circle in the reference image.
        /// </param>
        /// <param name="testCenter">
        /// The center of the circle in the test image.
        /// </param>
        /// <param name="radius">
        /// The radius of the circle used to detect rotation.
        /// </param>
        /// <param name="precision">The sampling period, in degrees, of the pixel values extracted from the circular region. The sampling period directly affects the speed of the method. If the sampling period is high (the number of samples along the circular region are few), the processing speed increases at the cost of reduced accuracy in the computed rotational shift. The method generates an error if the sampling period is less than or equal to 0.
        /// </param>
        /// <returns>
        /// A non-zero value that specifies the rotational shift between the reference image and the test image. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with image types U8, I16, and Single. The method extracts pixel values around a circular region in the reference image and compares these values to the same region in the test image. The algorithm looks for the rotational shift between those two samples.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i as New VisionImage
        /// Dim testCenter as New PointContour
        /// Dim referenceCenter as New PointContour
        /// Dim angle as Double
        ///  
        /// ' Load the test image into i
        ///  
        /// ' Set the test and reference centers 
        /// referenceCenter.Initialize (imageViewer1.Image.Width/2, imageViewer1.Image.Height/2)
        /// testCenter.Initialize (i.Width/2, i.Height/2)
        ///  
        /// ' Detect the rotational shift between the image on Viewer1 and i
        /// angle = Algorithms.DetectRotation (imageViewer1.Image, i, referenceCenter, testCenter, 10, angle)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// PointContour testCenter = new PointContour();
        /// PointContour referenceCenter = new PointContour();
        /// double angle = 0;
        ///     
        /// // Load the test image into i
        ///   
        /// // Set the test and reference centers 
        /// referenceCenter.initialize(imageViewer1.Image.width / 2, imageViewer1.Image.height / 2);
        /// testCenter.Initialize(i.width / 2, i.height / 2);
        ///     
        /// // Detect the rotational shift between the image on Viewer1 and i
        /// angle = Algorithms.DetectRotation(imageViewer1.Image, i, referenceCenter, testCenter, 10, angle);
        /// </code>
        /// </example>

        public static double DetectRotation(VisionImage reference, VisionImage test, PointContour referenceCenter, PointContour testCenter, int radius, double precision)
        {
            if (reference == null) { throw new ArgumentNullException("reference"); }
            reference.ThrowIfDisposed();
            if (test == null) { throw new ArgumentNullException("test"); }
            test.ThrowIfDisposed();
            if (referenceCenter == null) { throw new ArgumentNullException("referenceCenter"); }
            if (testCenter == null) { throw new ArgumentNullException("testCenter"); }
            CVI_PointFloat cviReferenceCenter = new CVI_PointFloat();
            cviReferenceCenter.ConvertFromExternal(referenceCenter);
            CVI_PointFloat cviTestCenter = new CVI_PointFloat();
            cviTestCenter.ConvertFromExternal(testCenter);
            double angle;
            Utilities.ThrowError(VisionDll.imaqDetectRotation(reference._image, test._image, cviReferenceCenter, cviTestCenter, radius, (float)precision, out angle));
            return angle;
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along an ROI in an image.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The ROI to find edges along.
        /// </param>
        /// <returns>
        /// An EdgeReport with information about the detected edges. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static EdgeReport EdgeTool(VisionImage image, Roi roi)
        {
            return EdgeTool(image, roi, EdgeProcess.All, new EdgeOptions(), false);
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along an ROI in an image.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The ROI to find edges along.
        /// </param>
        /// <param name="process">
        /// The type of search. The method can return the first edge, both the first edge and the last edge, all edges 
        /// found along the search path, or the strongest edge found along the search path. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.EdgeProcess.All" crefType="PartiallyQualified"/>.
        /// </param>
        /// <returns>
        /// An EdgeReport with information about the detected edges. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static EdgeReport EdgeTool(VisionImage image, Roi roi, EdgeProcess process)
        {
            return EdgeTool(image, roi, process, new EdgeOptions(), false);
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along an ROI in an image.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The ROI to find edges along.
        /// </param>
        /// <param name="process">
        /// The type of search. The method can return the first edge, both the first edge and the last edge, all edges 
        /// found along the search path, or the strongest edge found along the search path. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.EdgeProcess.All" crefType="PartiallyQualified"/>.
        /// </param>
        /// <param name="options">
        /// The parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <returns>
        /// An EdgeReport with information about the detected edges. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static EdgeReport EdgeTool(VisionImage image, Roi roi, EdgeProcess process, EdgeOptions options)
        {
            return EdgeTool(image, roi, process, options, false);
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along an ROI in an image.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The ROI to find edges along.
        /// </param>
        /// <param name="process">
        /// The type of search. The method can return the first edge, both the first edge and the last edge, all edges 
        /// found along the search path, or the strongest edge found along the search path. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.EdgeProcess.All" crefType="PartiallyQualified"/>.
        /// </param>
        /// <param name="options">
        /// The parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <param name="reverseDirection">
        /// Set to True to reverse the direction that the ROI traverses to find edges. The default is False.
        /// </param>
        /// <returns>
        /// An EdgeReport with information about the detected edges. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Find the edges along the image in imageViewer1, using the ROI in imageViewer1.
        /// Dim Report As EdgeReport = Algorithms.EdgeTool (imageViewer1.Image, imageViewer1.Roi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Find the edges along the image in imageViewer1, using the ROI in imageViewer1.
        /// EdgeReport report = Algorithms.EdgeTool(imageViewer1.Image, imageViewer1.Roi);
        /// </code>
        /// </example>

        public static EdgeReport EdgeTool(VisionImage image, Roi roi, EdgeProcess process, EdgeOptions options, bool reverseDirection)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_EdgeOptions2 cviOptions = new CVI_EdgeOptions2();
            cviOptions.ConvertFromExternal(options);
            IntPtr report = VisionDll.imaqEdgeTool4(image._image, roi._roi, process, ref cviOptions, reverseDirection ? (UInt32)1 : (UInt32)0);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<EdgeReport, CVI_EdgeReport2>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Detects straight edges inside an ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <returns>
        /// A FindEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindEdgeReport FindEdge(VisionImage image, Roi roi)
        {
            return FindEdge(image, roi, new FindEdgeOptions(), new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// Detects straight edges inside an ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <param name="findEdgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel 
        /// analysis of the edges.
        /// </param>
        /// <returns>
        /// A FindEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindEdgeReport FindEdge(VisionImage image, Roi roi, FindEdgeOptions findEdgeOptions)
        {
            return FindEdge(image, roi, findEdgeOptions, new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// Detects straight edges inside an ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <param name="findEdgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel 
        /// analysis of the edges.
        /// </param>
        /// <param name="transform">
        /// Specifies how to transform the location of the edge detection based on the difference between the reference 
        /// coordinate system and the measurement coordinate system.
        /// </param>
        /// <returns>
        /// A FindEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// ' Find the edges in the image in Viewer1 in a particular rectangle.
        /// Dim Report As FindEdgeReport = Algorithms.FindEdge(imageViewer1.Image, New Roi(New Shape() {New RectangleContour(10, 0, 80, 80)})
        ///  
        /// ' Overlay the found lines on the image.
        /// For Each Edge As StraightEdgeReportItem In Report.StraightEdges
        ///     imageViewer1.Image.Overlays.Default.AddLine(Edge.StraightEdge)
        /// Next
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Find the edges in the image in Viewer1 in a particular rectangle.
        /// FindEdgeReport report = Algorithms.FindEdge(imageViewer1.Image, new Roi(new Shape[] {new RectangleContour(10, 0, 80, 80)});
        ///  
        /// // Overlay the found lines on the image.
        /// foreach (StraightEdgeReportItem edge in report.StraightEdges) {
        ///     imageViewer1.Image.Overlays.Default.AddLine(edge.StraightEdge)
        /// }
        /// </code>
        /// </example>

        public static FindEdgeReport FindEdge(VisionImage image, Roi roi, FindEdgeOptions findEdgeOptions, CoordinateTransform transform)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (findEdgeOptions == null) { throw new ArgumentNullException("findEdgeOptions"); }
            if (findEdgeOptions.EdgeOptions == null) { throw new ArgumentNullException("findEdgeOptions"); }
            if (findEdgeOptions.StraightEdgeOptions == null) { throw new ArgumentNullException("findEdgeOptions"); }
            if (transform == null) { throw new ArgumentNullException("transform"); }
            CVI_FindEdgeOptions2 cviFindEdgeOptions = new CVI_FindEdgeOptions2();
            cviFindEdgeOptions.ConvertFromExternal(findEdgeOptions);
            CVI_StraightEdgeOptions cviStraightEdgeOptions = new CVI_StraightEdgeOptions();
            cviStraightEdgeOptions.ConvertFromExternal(findEdgeOptions.StraightEdgeOptions);
            CVI_CoordinateSystem cviBaseSystem = new CVI_CoordinateSystem();
            cviBaseSystem.ConvertFromExternal(transform.ReferenceSystem);
            CVI_CoordinateSystem cviNewSystem = new CVI_CoordinateSystem();
            cviNewSystem.ConvertFromExternal(transform.MeasurementSystem);
            IntPtr report = VisionDll.imaqFindEdge2(image._image, roi._roi, ref cviBaseSystem, ref cviNewSystem, ref cviFindEdgeOptions, ref cviStraightEdgeOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<FindEdgeReport, CVI_FindEdgeReport>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Computes a coordinate transform based on the position of an object in a search area of an image. The method 
        /// uses the location and orientation of the coordinate system it finds to create the reference system of a 
        /// coordinate transform or to update the measurement system of an existing coordinate transform.
        /// </summary>
        /// <param name="image">
        /// The image which the method uses to compute the coordinate transform. 
        /// </param>
        /// <param name="roi">
        /// Defines the area within which the edge detection is performed.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FindTransformReport" crefType="Unqualified"/> object containing the 
        /// axes that were located.
        /// </returns>
        /// <remarks>
        /// This method uses the following algorithm. First the method determines the position of the main axis of the 
        /// coordinate system. It locates the intersection points between a set of parallel search lines, or rake, 
        /// and the edge of an object. The method determines the intersection points based on their contrast, width, and 
        /// steepness. The method calculates a best-fit line using the points found. This line defines the main axis of the 
        /// coordinate system. The method then locates the intersection points between a set of parallel search lines that 
        /// are perpendicular to the main axis and the edge of the object. It calculates a hit-line to the object from the 
        /// edge closest to the search area detected and perpendicular to the main axis. This line defines the secondary axis 
        /// of the coordinate system. The intersection between the main axis and secondary axis is the origin of the 
        /// coordinate system.
        /// <para>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </para>
        /// </remarks>

        public static FindTransformReport FindTransformRectangle(VisionImage image, Roi roi)
        {
            return FindTransformRectangle(image, roi, FindTransformMode.FindReference, new CoordinateTransform(), new FindTransformRectOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Computes a coordinate transform based on the position of an object in a search area of an image. The method 
        /// uses the location and orientation of the coordinate system it finds to create the reference system of a 
        /// coordinate transform or to update the measurement system of an existing coordinate transform.
        /// </summary>
        /// <param name="image">
        /// The image which the method uses to compute the coordinate transform. 
        /// </param>
        /// <param name="roi">
        /// Defines the area within which the edge detection is performed.
        /// </param>
        /// <param name="mode">
        /// Specifies how the method updates the coordinate transform. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.FindTransformMode.FindReference" crefType="PartiallyQualified"/>
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FindTransformReport" crefType="Unqualified"/> object containing the 
        /// axes that were located.
        /// </returns>
        /// <remarks>
        /// This method uses the following algorithm. First the method determines the position of the main axis of the 
        /// coordinate system. It locates the intersection points between a set of parallel search lines, or rake, 
        /// and the edge of an object. The method determines the intersection points based on their contrast, width, and 
        /// steepness. The method calculates a best-fit line using the points found. This line defines the main axis of the 
        /// coordinate system. The method then locates the intersection points between a set of parallel search lines that 
        /// are perpendicular to the main axis and the edge of the object. It calculates a hit-line to the object from the 
        /// edge closest to the search area detected and perpendicular to the main axis. This line defines the secondary axis 
        /// of the coordinate system. The intersection between the main axis and secondary axis is the origin of the 
        /// coordinate system. 
        /// <para>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </para>
        /// </remarks>

        public static FindTransformReport FindTransformRectangle(VisionImage image, Roi roi, FindTransformMode mode)
        {
            return FindTransformRectangle(image, roi, mode, new CoordinateTransform(), new FindTransformRectOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Computes a coordinate transform based on the position of an object in a search area of an image. The method 
        /// uses the location and orientation of the coordinate system it finds to create the reference system of a 
        /// coordinate transform or to update the measurement system of an existing coordinate transform.
        /// </summary>
        /// <param name="image">
        /// The image which the method uses to compute the coordinate transform. 
        /// </param>
        /// <param name="roi">
        /// Defines the area within which the edge detection is performed.
        /// </param>
        /// <param name="mode">
        /// Specifies how the method updates the coordinate transform. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.FindTransformMode.FindReference" crefType="PartiallyQualified"/>
        /// </param>
        /// <param name="transform">
        /// Specifies how to transform pixel coordinates based on the difference between the reference coordinate system 
        /// and the measurement coordinate system.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FindTransformReport" crefType="Unqualified"/> object containing the 
        /// axes that were located.
        /// </returns>
        /// <remarks>
        /// This method uses the following algorithm. First the method determines the position of the main axis of the 
        /// coordinate system. It locates the intersection points between a set of parallel search lines, or rake, 
        /// and the edge of an object. The method determines the intersection points based on their contrast, width, and 
        /// steepness. The method calculates a best-fit line using the points found. This line defines the main axis of the 
        /// coordinate system. The method then locates the intersection points between a set of parallel search lines that 
        /// are perpendicular to the main axis and the edge of the object. It calculates a hit-line to the object from the 
        /// edge closest to the search area detected and perpendicular to the main axis. This line defines the secondary axis 
        /// of the coordinate system. The intersection between the main axis and secondary axis is the origin of the 
        /// coordinate system.
        /// <para>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </para>
        /// </remarks>

        public static FindTransformReport FindTransformRectangle(VisionImage image, Roi roi, FindTransformMode mode, CoordinateTransform transform)
        {
            return FindTransformRectangle(image, roi, mode, transform, new FindTransformRectOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Computes a coordinate transform based on the position of an object in a search area of an image. The method 
        /// uses the location and orientation of the coordinate system it finds to create the reference system of a 
        /// coordinate transform or to update the measurement system of an existing coordinate transform.
        /// </summary>
        /// <param name="image">
        /// The image which the method uses to compute the coordinate transform. 
        /// </param>
        /// <param name="roi">
        /// Defines the area within which the edge detection is performed.
        /// </param>
        /// <param name="mode">
        /// Specifies how the method updates the coordinate transform. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.FindTransformMode.FindReference" crefType="PartiallyQualified"/>
        /// </param>
        /// <param name="transform">
        /// Specifies how to transform pixel coordinates based on the difference between the reference coordinate system 
        /// and the measurement coordinate system.
        /// </param>
        /// <param name="findTransformRectOptions">
        /// Specifies options for detecting edges along search lines in the ROI and for overlaying search information.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FindTransformReport" crefType="Unqualified"/> object containing the 
        /// axes that were located.
        /// </returns>
        /// <remarks>
        /// This method uses the following algorithm. First the method determines the position of the main axis of the 
        /// coordinate system. It locates the intersection points between a set of parallel search lines, or rake, 
        /// and the edge of an object. The method determines the intersection points based on their contrast, width, and 
        /// steepness. The method calculates a best-fit line using the points found. This line defines the main axis of the 
        /// coordinate system. The method then locates the intersection points between a set of parallel search lines that 
        /// are perpendicular to the main axis and the edge of the object. It calculates a hit-line to the object from the 
        /// edge closest to the search area detected and perpendicular to the main axis. This line defines the secondary axis 
        /// of the coordinate system. The intersection between the main axis and secondary axis is the origin of the 
        /// coordinate system.
        /// <para>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Find the transformation in the image in Viewer1.
        /// Dim Report As FindTransformReport = Algorithms.FindTransformRectangle (imageViewer1.Image, imageViewer1.Roi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Find the transformation in the image in Viewer1.
        /// FindTransformReport report = Algorithms.FindTransformRectangle(imageViewer1.Image, imageViewer1.Roi);
        /// </code>
        /// </example>

        public static FindTransformReport FindTransformRectangle(VisionImage image, Roi roi, FindTransformMode mode, CoordinateTransform transform, FindTransformRectOptions findTransformRectOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (transform == null) { throw new ArgumentNullException("transform"); }
            if (findTransformRectOptions == null) { throw new ArgumentNullException("findTransformRectOptions"); }
            if (findTransformRectOptions.EdgeOptions == null) { throw new ArgumentNullException("findTransformRectOptions"); }
            if (findTransformRectOptions.StraightEdgeOptions == null) { throw new ArgumentNullException("findTransformRectOptions"); }
            CVI_FindTransformRectOptions2 cviFindTransformRectOptions = new CVI_FindTransformRectOptions2();
            cviFindTransformRectOptions.ConvertFromExternal(findTransformRectOptions);
            CVI_StraightEdgeOptions cviStraightEdgeOptions = new CVI_StraightEdgeOptions();
            cviStraightEdgeOptions.ConvertFromExternal(findTransformRectOptions.StraightEdgeOptions);
            CVI_CoordinateSystem cviBaseSystem = new CVI_CoordinateSystem();
            cviBaseSystem.ConvertFromExternal(transform.ReferenceSystem);
            CVI_CoordinateSystem cviNewSystem = new CVI_CoordinateSystem();
            cviNewSystem.ConvertFromExternal(transform.MeasurementSystem);
            CVI_AxisReport cviAxisReport = new CVI_AxisReport();
            Utilities.ThrowError(VisionDll.imaqFindTransformRect2(image._image, roi._roi, mode, ref cviBaseSystem, ref cviNewSystem, ref cviFindTransformRectOptions, ref cviStraightEdgeOptions, out cviAxisReport));
            // We have to construct the report ourselves.
            FindTransformReport report = new FindTransformReport();
            report.AxisReport = cviAxisReport.ConvertToExternal();
            report.Transform.ReferenceSystem = cviBaseSystem.ConvertToExternal();
            report.Transform.MeasurementSystem = cviNewSystem.ConvertToExternal();
            return report;
        }
        //==========================================================================================
        /// <summary>
        /// Computes a coordinate transform based on the position of an object in a search area of an image. The method uses 
        /// the location and orientation of the coordinate system it finds to create the reference system of a 
        /// coordinate transform or to update the measurement system of an existing coordinate transform.
        /// </summary>
        /// <param name="image">
        /// The image which the method uses to compute the coordinate transform. 
        /// </param>
        /// <param name="primaryRoi">
        /// Defines the area within which the edge detection is performed for the primary axis. 
        /// </param>
        /// <param name="secondaryRoi">
        /// Defines the area within which the edge detection is performed for the secondary axis. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FindTransformReport" crefType="Unqualified"/> object containing the 
        /// axes that were located.
        /// </returns>
        /// <remarks>
        /// This method uses the following algorithm. First the method determines the position of the main axis of the 
        /// coordinate system. It locates the intersection points between a set of parallel search lines in the primary 
        /// rectangle and the edge of an object. The method determines the intersection points based on their contrast, 
        /// width, and steepness. The method calculates a best-fit line through the points found. This line defines the 
        /// main axis of the coordinate system. The process is repeated perpendicularly in the secondary rectangle in 
        /// order to locate the secondary axis. The intersection between the main axis and the secondary axis is the origin 
        /// of the coordinate system.
        /// <para>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </para>
        /// </remarks>

        public static FindTransformReport FindTransformRectangles(VisionImage image, Roi primaryRoi, Roi secondaryRoi)
        {
            return FindTransformRectangles(image, primaryRoi, secondaryRoi, FindTransformMode.FindReference, new CoordinateTransform(), new FindTransformRectsOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Computes a coordinate transform based on the position of an object in a search area of an image. The method uses 
        /// the location and orientation of the coordinate system it finds to create the reference system of a 
        /// coordinate transform or to update the measurement system of an existing coordinate transform.
        /// </summary>
        /// <param name="image">
        /// The image which the method uses to compute the coordinate transform. 
        /// </param>
        /// <param name="primaryRoi">
        /// Defines the area within which the edge detection is performed for the primary axis. 
        /// </param>
        /// <param name="secondaryRoi">
        /// Defines the area within which the edge detection is performed for the secondary axis. 
        /// </param>
        /// <param name="mode">
        /// Specifies how the method updates the coordinate transform. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.FindTransformMode.FindReference" crefType="PartiallyQualified"/>
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FindTransformReport" crefType="Unqualified"/> object containing the 
        /// axes that were located.
        /// </returns>
        /// <remarks>
        /// This method uses the following algorithm. First the method determines the position of the main axis of the 
        /// coordinate system. It locates the intersection points between a set of parallel search lines in the primary 
        /// rectangle and the edge of an object. The method determines the intersection points based on their contrast, 
        /// width, and steepness. The method calculates a best-fit line through the points found. This line defines the 
        /// main axis of the coordinate system. The process is repeated perpendicularly in the secondary rectangle in 
        /// order to locate the secondary axis. The intersection between the main axis and the secondary axis is the origin 
        /// of the coordinate system.
        /// <para>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </para>
        /// </remarks>

        public static FindTransformReport FindTransformRectangles(VisionImage image, Roi primaryRoi, Roi secondaryRoi, FindTransformMode mode)
        {
            return FindTransformRectangles(image, primaryRoi, secondaryRoi, mode, new CoordinateTransform(), new FindTransformRectsOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Computes a coordinate transform based on the position of an object in a search area of an image. The method uses 
        /// the location and orientation of the coordinate system it finds to create the reference system of a 
        /// coordinate transform or to update the measurement system of an existing coordinate transform.
        /// </summary>
        /// <param name="image">
        /// The image which the method uses to compute the coordinate transform. 
        /// </param>
        /// <param name="primaryRoi">
        /// Defines the area within which the edge detection is performed for the primary axis. 
        /// </param>
        /// <param name="secondaryRoi">
        /// Defines the area within which the edge detection is performed for the secondary axis. 
        /// </param>
        /// <param name="mode">
        /// Specifies how the method updates the coordinate transform. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.FindTransformMode.FindReference" crefType="PartiallyQualified"/>
        /// </param>
        /// <param name="transform">
        /// Specifies how to transform pixel coordinates based on the difference between the reference coordinate system 
        /// and the measurement coordinate system.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FindTransformReport" crefType="Unqualified"/> object containing the 
        /// axes that were located.
        /// </returns>
        /// <remarks>
        /// This method uses the following algorithm. First the method determines the position of the main axis of the 
        /// coordinate system. It locates the intersection points between a set of parallel search lines in the primary 
        /// rectangle and the edge of an object. The method determines the intersection points based on their contrast, 
        /// width, and steepness. The method calculates a best-fit line through the points found. This line defines the 
        /// main axis of the coordinate system. The process is repeated perpendicularly in the secondary rectangle in 
        /// order to locate the secondary axis. The intersection between the main axis and the secondary axis is the origin 
        /// of the coordinate system.
        /// <para>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </para>
        /// </remarks>

        public static FindTransformReport FindTransformRectangles(VisionImage image, Roi primaryRoi, Roi secondaryRoi, FindTransformMode mode, CoordinateTransform transform)
        {
            return FindTransformRectangles(image, primaryRoi, secondaryRoi, mode, transform, new FindTransformRectsOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Computes a coordinate transform based on the position of an object in a search area of an image. The method uses 
        /// the location and orientation of the coordinate system it finds to create the reference system of a 
        /// coordinate transform or to update the measurement system of an existing coordinate transform.
        /// </summary>
        /// <param name="image">
        /// The image which the method uses to compute the coordinate transform. 
        /// </param>
        /// <param name="primaryRoi">
        /// Defines the area within which the edge detection is performed for the primary axis. 
        /// </param>
        /// <param name="secondaryRoi">
        /// Defines the area within which the edge detection is performed for the secondary axis. 
        /// </param>
        /// <param name="mode">
        /// Specifies how the method updates the coordinate transform. The default is 
        /// <see cref="NationalInstruments.Vision.Analysis.FindTransformMode.FindReference" crefType="PartiallyQualified"/>
        /// </param>
        /// <param name="transform">
        /// Specifies how to transform pixel coordinates based on the difference between the reference coordinate system 
        /// and the measurement coordinate system.
        /// </param>
        /// <param name="findTransformRectsOptions">
        /// Specifies options for detecting edges along search lines in the ROI and for overlaying search information.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FindTransformReport" crefType="Unqualified"/> object containing the 
        /// axes that were located.
        /// </returns>
        /// <remarks>
        /// This method uses the following algorithm. First the method determines the position of the main axis of the 
        /// coordinate system. It locates the intersection points between a set of parallel search lines in the primary 
        /// rectangle and the edge of an object. The method determines the intersection points based on their contrast, 
        /// width, and steepness. The method calculates a best-fit line through the points found. This line defines the 
        /// main axis of the coordinate system. The process is repeated perpendicularly in the secondary rectangle in 
        /// order to locate the secondary axis. The intersection between the main axis and the secondary axis is the origin 
        /// of the coordinate system.
        /// <para>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Set up the ROIs.
        /// Dim primaryRoi As New Roi(New Shape() {New RectangleContour(0, 0, 80, 80)})
        /// Dim secondaryRoi As New Roi(New Shape() {New RectangleContour(150, 0, 80, 80)})
        ///  
        /// 'Find the transformation in the image in Viewer1.
        /// Dim Report As FindTransformReport = Algorithms.FindTransformRectangles (imageViewer1.Image, primaryRoi, secondaryRoi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Set up the ROIs.
        /// Roi primaryRoi = new Roi(new Shape[] {new RectangleContour(0, 0, 80, 80)});
        /// Roi secondaryRoi = new Roi(new Shape[] {new RectangleContour(150, 0, 80, 80)});
        ///  
        /// //Find the transformation in the image in Viewer1.
        /// FindTransformReport report = Algorithms.FindTransformRectangles(imageViewer1.Image, primaryRoi, secondaryRoi);
        /// </code>
        /// </example>

        public static FindTransformReport FindTransformRectangles(VisionImage image, Roi primaryRoi, Roi secondaryRoi, FindTransformMode mode, CoordinateTransform transform, FindTransformRectsOptions findTransformRectsOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (primaryRoi == null) { throw new ArgumentNullException("primaryRoi"); }
            primaryRoi.ThrowIfDisposed();
            if (secondaryRoi == null) { throw new ArgumentNullException("secondaryRoi"); }
            secondaryRoi.ThrowIfDisposed();
            if (transform == null) { throw new ArgumentNullException("transform"); }
            if (findTransformRectsOptions == null) { throw new ArgumentNullException("findTransformRectsOptions"); }
            if (findTransformRectsOptions.PrimaryEdgeOptions == null) { throw new ArgumentNullException("findTransformRectsOptions"); }
            if (findTransformRectsOptions.SecondaryEdgeOptions == null) { throw new ArgumentNullException("findTransformRectsOptions"); }
            if (findTransformRectsOptions.PrimaryStraightEdgeOptions == null) { throw new ArgumentNullException("findTransformRectsOptions"); }
            if (findTransformRectsOptions.SecondaryStraightEdgeOptions == null) { throw new ArgumentNullException("findTransformRectsOptions"); }
            CVI_FindTransformRectsOptions2 cviFindTransformRectsOptions = new CVI_FindTransformRectsOptions2();
            cviFindTransformRectsOptions.ConvertFromExternal(findTransformRectsOptions);
            CVI_StraightEdgeOptions cviPrimaryStraightEdgeOptions = new CVI_StraightEdgeOptions();
            cviPrimaryStraightEdgeOptions.ConvertFromExternal(findTransformRectsOptions.PrimaryStraightEdgeOptions);
            CVI_StraightEdgeOptions cviSecondaryStraightEdgeOptions = new CVI_StraightEdgeOptions();
            cviSecondaryStraightEdgeOptions.ConvertFromExternal(findTransformRectsOptions.SecondaryStraightEdgeOptions);
            CVI_CoordinateSystem cviBaseSystem = new CVI_CoordinateSystem();
            cviBaseSystem.ConvertFromExternal(transform.ReferenceSystem);
            CVI_CoordinateSystem cviNewSystem = new CVI_CoordinateSystem();
            cviNewSystem.ConvertFromExternal(transform.MeasurementSystem);
            CVI_AxisReport cviAxisReport = new CVI_AxisReport();
            Utilities.ThrowError(VisionDll.imaqFindTransformRects2(image._image, primaryRoi._roi, secondaryRoi._roi, mode, ref cviBaseSystem, ref cviNewSystem, ref cviFindTransformRectsOptions, ref cviPrimaryStraightEdgeOptions, ref cviSecondaryStraightEdgeOptions, out cviAxisReport));
            // We have to construct the report ourselves.
            FindTransformReport report = new FindTransformReport();
            report.AxisReport = cviAxisReport.ConvertToExternal();
            report.Transform.ReferenceSystem = cviBaseSystem.ConvertToExternal();
            report.Transform.MeasurementSystem = cviNewSystem.ConvertToExternal();
            return report;
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along a set of parallel lines defined inside a rectangular region. Edges are determined based on their 
        /// contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangle or rotated rectangular region within which the edge detection is performed.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.RakeReport" crefType="Unqualified"/> object containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the rake:
        /// <image src="rake.gif"/>
        /// </remarks>

        public static RakeReport Rake(VisionImage image, Roi roi)
        {
            return Rake(image, roi, RakeDirection.LeftToRight, EdgeProcess.All, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along a set of parallel lines defined inside a rectangular region. Edges are determined based on their 
        /// contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangle or rotated rectangular region within which the edge detection is performed.
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which edges are searched for along the lines. The default is RakeDirection.LeftToRight.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.RakeReport" crefType="Unqualified"/> object containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the rake:
        /// <image src="rake.gif"/>
        /// </remarks>

        public static RakeReport Rake(VisionImage image, Roi roi, RakeDirection direction)
        {
            return Rake(image, roi, direction, EdgeProcess.All, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along a set of parallel lines defined inside a rectangular region. Edges are determined based on their 
        /// contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangle or rotated rectangular region within which the edge detection is performed.
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which edges are searched for along the lines. The default is RakeDirection.LeftToRight.
        /// </param>
        /// <param name="process">
        /// Determines the type of search. The method can return the first edge, both the first and the last edge, all edges, or the
        /// best edge found along the paths. The default is All.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.RakeReport" crefType="Unqualified"/> object containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the rake:
        /// <image src="rake.gif"/>
        /// </remarks>

        public static RakeReport Rake(VisionImage image, Roi roi, RakeDirection direction, EdgeProcess process)
        {
            return Rake(image, roi, direction, process, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along a set of parallel lines defined inside a rectangular region. Edges are determined based on their 
        /// contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangle or rotated rectangular region within which the edge detection is performed.
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which edges are searched for along the lines. The default is RakeDirection.LeftToRight.
        /// </param>
        /// <param name="process">
        /// Determines the type of search. The method can return the first edge, both the first and the last edge, all edges, or the
        /// best edge found along the paths. The default is All.
        /// </param>
        /// <param name="stepSize">
        /// Defines the distance, in pixels, between the parallel lines inside the rectangular region. The default is 5.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.RakeReport" crefType="Unqualified"/> object containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the rake:
        /// <image src="rake.gif"/>
        /// </remarks>

        public static RakeReport Rake(VisionImage image, Roi roi, RakeDirection direction, EdgeProcess process, int stepSize)
        {
            return Rake(image, roi, direction, process, stepSize, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along a set of parallel lines defined inside a rectangular region. Edges are determined based on their 
        /// contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangle or rotated rectangular region within which the edge detection is performed.
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which edges are searched for along the lines. The default is RakeDirection.LeftToRight.
        /// </param>
        /// <param name="process">
        /// Determines the type of search. The method can return the first edge, both the first and the last edge, all edges, or the
        /// best edge found along the paths. The default is All.
        /// </param>
        /// <param name="stepSize">
        /// Defines the distance, in pixels, between the parallel lines inside the rectangular region. The default is 5.
        /// </param>
        /// <param name="options">
        /// Defines the characteristics that the method uses to find the edges and the parameters it needs for 
        /// subpixel analysis of the edges.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.RakeReport" crefType="Unqualified"/> object containing information 
        /// about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the rake:
        /// <image src="rake.gif"/>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// [Visual Basic]
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This routine assumes that you have a rectangle or rotated rectangle selected on the viewer
        ///  
        /// 'Perform a rake. Find only the first edge along each line
        /// Dim Report As RakeReport = Algorithms.Rake (imageViewer1.Image, imageViewer1.Roi, RakeDirection.LeftToRight, EdgeProcess.First)
        /// 'Overlay the search lines
        /// For Each LineInfo As SearchLineInfo In Report.SearchLines
        ///     imageViewer1.Image.Overlays.Default.AddLine (LineInfo.Line, Rgb32Value.BlueColor)
        /// Next
        /// 'Overlay edge points found
        /// For Each Edge As EdgeInfo In Report.FirstEdges
        ///     Dim pt As PointContour = Edge.Position
        ///     imageViewer1.Image.Overlays.Default.AddOval (New OvalContour(pt.X - 2, pt.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.DrawValue)
        /// Next
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // This routine assumes that you have a rectangle or rotated rectangle selected on the viewer
        ///  
        /// // Perform a rake. Find only the first edge along each line
        /// RakeReport report = Algorithms.Rake(imageViewer1.Image, imageViewer1.Roi, RakeDirection.LeftToRight, EdgeProcess.First);
        /// // Overlay the search lines
        /// foreach (SearchLineInfo lineInfo in report.SearchLines) {
        ///     imageViewer1.Image.Overlays.Default.AddLine(lineInfo.Line, Rgb32Value.BlueColor);
        /// }
        /// // Overlay edge points found
        /// foreach (EdgeInfo edge in report.FirstEdges) {
        ///     PointContour pt = edge.Position;
        ///     imageViewer1.Image.Overlays.Default.AddOval(New OvalContour(pt.X - 2, pt.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.DrawValue);
        /// }
        /// </code>
        /// </example>

        public static RakeReport Rake(VisionImage image, Roi roi, RakeDirection direction, EdgeProcess process, int stepSize, EdgeOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_EdgeOptions2 cviOptions = new CVI_EdgeOptions2();
            cviOptions.ConvertFromExternal(options);
            IntPtr report = VisionDll.imaqRake2(image._image, roi._roi, direction, process, stepSize, ref cviOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<RakeReport, CVI_RakeReport2>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along a set of parallel lines defined inside a rectangular region. Edges are determined based on their 
        /// contrast and slope. Only the first edge along each search line is returned.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangle or rotated rectangular region within which the edge detection is performed.
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which edges are searched for along the lines. The default is RakeDirection.LeftToRight.
        /// </param>
        /// <param name="process">
        /// Determines the type of search. The method can return the first edge, both the first and the last edge, all edges, or the
        /// best edge found along the paths. The default is All.
        /// </param>
        /// <param name="stepSize">
        /// Defines the distance, in pixels, between the parallel lines inside the rectangular region. The default is 5.
        /// </param>
        /// <param name="options">
        /// Defines the characteristics that the method uses to find the edges and the parameters it needs for 
        /// subpixel analysis of the edges.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FirstEdgeRakeReport" crefType="Unqualified"/> object containing information 
        /// about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the rake:
        /// <image src="rake.gif"/>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// [Visual Basic]
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This routine assumes that you have a rectangle or rotated rectangle selected on the viewer
        ///  
        /// 'Perform a rake. Find only the first edge along each line
        /// Dim Report As RakeReport = Algorithms.Rake (imageViewer1.Image, imageViewer1.Roi, RakeDirection.LeftToRight, EdgeProcess.First)
        /// 'Overlay the search lines
        /// For Each LineInfo As SearchLineInfo In Report.SearchLines
        ///     imageViewer1.Image.Overlays.Default.AddLine (LineInfo.Line, Rgb32Value.BlueColor)
        /// Next
        /// 'Overlay edge points found
        /// For Each Edge As EdgeInfo In Report.FirstEdges
        ///     Dim pt As PointContour = Edge.Position
        ///     imageViewer1.Image.Overlays.Default.AddOval (New OvalContour(pt.X - 2, pt.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.DrawValue)
        /// Next
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // This routine assumes that you have a rectangle or rotated rectangle selected on the viewer
        ///  
        /// // Perform a rake. Find only the first edge along each line
        /// RakeReport report = Algorithms.Rake(imageViewer1.Image, imageViewer1.Roi, RakeDirection.LeftToRight, EdgeProcess.First);
        /// // Overlay the search lines
        /// foreach (SearchLineInfo lineInfo in report.SearchLines) {
        ///     imageViewer1.Image.Overlays.Default.AddLine(lineInfo.Line, Rgb32Value.BlueColor);
        /// }
        /// // Overlay edge points found
        /// foreach (EdgeInfo edge in report.FirstEdges) {
        ///     PointContour pt = edge.Position;
        ///     imageViewer1.Image.Overlays.Default.AddOval(New OvalContour(pt.X - 2, pt.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.DrawValue);
        /// }
        /// </code>
        /// </example>

        public static FirstEdgeRakeReport FirstEdgeRake(VisionImage image, Roi roi, RakeDirection direction, EdgeProcess process, int stepSize, EdgeOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_EdgeOptions2 cviOptions = new CVI_EdgeOptions2();
            cviOptions.ConvertFromExternal(options);
            // Get a pointer to the c-style rake report
            IntPtr report = VisionDll.imaqRake2(image._image, roi._roi, direction, process, stepSize, ref cviOptions);
            Utilities.ThrowError(report);
            // Marshal a .NET representation of the spoke report
            CVI_RakeReport2 cviRakeReport = (CVI_RakeReport2)Marshal.PtrToStructure(report, typeof(CVI_RakeReport2));
            // Convert only the needed information into an external version of the report
            FirstEdgeRakeReport toReturn = new FirstEdgeRakeReport(cviRakeReport.FirstEdges, cviRakeReport.NumFirstEdges);
            // Dispose the original C report now that we have everything we need in .NET representation
            VisionDllCommon.imaqDispose(report);
            return toReturn;
        }
        //==========================================================================================
        /// <summary>
        /// Finds prominent edges along a collection of pixel coordinates. This method can return the first, 
        /// both the first and the last, or all the edges found.
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges.
        /// </param>
        /// <param name="points">
        /// The ROI containing the path along which the method detects the edges.
        /// </param>
        /// <returns>
        /// A collection of PointContour objects containing the coordinates of the edge points.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// <para>
        /// You can use this method in conjunction with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.RoiProfile" crefType="Unqualified"/> method to 
        /// obtain the PointContours that define the path.
        /// </para>
        /// </remarks>

        public static Collection<PointContour> SimpleEdge(VisionImage image, Roi points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return SimpleEdge(image, Utilities.ConvertRoiToPoints(points), new SimpleEdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds prominent edges along a collection of pixel coordinates. This method can return the first, 
        /// both the first and the last, or all the edges found.
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges.
        /// </param>
        /// <param name="points">
        /// The coordinates of the path along which the method detects the edges.
        /// </param>
        /// <returns>
        /// A collection of PointContour objects containing the coordinates of the edge points.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// <para>
        /// You can use this method in conjunction with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.RoiProfile" crefType="Unqualified"/> method to 
        /// obtain the PointContours that define the path.
        /// </para>
        /// </remarks>

        public static Collection<PointContour> SimpleEdge(VisionImage image, Collection<PointContour> points)
        {
            return SimpleEdge(image, points, new SimpleEdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds prominent edges along a collection of pixel coordinates. This method can return the first, 
        /// both the first and the last, or all the edges found.
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges.
        /// </param>
        /// <param name="points">
        /// The ROI containing the path along which the method detects the edges.
        /// </param>
        /// <param name="options">
        /// Describes how you want the function to find edges. 
        /// </param>
        /// <returns>
        /// A collection of PointContour objects containing the coordinates of the edge points.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// <para>
        /// You can use this method in conjunction with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.RoiProfile" crefType="Unqualified"/> method to 
        /// obtain the PointContours that define the path.
        /// </para>
        /// </remarks>

        public static Collection<PointContour> SimpleEdge(VisionImage image, Roi points, SimpleEdgeOptions options)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return SimpleEdge(image, Utilities.ConvertRoiToPoints(points), options);
        }
        //==========================================================================================
        /// <summary>
        /// Finds prominent edges along a collection of pixel coordinates. This method can return the first, 
        /// both the first and the last, or all the edges found.
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges.
        /// </param>
        /// <param name="points">
        /// The coordinates of the path along which the method detects the edges.
        /// </param>
        /// <param name="options">
        /// Describes how you want the function to find edges. 
        /// </param>
        /// <returns>
        /// A collection of PointContour objects containing the coordinates of the edge points.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// <para>
        /// You can use this method in conjunction with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.RoiProfile" crefType="Unqualified"/> method to 
        /// obtain the PointContours that define the path.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Find the edge coordinates along a path defined by regions
        /// 'on Viewer1 and display the coordinates on the image.
        ///  
        /// 'First, find the coordinates of the points along the path
        /// 'defined by the regions on Viewer1
        /// Dim Profile As RoiProfileReport = Algorithms.RoiProfile (imageViewer1.Image, imageViewer1.Roi)
        ///  
        /// 'Find the edges along the selected path
        /// Dim EdgeCoordinates As Collection(Of PointContour) = Algorithms.SimpleEdge (imageViewer1.Image, Profile.Pixels)
        ///  
        /// 'Display the results
        /// For Each Pt As PointContour In EdgeCoordinates
        ///     Dim oval As New OvalContour(Pt.X - 4, Pt.Y - 4, 8, 8)
        ///     imageViewer1.Image.Overlays.Default.AddOval (oval, Rgb32Value.RedColor, DrawingMode.PaintValue)
        /// Next
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Find the edge coordinates along a path defined by regions
        /// // on Viewer1 and display the coordinates on the image.
        ///  
        /// // First, find the coordinates of the points along the path
        /// // defined by the regions on Viewer1
        /// RoiProfileReport profile = Algorithms.RoiProfile(imageViewer1.Image, imageViewer1.Roi);
        ///  
        /// // Find the edges along the selected path
        /// Collection&lt;PointContour&gt; edgeCoordinates = Algorithms.SimpleEdge (imageViewer1.Image, profile.Pixels);
        ///  
        /// // Display the results
        /// foreach (PointContour pt in edgeCoordinates) {
        ///     OvalContour oval = new OvalContour(pt.X - 4, pt.Y - 4, 8, 8);
        ///     imageViewer1.Image.Overlays.Default.AddOval(oval, Rgb32Value.RedColor, DrawingMode.PaintValue);
        /// }
        /// </code>
        /// </example>

        public static Collection<PointContour> SimpleEdge(VisionImage image, Collection<PointContour> points, SimpleEdgeOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (points == null) { throw new ArgumentNullException("points"); }
            if (options == null) { throw new ArgumentNullException("options"); }

            CVI_Point[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_Point>(points);
            CVI_SimpleEdgeOptions cviOptions = new CVI_SimpleEdgeOptions();
            cviOptions.ConvertFromExternal(options);
            Int32 numEdges;
            IntPtr report = VisionDll.imaqSimpleEdge(image._image, cviPoints, cviPoints.Length, ref cviOptions, out numEdges);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToCollection<PointContour, CVI_PointFloat>(report, numEdges, true);
        }

        //==========================================================================================
        /// <summary>
        /// Finds edges along radial lines specified inside an annular region. 
        /// The edges are determined based on their contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangular region the function looks in for the edges. The first contour of roi must be a rectangle or a rotated rectangle. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.SpokeReport" crefType="Unqualified"/> object 
        /// containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the spoke:
        /// <image src="spoke.gif"/>
        /// </remarks>

        public static SpokeReport Spoke(VisionImage image, Roi roi)
        {
            return Spoke(image, roi, SpokeDirection.OutsideToInside, EdgeProcess.All, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along radial lines specified inside an annular region. 
        /// The edges are determined based on their contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangular region the function looks in for the edges. The first contour of roi must be a rectangle or a rotated rectangle. 
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which the edges are searched for. The default is OutsideToInside.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.SpokeReport" crefType="Unqualified"/> object 
        /// containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the spoke:
        /// <image src="spoke.gif"/>
        /// </remarks>

        public static SpokeReport Spoke(VisionImage image, Roi roi, SpokeDirection direction)
        {
            return Spoke(image, roi, direction, EdgeProcess.All, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along radial lines specified inside an annular region. 
        /// The edges are determined based on their contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangular region the function looks in for the edges. The first contour of roi must be a rectangle or a rotated rectangle. 
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which the edges are searched for. The default is OutsideToInside.
        /// </param>
        /// <param name="process">
        /// Determines the type of search. The method can return the first edge, both the first and the last edge, 
        /// all edges, or the best edge found along the paths. The default is All.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.SpokeReport" crefType="Unqualified"/> object 
        /// containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the spoke:
        /// <image src="spoke.gif"/>
        /// </remarks>

        public static SpokeReport Spoke(VisionImage image, Roi roi, SpokeDirection direction, EdgeProcess process)
        {
            return Spoke(image, roi, direction, process, 5, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along radial lines specified inside an annular region. 
        /// The edges are determined based on their contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangular region the function looks in for the edges. The first contour of roi must be a rectangle or a rotated rectangle. 
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which the edges are searched for. The default is OutsideToInside.
        /// </param>
        /// <param name="process">
        /// Determines the type of search. The method can return the first edge, both the first and the last edge, 
        /// all edges, or the best edge found along the paths. The default is All.
        /// </param>
        /// <param name="stepSize">
        /// Specifies the number of pixels between each search line. The default is 5.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.SpokeReport" crefType="Unqualified"/> object 
        /// containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the spoke:
        /// <image src="spoke.gif"/>
        /// </remarks>

        public static SpokeReport Spoke(VisionImage image, Roi roi, SpokeDirection direction, EdgeProcess process, int stepSize)
        {
            return Spoke(image, roi, direction, process, stepSize, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along radial lines specified inside an annular region. 
        /// The edges are determined based on their contrast and slope.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangular region the function looks in for the edges. The first contour of roi must be a rectangle or a rotated rectangle. 
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which the edges are searched for. The default is OutsideToInside.
        /// </param>
        /// <param name="process">
        /// Determines the type of search. The method can return the first edge, both the first and the last edge, 
        /// all edges, or the best edge found along the paths. The default is All.
        /// </param>
        /// <param name="stepSize">
        /// Specifies the number of pixels between each search line. The default is 5.
        /// </param>
        /// <param name="options">
        /// Specifies the parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.SpokeReport" crefType="Unqualified"/> object 
        /// containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the spoke:
        /// <image src="spoke.gif"/>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This routine assumes that you have an annulus selected on the viewer
        ///  
        /// 'Perform a spoke. Find only the first edge along each line
        /// Dim Report As SpokeReport = Algorithms.Spoke (imageViewer1.Image, imageViewer1.Roi, SpokeDirection.OutsideToInside, EdgeProcess.First, 10)
        ///  
        /// 'Overlay the search lines
        /// For Each LineInfo As SearchLineInfo In Report.SearchLines
        ///     imageViewer1.Image.Overlays.Default.AddLine (LineInfo.Line, Rgb32Value.BlueColor)
        /// Next
        /// 'Overlay edge points found
        /// For Each Edge As EdgeInfo In Report.FirstEdges
        ///     Dim pt As PointContour = Edge.Position
        ///     imageViewer1.Image.Overlays.Default.AddOval (New OvalContour(pt.X - 2, pt.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.DrawValue)
        /// Next
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // This routine assumes that you have an annulus selected on the viewer
        ///  
        /// // Perform a spoke. Find only the first edge along each line
        /// SpokeReport report = Algorithms.Spoke(imageViewer1.Image, imageViewer1.Roi, SpokeDirection.OutsideToInside, EdgeProcess.First, 10);
        ///  
        /// // Overlay the search lines
        /// foreach (SearchLineInfo lineInfo in report.SearchLines) {
        ///     imageViewer1.Image.Overlays.Default.AddLine(lineInfo.Line, Rgb32Value.BlueColor);
        /// }
        /// // Overlay edge points found
        /// foreach (EdgeInfo edge in report.FirstEdges) {
        ///     PointContour pt = edge.Position;
        ///     imageViewer1.Image.Overlays.Default.AddOval(New OvalContour(pt.X - 2, pt.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.DrawValue);
        /// }
        /// </code>
        /// </example>

        public static SpokeReport Spoke(VisionImage image, Roi roi, SpokeDirection direction, EdgeProcess process, int stepSize, EdgeOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_EdgeOptions2 cviOptions = new CVI_EdgeOptions2();
            cviOptions.ConvertFromExternal(options);
            IntPtr report = VisionDll.imaqSpoke2(image._image, roi._roi, direction, process, stepSize, ref cviOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<SpokeReport, CVI_SpokeReport2>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Finds edges along radial lines specified inside an annular region. 
        /// The edges are determined based on their contrast and slope.
        /// Finds and returns only the first edges for optimization.
        /// </summary>
        /// <param name="image">
        /// The image in which to find the edges.
        /// </param>
        /// <param name="roi">
        /// The rectangular region the function looks in for the edges. The first contour of roi must be a rectangle or a rotated rectangle. 
        /// </param>
        /// <param name="direction">
        /// Defines the direction along which the edges are searched for. The default is OutsideToInside.
        /// </param>
        /// <param name="process">
        /// Determines the type of search. The method can return the first edge, both the first and the last edge, 
        /// all edges, or the best edge found along the paths. The default is All.
        /// </param>
        /// <param name="stepSize">
        /// Specifies the number of pixels between each search line. The default is 5.
        /// </param>
        /// <param name="options">
        /// Specifies the parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.SpokeReport" crefType="Unqualified"/> object 
        /// containing information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images. The following image illustrates the spoke:
        /// <image src="spoke.gif"/>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This routine assumes that you have an annulus selected on the viewer
        ///  
        /// 'Perform a spoke. Find only the first edge along each line
        /// Dim Report As SpokeReport = Algorithms.Spoke (imageViewer1.Image, imageViewer1.Roi, SpokeDirection.OutsideToInside, EdgeProcess.First, 10)
        ///  
        /// 'Overlay the search lines
        /// For Each LineInfo As SearchLineInfo In Report.SearchLines
        ///     imageViewer1.Image.Overlays.Default.AddLine (LineInfo.Line, Rgb32Value.BlueColor)
        /// Next
        /// 'Overlay edge points found
        /// For Each Edge As EdgeInfo In Report.FirstEdges
        ///     Dim pt As PointContour = Edge.Position
        ///     imageViewer1.Image.Overlays.Default.AddOval (New OvalContour(pt.X - 2, pt.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.DrawValue)
        /// Next
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // This routine assumes that you have an annulus selected on the viewer
        ///  
        /// // Perform a spoke. Find only the first edge along each line
        /// SpokeReport report = Algorithms.Spoke(imageViewer1.Image, imageViewer1.Roi, SpokeDirection.OutsideToInside, EdgeProcess.First, 10);
        ///  
        /// // Overlay the search lines
        /// foreach (SearchLineInfo lineInfo in report.SearchLines) {
        ///     imageViewer1.Image.Overlays.Default.AddLine(lineInfo.Line, Rgb32Value.BlueColor);
        /// }
        /// // Overlay edge points found
        /// foreach (EdgeInfo edge in report.FirstEdges) {
        ///     PointContour pt = edge.Position;
        ///     imageViewer1.Image.Overlays.Default.AddOval(New OvalContour(pt.X - 2, pt.Y - 2, 5, 5), Rgb32Value.RedColor, DrawingMode.DrawValue);
        /// }
        /// </code>
        /// </example>

        public static FirstEdgeRakeReport FirstEdgeSpoke(VisionImage image, Roi roi, SpokeDirection direction, EdgeProcess process, int stepSize, EdgeOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_EdgeOptions2 cviOptions = new CVI_EdgeOptions2();
            cviOptions.ConvertFromExternal(options);
            // Get the c-pointer to the spoke report
            IntPtr report = VisionDll.imaqSpoke2(image._image, roi._roi, direction, process, stepSize, ref cviOptions);
            Utilities.ThrowError(report);
            // Marshal a .NET representation of the spoke report
            CVI_SpokeReport2 cviRakeReport = (CVI_SpokeReport2)Marshal.PtrToStructure(report, typeof(CVI_SpokeReport2));
            // Convert only the needed information into an external version of the report
            FirstEdgeRakeReport toReturn = new FirstEdgeRakeReport(cviRakeReport.FirstEdges, cviRakeReport.NumFirstEdges);
            // Dispose the original C report now that we have everything we need in .NET representation
            VisionDllCommon.imaqDispose(report);
            return toReturn;
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static StraightEdgeReport StraightEdge(VisionImage image, Roi roi)
        {
            return StraightEdge(image, roi, SearchDirection.LeftToRight, new EdgeOptions(), new StraightEdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <param name="direction">
        /// The direction to search for straight lines. The default is LeftToRight.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static StraightEdgeReport StraightEdge(VisionImage image, Roi roi, SearchDirection direction)
        {
            return StraightEdge(image, roi, direction, new EdgeOptions(), new StraightEdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <param name="direction">
        /// The direction to search for straight lines. The default is LeftToRight.
        /// </param>
        /// <param name="edgeOptions">
        /// Specifies the parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static StraightEdgeReport StraightEdge(VisionImage image, Roi roi, SearchDirection direction, EdgeOptions edgeOptions)
        {
            return StraightEdge(image, roi, direction, edgeOptions, new StraightEdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <param name="direction">
        /// The direction to search for straight lines. The default is LeftToRight.
        /// </param>
        /// <param name="edgeOptions">
        /// Specifies the parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <param name="straightEdgeOptions">
        /// Specifies the options used to fit a line in <format type="italics">roi</format>. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Look for a horizontal edge in the image in viewer1, searching top to bottom.
        /// Dim Report As StraightEdgeReport = Algorithms.StraightEdge (imageViewer1.Image, imageViewer1.Roi, SearchDirection.TopToBottom)
        ///  
        /// 'Overlay the edges found on the image.
        /// For Each ReportItem As StraightEdgeReportItem In Report.StraightEdges
        ///     imageViewer1.Image.Overlays.Default.AddLine (ReportItem.StraightEdge)
        /// Next
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Look for a horizontal edge in the image in viewer1, searching top to bottom.
        /// StraightEdgeReport report = Algorithms.StraightEdge(imageViewer1.Image, imageViewer1.Roi, SearchDirection.TopToBottom);
        ///  
        /// // Overlay the edges found on the image.
        /// foreach (StraightEdgeReportItem reportItem in report.StraightEdges) {
        ///     imageViewer1.Image.Overlays.Default.AddLine(reportItem.StraightEdge);
        /// }
        /// </code>
        /// </example>

        public static StraightEdgeReport StraightEdge(VisionImage image, Roi roi, SearchDirection direction, EdgeOptions edgeOptions, StraightEdgeOptions straightEdgeOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (edgeOptions == null) { throw new ArgumentNullException("edgeOptions"); }
            if (straightEdgeOptions == null) { throw new ArgumentNullException("straightEdgeOptions"); }

            CVI_EdgeOptions2 cviEdgeOptions = new CVI_EdgeOptions2();
            cviEdgeOptions.ConvertFromExternal(edgeOptions);
            CVI_StraightEdgeOptions cviStraightEdgeOptions = new CVI_StraightEdgeOptions();
            cviStraightEdgeOptions.ConvertFromExternal(straightEdgeOptions);
            IntPtr report = VisionDll.imaqStraightEdge(image._image, roi._roi, direction, ref cviEdgeOptions, ref cviStraightEdgeOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<StraightEdgeReport, CVI_StraightEdgeReport2>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static StraightEdgeReport StraightEdge2(VisionImage image, Roi roi)
        {
            return StraightEdge2(image, roi, SearchDirection.LeftToRight);
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <param name="direction">
        /// The direction to search for straight lines. The default is LeftToRight.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static StraightEdgeReport StraightEdge2(VisionImage image, Roi roi, SearchDirection direction)
        {
            return StraightEdge2(image, roi, direction, new EdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <param name="direction">
        /// The direction to search for straight lines. The default is LeftToRight.
        /// </param>
        /// <param name="edgeOptions">
        /// Specifies the parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static StraightEdgeReport StraightEdge2(VisionImage image, Roi roi, SearchDirection direction, EdgeOptions edgeOptions)
        {
            return StraightEdge2(image, roi, direction, edgeOptions, new StraightEdgeOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <param name="direction">
        /// The direction to search for straight lines. The default is LeftToRight.
        /// </param>
        /// <param name="edgeOptions">
        /// Specifies the parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <param name="straightEdgeOptions">
        /// Specifies the options used to fit a line in <format type="italics">roi</format>. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static StraightEdgeReport StraightEdge2(VisionImage image, Roi roi, SearchDirection direction, EdgeOptions edgeOptions, StraightEdgeOptions straightEdgeOptions)
        {
            return StraightEdge2(image, roi, direction, edgeOptions, straightEdgeOptions, true);
        }
        //==========================================================================================
        /// <summary>
        /// Finds straight edges inside an ROI in an image. 
        /// </summary>
        /// <param name="image">
        /// The image in which to find edges. 
        /// </param>
        /// <param name="roi">
        /// The ROI to find straight edges inside. The first contour of roi must be a rectangle, rotated rectangle, or a 
        /// 4-sided closed contour. 
        /// </param>
        /// <param name="direction">
        /// The direction to search for straight lines. The default is LeftToRight.
        /// </param>
        /// <param name="edgeOptions">
        /// Specifies the parameters that are used to compute the edge profile and detect edges. 
        /// </param>
        /// <param name="straightEdgeOptions">
        /// Specifies the options used to fit a line in <format type="italics">roi</format>. 
        /// </param>
        /// <param name="optimizedMode">
        /// Specifies whether to use the optimized mode or not. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.StraightEdgeReport" crefType="Unqualified"/> object containing 
        /// information about the edges found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static StraightEdgeReport StraightEdge2(VisionImage image, Roi roi, SearchDirection direction, EdgeOptions edgeOptions, StraightEdgeOptions straightEdgeOptions, bool optimizedMode)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (edgeOptions == null) { throw new ArgumentNullException("edgeOptions"); }
            if (straightEdgeOptions == null) { throw new ArgumentNullException("straightEdgeOptions"); }
            Int32 optimized = optimizedMode ? 1 : 0;
            CVI_EdgeOptions2 cviEdgeOptions = new CVI_EdgeOptions2();
            cviEdgeOptions.ConvertFromExternal(edgeOptions);
            CVI_StraightEdgeOptions cviStraightEdgeOptions = new CVI_StraightEdgeOptions();
            cviStraightEdgeOptions.ConvertFromExternal(straightEdgeOptions);
            IntPtr report = VisionDll.imaqStraightEdge2(image._image, roi._roi, direction, ref cviEdgeOptions, ref cviStraightEdgeOptions, optimized);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<StraightEdgeReport, CVI_StraightEdgeReport2>(report, true);
        }
        #endregion

        #region Spatial Filters functions
        //==========================================================================================
        /// <summary>
        /// Applies a linear filter to an image by convolving the image with a filtering kernel.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="kernel">
        /// The convolution matrix.
        /// </param>
        /// <remarks>This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16, and Sgl images. The source and the destination images must be the same type of image. The convolution matrix must have an odd width and height. The source image must have been created with a border capable of using the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is not important. This functions is optimized for MMX.
        /// </remarks>

        public static void Convolute(VisionImage source, VisionImage destination, Kernel kernel)
        {
            Convolute(source, destination, kernel, null);
        }
        //==========================================================================================
        /// <summary>
        /// Applies a linear filter to an image by convolving the image with a filtering kernel.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="kernel">
        /// The convolution matrix.
        /// </param>
        /// <param name="mask">
        /// Specifies the region of the source image in which the method applies the convolution. The method applies the convolution to only those source pixels whose corresponding mask pixels are non-zero. Do not set this parameter if you want the method to convolve the entire image.
        /// </param>
        /// <remarks>This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16, and Sgl images. The source and the destination images must be the same type of image. <format type="italics">mask</format> must be a U8 image. The convolution matrix must have an odd width and height. The source image must have been created with a border capable of using the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is not important. This functions is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim k As New Kernel(KernelFamily.Gradient, 3, 2)
        /// ' Perform a convolution using a gradient filter on the image in Viewer1.
        /// ' Store the result in i.
        /// Algorithms.Convolute (imageViewer1.Image, i, k)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// Kernel k = new Kernel(KernelFamily.Gradient, 3, 2);
        /// // Perform a convolution using a gradient filter on the image in Viewer1.
        /// // Store the result in i.
        /// Algorithms.Convolute(imageViewer1.Image, i, k);
        /// </code>
        /// </example>

        public static void Convolute(VisionImage source, VisionImage destination, Kernel kernel, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (kernel == null) { throw new ArgumentNullException("kernel"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            Utilities.ThrowError(VisionDll.imaqConvolve2(destination._image, source._image, kernel.GetInternalEntries(), kernel.Height, kernel.Width, (float)kernel.Divider, VisionImage.GetIntPtr(mask), CVI_RoundingMode.Optimize));
        }
        //==========================================================================================
        /// <summary>
        /// Outlines edges in an image using the Canny algorithm, which is well-suited to images with poor signal-to-noise ratios.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The result of the processing.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void CannyEdgeFilter(VisionImage source, VisionImage destination)
        {
            CannyEdgeFilter(source, destination, new CannyOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Outlines edges in an image using the Canny algorithm, which is well-suited to images with poor signal-to-noise ratios.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The result of the processing.
        /// </param>
        /// <param name="options">
        /// The options to use to perform the Canny algorithm.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Threshold the image in Viewer1 inplace.
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), True, 255)
        ///  
        /// 'Outline the edges of the image in Viewer1 and
        /// 'store the results in i
        /// Algorithms.CannyEdgeFilter (imageViewer1.Image, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// //Threshold the image in Viewer1 inplace.
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), true, 255);
        ///  
        /// //Outline the edges of the image in Viewer1 and
        /// //store the results in i
        /// Algorithms.CannyEdgeFilter(imageViewer1.Image, i);
        ///  
        /// </code>
        /// </example>

        public static void CannyEdgeFilter(VisionImage source, VisionImage destination, CannyOptions options)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_CannyOptions cviOptions = new CVI_CannyOptions();
            cviOptions.ConvertFromExternal(options);
            Utilities.ThrowError(VisionDll.imaqCannyEdgeFilter(destination._image, source._image, ref cviOptions));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the normalized cross correlation between the source image and the template image.
        /// </summary>
        /// <param name="source">
        /// The source image. The correlation modifies the border of the source image. The border must be at least half as large as the larger dimension of the template image.
        /// </param>
        /// <param name="template">
        /// The template image to correlate against the source.
        /// </param>
        /// <param name="destination">
        /// The resulting 8-bit image containing the cross correlation values normalized to lie in the range [0, 255]. A value of 255 indicates a very high correlation and a value of 0 indicates no correlation.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8 images. This method is optimized for MMX. However, MMX optimization is valid only when the width of the template image is a multiple of 4.
        /// Correlation is a time-intensive operation. You can reduce the time required to perform a correlation by keeping the template size small and reducing the search area in the source image using a RectangleContour.
        /// </remarks>

        public static void Correlate(VisionImage source, VisionImage template, VisionImage destination)
        {
            Correlate(source, template, destination, RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the normalized cross correlation between the source image and the template image.
        /// </summary>
        /// <param name="source">
        /// The source image. The correlation modifies the border of the source image. The border must be at least half as large as the larger dimension of the template image.
        /// </param>
        /// <param name="template">
        /// The template image to correlate against the source.
        /// </param>
        /// <param name="destination">
        /// The resulting 8-bit image containing the cross correlation values normalized to lie in the range [0, 255]. A value of 255 indicates a very high correlation and a value of 0 indicates no correlation.
        /// </param>
        /// <param name="rectangle">
        /// Defines a rectangular region in the source image that is used for the correlation process. This parameter must contain 0 or 1 contours. If it contains 1 contour, it must be a RectangleContour. Correlation is applied to the entire image if RectangleContour.None is passed.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8 images. This method is optimized for MMX. However, MMX optimization is valid only when the width of the template image is a multiple of 4.
        /// Correlation is a time-intensive operation. You can reduce the time required to perform a correlation by keeping the template size small and reducing the search area in the source image using a RectangleContour.
        /// </remarks>

        public static void Correlate(VisionImage source, VisionImage template, VisionImage destination, Roi rectangle)
        {
            Roi.ThrowIfNonNullAndDisposed(rectangle);
            Correlate(source, template, destination, Utilities.ConvertRoiToRectangle(rectangle));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the normalized cross correlation between the source image and the template image.
        /// </summary>
        /// <param name="source">
        /// The source image. The correlation modifies the border of the source image. The border must be at least half as large as the larger dimension of the template image.
        /// </param>
        /// <param name="template">
        /// The template image to correlate against the source.
        /// </param>
        /// <param name="destination">
        /// The resulting 8-bit image containing the cross correlation values normalized to lie in the range [0, 255]. A value of 255 indicates a very high correlation and a value of 0 indicates no correlation.
        /// </param>
        /// <param name="rectangle">
        /// Defines a rectangular region in the source image that is used for the correlation process. Correlation is applied to the entire image if RectangleContour.None is passed.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8 images. This method is optimized for MMX. However, MMX optimization is valid only when the width of the template image is a multiple of 4.
        /// Correlation is a time-intensive operation. You can reduce the time required to perform a correlation by keeping the template size small and reducing the search area in the source image using a RectangleContour.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim patternImage As New VisionImage
        /// Dim i As New VisionImage
        /// Dim rectangle As New RectangleContour
        ///  
        /// ' Populate patternImage and rectangle
        /// ' Correlate the patternImage with the image in Viewer1
        /// ' Store the results in i.
        /// Algorithms.Correlate (imageViewer1.Image, patternImage, i, rectangle)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage patternImage = new VisionImage();
        /// VisionImage i = new VisionImage();
        /// RectangleContour rectangle = new RectangleContour();
        ///     
        /// // Populate patternImage and rectangle
        /// // Correlate the patternImage with the image in Viewer1
        /// // Store the results in i.
        /// Algorithms.Correlate(imageViewer1.Image, patternImage, i, rectangle);
        /// </code>
        /// </example>

        public static void Correlate(VisionImage source, VisionImage template, VisionImage destination, RectangleContour rectangle)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (rectangle == null) { throw new ArgumentNullException("rectangle"); }
            CVI_Rectangle cviRect = new CVI_Rectangle();
            cviRect.ConvertFromExternal(rectangle);
            Utilities.ThrowError(VisionDll.imaqCorrelate(destination._image, source._image, template._image, cviRect));
        }
        //==========================================================================================
        /// <summary>
        /// Hightlights the edges of an image.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> must be the same type of image. The source image must have a border 
        /// capable of supporting the size of the processing matrix. For example, a 3 x 3 matrix has a minimum border size of 1. 
        /// The border size of the destination image is not important.
        /// </remarks>

        public static void EdgeFilter(VisionImage source, VisionImage destination)
        {
            EdgeFilter(source, destination, OutlineMethod.Difference, 0, null);
        }
        //==========================================================================================
        /// <summary>
        /// Hightlights the edges of an image.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="method">
        /// The type of edge-detection filter to use. The default is <see cref="NationalInstruments.Vision.Analysis.OutlineMethod.Difference" crefType="PartiallyQualified"/>.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> must be the same type of image. The source image must have a border 
        /// capable of supporting the size of the processing matrix. For example, a 3 x 3 matrix has a minimum border size of 1. 
        /// The border size of the destination image is not important.
        /// </remarks>

        public static void EdgeFilter(VisionImage source, VisionImage destination, OutlineMethod method)
        {
            EdgeFilter(source, destination, method, 0, null);
        }
        //==========================================================================================
        /// <summary>
        /// Hightlights the edges of an image.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="method">
        /// The type of edge-detection filter to use. The default is <see cref="NationalInstruments.Vision.Analysis.OutlineMethod.Difference" crefType="PartiallyQualified"/>.
        /// </param>
        /// <param name="threshold">
        /// The minimum pixel value to appear in the resulting image. It is rare to use a value greater than 0 
        /// for this type of processing because the results from this processing are usually dark and are not dynamic. 
        /// This parameter has a default value of 0.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> must be the same type of image. The source image must have a border 
        /// capable of supporting the size of the processing matrix. For example, a 3 x 3 matrix has a minimum border size of 1. 
        /// The border size of the destination image is not important.
        /// </remarks>

        public static void EdgeFilter(VisionImage source, VisionImage destination, OutlineMethod method, double threshold)
        {
            EdgeFilter(source, destination, method, threshold, null);
        }
        //==========================================================================================
        /// <summary>
        /// Hightlights the edges of an image.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="method">
        /// The type of edge-detection filter to use. The default is <see cref="NationalInstruments.Vision.Analysis.OutlineMethod.Difference" crefType="PartiallyQualified"/>.
        /// </param>
        /// <param name="threshold">
        /// The minimum pixel value to appear in the resulting image. It is rare to use a value greater than 0 
        /// for this type of processing because the results from this processing are usually dark and are not dynamic. 
        /// This parameter has a default value of 0.
        /// </param>
        /// <param name="mask">
        /// The mask to apply to the source image. The method processes only those pixels in the image whose corresponding 
        /// pixels in the mask are non-zero. Pass null or Nothing if you want to apply the filter to the entire image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images. <format type="italics">mask</format> must be a U8 image. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> must be the same type of image. The source image must have a border 
        /// capable of supporting the size of the processing matrix. For example, a 3 x 3 matrix has a minimum border size of 1. 
        /// The border size of the destination image is not important.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim maskImage As New VisionImage
        ///  
        /// ' Apply a Sobel filter to a portion of the image in Viewer1
        /// ' defined by regions selected on Viewer1.
        /// ' Store the result in i.
        /// Algorithms.RoiToMask (maskImage, imageViewer1.Roi)
        /// Algorithms.EdgeFilter (imageViewer1.Image, i, OutlineMethod.Sobel, 0, maskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// VisionImage maskImage = new VisionImage();
        ///     
        /// // Apply a Sobel filter to a portion of the image in Viewer1
        /// // defined by regions selected on Viewer1.
        /// // Store the result in i.
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// Algorithms.EdgeFilter(imageViewer1.Image, i, OutlineMethod.Sobel, 0, maskImage);
        /// </code>
        /// </example>

        public static void EdgeFilter(VisionImage source, VisionImage destination, OutlineMethod method, double threshold, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            Utilities.ThrowError(VisionDll.Priv_EdgeFilter(destination._image, source._image, method, (float)threshold, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the interpixel variation between the pixel being processed and those pixels surrounding it. 
        /// If the pixel being processed has a variation greater than a specified percentage, it is set to the 
        /// average pixel value as calculated from the neighboring pixels.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8, I16, and Single images. The <format type="italics">source</format> and 
        /// <format type="italics">destination</format> images must be the same type of image.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The source image must have been created with a border 
        /// capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, 
        /// a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is 
        /// not important.
        /// </para>
        /// 	<para>
        /// This function is optimized for MMX.
        /// </para>
        /// </remarks>

        public static void LowPass(VisionImage source, VisionImage destination)
        {
            LowPass(source, destination, new LowPassOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the interpixel variation between the pixel being processed and those pixels surrounding it. 
        /// If the pixel being processed has a variation greater than a specified percentage, it is set to the 
        /// average pixel value as calculated from the neighboring pixels.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="options">
        /// The options to use to filter the image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8, I16, and Single images. The <format type="italics">source</format> and 
        /// <format type="italics">destination</format> images must be the same type of image.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The source image must have been created with a border 
        /// capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, 
        /// a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is 
        /// not important.
        /// </para>
        /// 	<para>
        /// This function is optimized for MMX.
        /// </para>
        /// </remarks>

        public static void LowPass(VisionImage source, VisionImage destination, LowPassOptions options)
        {
            LowPass(source, destination, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the interpixel variation between the pixel being processed and those pixels surrounding it. 
        /// If the pixel being processed has a variation greater than a specified percentage, it is set to the 
        /// average pixel value as calculated from the neighboring pixels.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="options">
        /// The options to use to filter the image.
        /// </param>
        /// <param name="mask">
        /// The region of the image where the convolution is applied. The method processes only those pixels in the 
        /// image whose corresponding pixels in the mask are non-zero. Pass null or Nothing for this parameter if 
        /// you want to filter the entire image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8, I16, and Single images. The <format type="italics">source</format> and 
        /// <format type="italics">destination</format> images must be the same type of image. 
        /// <format type="italics">mask</format> must be a U8 image.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The source image must have been created with a border 
        /// capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, 
        /// a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is 
        /// not important.
        /// </para>
        /// 	<para>
        /// This function is optimized for MMX.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim options as New LowPassOptions
        /// Dim maskImage As New VisionImage
        ///  
        /// 'Apply a low-pass filter to a portion of the image in Viewer1
        /// 'defined by regions selected on Viewer1.
        /// 'Store the result in i.
        /// Algorithms.RoiToMask (maskImage, imageViewer1.Roi)
        /// Algorithms.LowPass (imageViewer1.Image, i, options, maskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// LowPassOptions options = new LowPassOptions();
        /// VisionImage maskImage = new VisionImage();
        ///     
        /// //Apply a low-pass filter to a portion of the image in Viewer1
        /// //defined by regions selected on Viewer1.
        /// //Store the result in i.
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// Algorithms.LowPass(imageViewer1.Image, i, options, maskImage);
        /// </code>
        /// </example>

        public static void LowPass(VisionImage source, VisionImage destination, LowPassOptions options, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            Utilities.ThrowError(VisionDll.imaqLowPass(destination._image, source._image, options.Width, options.Height, (float)options.Tolerance, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Filters an image using a nonlinear filter. For each pixel, the algorithm takes the neighborhood 
        /// specified by the given filter sizes and replaces the pixel with the median value of the neighborhood.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The source image must have been created with a 
        /// border capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a 
        /// minimum border of 1, a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of 
        /// the destination image is not important.
        /// </para>
        /// </remarks>

        public static void MedianFilter(VisionImage source, VisionImage destination)
        {
            MedianFilter(source, destination, 3, 3, null);
        }
        //==========================================================================================
        /// <summary>
        /// Filters an image using a nonlinear filter. For each pixel, the algorithm takes the neighborhood 
        /// specified by the given filter sizes and replaces the pixel with the median value of the neighborhood.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="width">
        /// The width of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <param name="height">
        /// The height of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The source image must have been created with a 
        /// border capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a 
        /// minimum border of 1, a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of 
        /// the destination image is not important.
        /// </para>
        /// </remarks>

        public static void MedianFilter(VisionImage source, VisionImage destination, int width, int height)
        {
            MedianFilter(source, destination, width, height, null);
        }
        //==========================================================================================
        /// <summary>
        /// Filters an image using a nonlinear filter. For each pixel, the algorithm takes the neighborhood 
        /// specified by the given filter sizes and replaces the pixel with the median value of the neighborhood.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="width">
        /// The width of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <param name="height">
        /// The height of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <param name="mask">
        /// The mask applied to the source image. The mask indicates the region in which the method applies the 
        /// median filter. The method processes only those pixels in the image whose corresponding pixels in 
        /// the mask are non-zero. Pass null or Nothing for this parameter if you want to apply the filter to 
        /// the entire image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8, I16, and Single images. <format type="italics">mask</format> must be a U8 image.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The source image must have been created with a 
        /// border capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a 
        /// minimum border of 1, a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of 
        /// the destination image is not important.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim maskImage As New VisionImage
        ///  
        /// 'Apply a median filter to a portion of the image in Viewer1
        /// 'defined by regions selected on Viewer1.
        /// 'Store the result in i.
        /// Algorithms.RoiToMask (maskImage, imageViewer1.Roi)
        /// Algorithms.MedianFilter (imageViewer1.Image, i, 3, 3, maskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// VisionImage maskImage = new VisionImage();
        ///     
        /// //Apply a median filter to a portion of the image in Viewer1
        /// //defined by regions selected on Viewer1.
        /// //Store the result in i.
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// Algorithms.MedianFilter(imageViewer1.Image, i, 3, 3, maskImage);
        /// </code>
        /// </example>

        public static void MedianFilter(VisionImage source, VisionImage destination, int width, int height, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            Utilities.ThrowError(VisionDll.imaqMedianFilter(destination._image, source._image, width, height, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Filters an image using a nonlinear filter. For each pixel, the algorithm takes the neighborhood specified by the 
        /// given filter sizes and replaces the pixel with the <format type="italics">n</format>th smallest value in the 
        /// neighborhood.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// 	<para>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The connected source image must have been created with a 
        /// border capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, 
        /// a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is not important.
        /// This method is optimized for MMX.</para>
        /// </remarks>

        public static void NthOrder(VisionImage source, VisionImage destination)
        {
            NthOrder(source, destination, 3, 3, 4, null);
        }
        //==========================================================================================
        /// <summary>
        /// Filters an image using a nonlinear filter. For each pixel, the algorithm takes the neighborhood specified by the 
        /// given filter sizes and replaces the pixel with the <format type="italics">n</format>th smallest value in the 
        /// neighborhood.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="width">
        /// The width of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <param name="height">
        /// The height of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.</param>
        /// <remarks>
        /// 	<para>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The connected source image must have been created with a 
        /// border capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, 
        /// a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is not important.
        /// This method is optimized for MMX.</para>
        /// </remarks>

        public static void NthOrder(VisionImage source, VisionImage destination, int width, int height)
        {
            NthOrder(source, destination, width, height, 4, null);
        }
        //==========================================================================================
        /// <summary>
        /// Filters an image using a nonlinear filter. For each pixel, the algorithm takes the neighborhood specified by the 
        /// given filter sizes and replaces the pixel with the <format type="italics">n</format>th smallest value in the 
        /// neighborhood.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="width">
        /// The width of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <param name="height">
        /// The height of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <param name="n">
        /// Specifies which value in the neighborhood to place in the destination. Set <format type="italics">n</format> to 0 
        /// to select the smallest value in the neighborhood, set <format type="italics">n</format> to 1 to select the next 
        /// smallest value, and so on. The default is 4.
        /// </param>
        /// <remarks>
        /// 	<para>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The connected source image must have been created with a 
        /// border capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, 
        /// a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is not important.
        /// This method is optimized for MMX.</para>
        /// </remarks>

        public static void NthOrder(VisionImage source, VisionImage destination, int width, int height, int n)
        {
            NthOrder(source, destination, width, height, n, null);
        }
        //==========================================================================================
        /// <summary>
        /// Filters an image using a nonlinear filter. For each pixel, the algorithm takes the neighborhood specified by the 
        /// given filter sizes and replaces the pixel with the <format type="italics">n</format>th smallest value in the 
        /// neighborhood.
        /// </summary>
        /// <param name="source">
        /// The image to filter.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="width">
        /// The width of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <param name="height">
        /// The height of the rectangular neighborhood around the pixel on which the function operates. This number must be odd. 
        /// The default is 3.
        /// </param>
        /// <param name="n">
        /// Specifies which value in the neighborhood to place in the destination. Set <format type="italics">n</format> to 0 
        /// to select the smallest value in the neighborhood, set <format type="italics">n</format> to 1 to select the next 
        /// smallest value, and so on. The default is 4.
        /// </param>
        /// <param name="mask">
        /// The mask to apply to the source image. It indicates the region of the image in which the method applies the 
        /// NthOrder filter. The method processes only those pixels in the image whose corresponding pixels in the mask are 
        /// non-zero. Pass null or Nothing for this parameter if you want to filter the entire image.
        /// </param>
        /// <remarks>
        /// 	<para>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// The filter modifies the border of the source image. The connected source image must have been created with a 
        /// border capable of supporting the size of the convolution matrix. A 3 x 3 matrix must have a minimum border of 1, 
        /// a 5 x 5 matrix must have a minimum border of 2, and so on. The border size of the destination image is not important.
        /// This method is optimized for MMX.</para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim maskImage As New VisionImage
        ///  
        /// ' Apply a Nth order filter to a portion of the image in Viewer1
        /// ' defined by regions selected on Viewer1.
        /// ' Store the result in i.
        /// Algorithms.RoiToMask (maskImage, imageViewer1.Roi)
        /// Algorithms.NthOrder (imageViewer1.Image, i, 3, 3, 5, maskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// VisionImage maskImage = new VisionImage();
        ///     
        /// // Apply a Nth order filter to a portion of the image in Viewer1
        /// // defined by regions selected on Viewer1.
        /// // Store the result in i.
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// Algorithms.NthOrder(imageViewer1.Image, i, 3, 3, 5, maskImage);
        /// </code>
        /// </example>

        public static void NthOrder(VisionImage source, VisionImage destination, int width, int height, int n, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            Utilities.ThrowError(VisionDll.imaqNthOrderFilter(destination._image, source._image, width, height, n, VisionImage.GetIntPtr(mask)));
        }
        #endregion

        #region Drawing functions
        //==========================================================================================
        /// <summary>Draws a line on an image.
        /// </summary>
        /// <param name="source">The input image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="line">Defines the line to draw.
        /// </param>
        /// <param name="mode">Defines how to draw the line.
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void DrawLine(VisionImage source, VisionImage destination, LineContour line, DrawingMode mode)
        {
            if (source == null)
            {
                // We're about to fail anyway, so just pass anything for the drawValue.
                DrawLine(source, destination, line, mode, new PixelValue(0.0F));
            }
            else
            {
                DrawLine(source, destination, line, mode, new PixelValue(source.Type));
            }
        }
        //==========================================================================================
        /// <summary>Draws a line on an image.
        /// </summary>
        /// <param name="source">The input image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="line">Defines the line to draw.
        /// </param>
        /// <param name="mode">Defines how to draw the line.
        /// </param>
        /// <param name="drawValue">
        /// The pixel value that the method uses for drawing.
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim l As New LineContour
        ///  
        /// ' Draw a line (10,20) - (50,100) on the image in Viewer1.
        /// ' Store the result in i.
        /// l.Start.Initialize(10, 20)
        /// l.End.Initialize(50, 100)
        /// Algorithms.DrawLine (imageViewer.Image, i, l, DrawingMode.DrawValue, New PixelValue(255))
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// LineContour l = new LineContour();
        ///     
        /// // Draw a line (10,20) - (50,100) on the image in Viewer1.
        /// // Store the result in i.
        /// l.Start.Initialize(10, 20);
        /// l.End.Initialize(50, 100);
        /// Algorithms.DrawLine(imageViewer.Image, i, l, DrawingMode.DrawValue, new PixelValue(255));
        /// </code>
        /// </example>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void DrawLine(VisionImage source, VisionImage destination, LineContour line, DrawingMode mode, PixelValue drawValue)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (line == null) { throw new ArgumentNullException("line"); }
            CVI_Point cviStart = new CVI_Point();
            cviStart.ConvertFromExternal(line.Start);
            CVI_Point cviEnd = new CVI_Point();
            cviEnd.ConvertFromExternal(line.End);
            // Since imaqDrawLineOnImage takes HSL and RGB values as a float we have to do some trickery here.
            float valueToPass = Utilities.ConvertPixelValueToFloat(drawValue, source.Type);
            Utilities.ThrowError(VisionDll.imaqDrawLineOnImage(VisionImage.GetIntPtr(destination), source._image, (Int32)mode, cviStart, cviEnd, valueToPass));
        }
        //==========================================================================================
        /// <summary>
        /// Draws an oval on an image.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="oval">
        /// Defines the oval to draw.
        /// </param>
        /// <param name="mode">
        /// Defines how to draw the oval.
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void DrawOval(VisionImage source, VisionImage destination, OvalContour oval, DrawingMode mode)
        {
            if (source == null)
            {
                // We're about to fail anyway, so just pass anything for the drawValue.
                DrawOval(source, destination, oval, mode, new PixelValue(0.0F));
            }
            else
            {
                DrawOval(source, destination, oval, mode, new PixelValue(source.Type));
            }
        }
        //==========================================================================================
        /// <summary>
        /// Draws an oval on an image.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="oval">
        /// Defines the oval to draw.
        /// </param>
        /// <param name="mode">
        /// Defines how to draw the oval.
        /// </param>
        /// <param name="drawValue">
        /// The pixel value that the method uses for drawing.
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim o As New OvalContour
        ///  
        /// ' Draw an oval on the image in Viewer1.
        /// ' Store the result in i.
        /// o.Initialize (10, 20, 50, 100)
        /// Algorithms.DrawOval (imageViewer1.Image, i, o, DrawingMode.PaintInvert)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// OvalContour o = new OvalContour();
        ///     
        /// // Draw an oval on the image in Viewer1.
        /// // Store the result in i.
        /// o.Initialize(10, 20, 50, 100);
        /// Algorithms.DrawOval(imageViewer1.Image, i, o, DrawingMode.PaintInvert);
        /// </code>
        /// </example>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void DrawOval(VisionImage source, VisionImage destination, OvalContour oval, DrawingMode mode, PixelValue drawValue)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (oval == null) { throw new ArgumentNullException("oval"); }
            CVI_Rectangle cviRect = new CVI_Rectangle();
            cviRect.ConvertFromExternal(oval);
            // Since imaqDrawShapeOnImage takes HSL and RGB values as a float we have to do some trickery here.
            float valueToPass = Utilities.ConvertPixelValueToFloat(drawValue, source.Type);
            Utilities.ThrowError(VisionDll.imaqDrawShapeOnImage(VisionImage.GetIntPtr(destination), source._image, cviRect, (Int32)mode, CVI_ShapeMode.Oval, valueToPass));
        }
        //==========================================================================================
        /// <summary>Draws a rectangle on an image.</summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="rectangle">
        /// Defines the rectangle to draw.
        /// </param>
        /// <param name="mode">
        /// Defines how to draw the rectangle.
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void DrawRectangle(VisionImage source, VisionImage destination, RectangleContour rectangle, DrawingMode mode)
        {
            if (source == null)
            {
                // We're about to fail anyway, so just pass anything for the drawValue.
                DrawRectangle(source, destination, rectangle, mode, new PixelValue(0.0F));
            }
            else
            {
                DrawRectangle(source, destination, rectangle, mode, new PixelValue(source.Type));
            }
        }
        //==========================================================================================
        /// <summary>Draws a rectangle on an image.</summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="rectangle">
        /// Defines the rectangle to draw.
        /// </param>
        /// <param name="mode">
        /// Defines how to draw the rectangle.
        /// </param>
        /// <param name="drawValue">The pixel value that the method uses for drawing.
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim r As New RectangleContour
        ///  
        /// ' Draw a rectangle on the image in Viewer1.
        /// ' Store the result in i.
        /// r.Initialize (10, 20, 50, 100)
        /// Algorithms.DrawRectangle (imageViewer1.Image, i, r, DrawingMode.PaintValue, New PixelValue(128))
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// RectangleContour r = new RectangleContour();
        ///     
        /// // Draw a rectangle on the image in Viewer1.
        /// // Store the result in i.
        /// r.Initialize(10, 20, 50, 100);
        /// Algorithms.DrawRectangle(imageViewer1.Image, i, r, DrawingMode.PaintValue, new PixelValue(128));
        /// </code>
        /// </example>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void DrawRectangle(VisionImage source, VisionImage destination, RectangleContour rectangle, DrawingMode mode, PixelValue drawValue)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (rectangle == null) { throw new ArgumentNullException("rectangle"); }
            CVI_Rectangle cviRect = new CVI_Rectangle();
            cviRect.ConvertFromExternal(rectangle);
            // Since imaqDrawShapeOnImage takes HSL and RGB values as a float we have to do some trickery here.
            float valueToPass = Utilities.ConvertPixelValueToFloat(drawValue, source.Type);
            Utilities.ThrowError(VisionDll.imaqDrawShapeOnImage(VisionImage.GetIntPtr(destination), source._image, cviRect, (Int32)mode, CVI_ShapeMode.Rect, valueToPass));
        }
        //==========================================================================================
        /// <summary>Draws text on an image.
        /// </summary>
        /// <param name="source">The input image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="point">The point where the method inserts the text.
        /// </param>
        /// <param name="text">The text that the method draws.
        /// </param>
        /// <returns>
        /// 	<format type="bold">true</format> if the user supplied font name was used. <format type="bold">false</format> if the default font name was used.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 and Rgb32 images.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static bool DrawText(VisionImage source, VisionImage destination, PointContour point, string text)
        {
            return DrawText(source, destination, point, text, new DrawTextOptions());
        }
        //==========================================================================================
        /// <summary>Draws text on an image.
        /// </summary>
        /// <param name="source">The input image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="point">The point where the method inserts the text.
        /// </param>
        /// <param name="text">The text that the method draws.
        /// </param>
        /// <param name="options">Describes how the method draws text.
        /// </param>
        /// <returns>
        /// 	<format type="bold">true</format> if the user supplied font name was used. <format type="bold">false</format> if the default font name was used.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 and RGB32 images. If the image type is U8, DrawTextOptions.Color must be grayscale.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// ' Draw text on the image in Viewer1 at the point (10, 20)
        /// Algorithms.DrawText (imageViewer1.Image, imageViewer1.Image, New PointContour (10, 20), "sample text")
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Draw text on the image in Viewer1 at the point (10, 20)
        /// Algorithms.DrawText (imageViewer1.Image, imageViewer1.Image, new PointContour (10, 20), "sample text");
        /// </code>
        /// </example>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static bool DrawText(VisionImage source, VisionImage destination, PointContour point, string text, DrawTextOptions options)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (point == null) { throw new ArgumentNullException("point"); }
            if (text == null) { throw new ArgumentNullException("text"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_DrawTextOptions cviOptions = new CVI_DrawTextOptions();
            cviOptions.ConvertFromExternal(options);
            CVI_Point cviPoint = new CVI_Point();
            cviPoint.ConvertFromExternal(point);
            Int32 fontNameUsed;
            Utilities.ThrowError(VisionDll.imaqDrawTextOnImage(destination._image, source._image, cviPoint, text, ref cviOptions, out fontNameUsed));
            return fontNameUsed != 0;
        }
        #endregion

        #region Interlacing functions
        //==========================================================================================
        /// <summary>
        /// Combines two field images to create a single frame image.
        /// </summary>
        /// <param name="sourceOdd">
        /// The odd field.
        /// </param>
        /// <param name="sourceEven">
        /// The even field.
        /// </param>
        /// <param name="destination">
        /// The resulting combined image.
        /// </param>
        /// <remarks>
        /// Use this method with all image types.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This example swaps every alternate row in the image on Viewer1
        /// Dim even As New VisionImage
        /// Dim odd As New VisionImage
        ///  
        /// 'First separate the even and odd frames of the image on Viewer1.
        /// Algorithms.InterlaceSeparate (imageViewer1.Image, odd, even)
        ///  
        /// 'Combine the even and odd frames so that the old odd is
        /// 'the new even frame and the old even is the new odd frame.
        /// 'Store the result in the image on Viewer1.
        /// Algorithms.InterlaceCombine (even, odd, imageViewer1.Image)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //This example swaps every alternate row in the image on Viewer1
        ///  
        /// VisionImage even = new VisionImage();
        /// VisionImage odd = new VisionImage();
        ///     
        /// //First separate the even and odd frames of the image on Viewer1.
        /// Algorithms.InterlaceSeparate(imageViewer1.Image, odd, even);
        ///     
        /// //Combine the even and odd frames so that the old odd is
        /// //the new even frame and the old even is the new odd frame.
        /// //Store the result in the image on Viewer1.
        /// Algorithms.InterlaceCombine(even, odd, imageViewer1.Image);
        /// </code>
        /// </example>

        public static void InterlaceCombine(VisionImage sourceOdd, VisionImage sourceEven, VisionImage destination)
        {
            if (sourceOdd == null) { throw new ArgumentNullException("sourceOdd"); }
            sourceOdd.ThrowIfDisposed();
            if (sourceEven == null) { throw new ArgumentNullException("sourceEven"); }
            sourceEven.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqInterlaceCombine(destination._image, sourceOdd._image, sourceEven._image));
        }
        //==========================================================================================
        /// <summary>
        /// Separates a frame image into two field images.
        /// </summary>
        /// <param name="source">
        /// The source frame to separate.
        /// </param>
        /// <param name="destinationOdd">
        /// The image into which the function places the odd field of the frame area.
        /// </param>
        /// <param name="destinationEven">
        /// The image into which the function places the even field of the frame area.
        /// </param>
        /// <remarks>
        /// Use this method with all image types.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This example swaps every alternate row in the image on Viewer1
        /// Dim even As New VisionImage
        /// Dim odd As New VisionImage
        ///  
        /// 'First separate the even and odd frames of the image on Viewer1.
        /// Algorithms.InterlaceSeparate (imageViewer1.Image, odd, even)
        ///  
        /// 'Combine the even and odd frames so that the old odd is
        /// 'the new even frame and the old even is the new odd frame.
        /// 'Store the result in the image on Viewer1.
        /// Algorithms.InterlaceCombine (even, odd, imageViewer1.Image)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //This example swaps every alternate row in the image on Viewer1
        ///  
        /// VisionImage even = new VisionImage();
        /// VisionImage odd = new VisionImage();
        ///     
        /// //First separate the even and odd frames of the image on Viewer1.
        /// Algorithms.InterlaceSeparate(imageViewer1.Image, odd, even);
        ///     
        /// //Combine the even and odd frames so that the old odd is
        /// //the new even frame and the old even is the new odd frame.
        /// //Store the result in the image on Viewer1.
        /// Algorithms.InterlaceCombine(even, odd, imageViewer1.Image);
        /// </code>
        /// </example>

        public static void InterlaceSeparate(VisionImage source, VisionImage destinationOdd, VisionImage destinationEven)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(destinationOdd);
            VisionImage.ThrowIfNonNullAndDisposed(destinationEven);
            Utilities.ThrowError(VisionDll.imaqInterlaceSeparate(source._image, VisionImage.GetIntPtr(destinationOdd), VisionImage.GetIntPtr(destinationEven)));
        }
        #endregion

        #region Morphology functions
        //==========================================================================================
        /// <summary>
        /// Eliminates particles that touch the border of an image.
        /// </summary>
        /// <param name="source">
        /// The source image. If the image has a border, the method sets all border pixel values to 0.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// <para>
        /// 		<format type="italics">source</format> and <format type="italics">destination</format> must be U8 images.
        /// </para>
        /// </remarks>

        public static void RejectBorder(VisionImage source, VisionImage destination)
        {
            RejectBorder(source, destination, Connectivity.Connectivity8);
        }
        //==========================================================================================
        /// <summary>
        /// Eliminates particles that touch the border of an image.
        /// </summary>
        /// <param name="source">
        /// The source image. If the image has a border, the method sets all border pixel values to 0.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are 
        /// touching. Set this parameter to Connectivity4 to use connectivity-4 to determine whether particles 
        /// are touching. The default is Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// <para>
        /// 		<format type="italics">source</format> and <format type="italics">destination</format> must be U8 images.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// 'Reject the particles touching the border of the image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.RejectBorder (imageViewer1.Image, i, Connectivity.Connectivity8)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// //Reject the particles touching the border of the image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.RejectBorder(imageViewer1.Image, i, Connectivity.Connectivity8);
        /// </code>
        /// </example>

        public static void RejectBorder(VisionImage source, VisionImage destination, Connectivity connectivity)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqRejectBorder(destination._image, source._image, (Int32) connectivity));
        }
        //==========================================================================================
        /// <summary>
        /// Fills holes in particles. The method fills the holes with a pixel value of 1.
        /// </summary>
        /// <param name="source">
        /// The image containing particles with holes to fill.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images. The method does not fill areas touching the edge of the image that 
        /// appear to be holes because these areas could be either holes or areas of concavity. This function 
        /// is optimized for MMX.
        /// </remarks>

        public static void FillHoles(VisionImage source, VisionImage destination)
        {
            FillHoles(source, destination, Connectivity.Connectivity8);
        }
        //==========================================================================================
        /// <summary>
        /// Fills holes in particles. The method fills the holes with a pixel value of 1.
        /// </summary>
        /// <param name="source">
        /// The image containing particles with holes to fill.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are 
        /// touching. Set this parameter to Connectivity4 to use connectivity-4 to determine whether particles 
        /// are touching. The default is Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images. The method does not fill areas touching the edge of the image that 
        /// appear to be holes because these areas could be either holes or areas of concavity. This function 
        /// is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Threshold the image on Viewer1 inplace.
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), True, 255)
        ///  
        /// ' Fill the holes in the image on Viewer1 that have connectivity-4 and
        /// ' store the result in i.
        /// Algorithms.FillHoles (imageViewer1.Image, i, Connectivity.Connectivity4)
        ///  
        /// ' Fill the holes in the image on Viewer1 that have connectivity-8.
        /// ' Do this operation in place (Store the result in the image on Viewer1).
        /// Algorithms.FillHoles (imageViewer1.Image, imageViewer1.Image, Connectivity.Connectivity8)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// // Threshold the image on Viewer1 inplace.
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), true, 255);
        ///     
        /// // Fill the holes in the image on Viewer1 that have connectivity-4 and
        /// // store the result in i.
        /// Algorithms.FillHoles(imageViewer1.Image, i, Connectivity.Connectivity4);
        ///     
        /// // Fill the holes in the image on Viewer1 that have connectivity-8.
        /// // Do this operation in place (Store the result in the image on Viewer1).
        /// Algorithms.FillHoles(imageViewer1.Image, imageViewer1.Image, Connectivity.Connectivity8);
        /// </code>
        /// </example>

        public static void FillHoles(VisionImage source, VisionImage destination, Connectivity connectivity)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqFillHoles(destination._image, source._image, connectivity));
        }
        //==========================================================================================
        /// <summary>
        /// Eliminates or keeps particles resistant to a specified number of erosions. The particles that are kept are 
        /// exactly the same as those found in the original source image.
        /// </summary>
        /// <param name="source">
        /// The image in which to remove the particles.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// <para>
        /// Use this method with U8 images.
        /// </para>
        /// 	<para>
        /// The source image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// destination image is not important.
        /// </para>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>

        public static void RemoveParticle(VisionImage source, VisionImage destination)
        {
            RemoveParticle(source, destination, 2, SizeToKeep.KeepLarge, Connectivity.Connectivity8, null);
        }
        //==========================================================================================
        /// <summary>
        /// Eliminates or keeps particles resistant to a specified number of erosions. The particles that are kept are 
        /// exactly the same as those found in the original source image.
        /// </summary>
        /// <param name="source">
        /// The image in which to remove the particles.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="erosions">
        /// The number of erosions to perform. The default is 2.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// <para>
        /// Use this method with U8 images.
        /// </para>
        /// 	<para>
        /// The source image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// destination image is not important.
        /// </para>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>

        public static void RemoveParticle(VisionImage source, VisionImage destination, int erosions)
        {
            RemoveParticle(source, destination, erosions, SizeToKeep.KeepLarge, Connectivity.Connectivity8, null);
        }
        //==========================================================================================
        /// <summary>
        /// Eliminates or keeps particles resistant to a specified number of erosions. The particles that are kept are 
        /// exactly the same as those found in the original source image.
        /// </summary>
        /// <param name="source">
        /// The image in which to remove the particles.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="erosions">
        /// The number of erosions to perform. The default is 2.
        /// </param>
        /// <param name="sizeType">
        /// Specifies whether to keep small particles or large particles.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// <para>
        /// Use this method with U8 images.
        /// </para>
        /// 	<para>
        /// The source image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// destination image is not important.
        /// </para>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>

        public static void RemoveParticle(VisionImage source, VisionImage destination, int erosions, SizeToKeep sizeType)
        {
            RemoveParticle(source, destination, erosions, sizeType, Connectivity.Connectivity8, null);
        }
        //==========================================================================================
        /// <summary>
        /// Eliminates or keeps particles resistant to a specified number of erosions. The particles that are kept are 
        /// exactly the same as those found in the original source image.
        /// </summary>
        /// <param name="source">
        /// The image in which to remove the particles.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="erosions">
        /// The number of erosions to perform. The default is 2.
        /// </param>
        /// <param name="sizeType">
        /// Specifies whether to keep small particles or large particles.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are 
        /// touching. Set this parameter to Connectivity4 to use connectivity-4 to determine whether particles 
        /// are touching. The default is Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// <para>
        /// Use this method with U8 images.
        /// </para>
        /// 	<para>
        /// The source image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// destination image is not important.
        /// </para>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>

        public static void RemoveParticle(VisionImage source, VisionImage destination, int erosions, SizeToKeep sizeType, Connectivity connectivity)
        {
            RemoveParticle(source, destination, erosions, sizeType, connectivity, null);
        }
        //==========================================================================================
        /// <summary>
        /// Eliminates or keeps particles resistant to a specified number of erosions. The particles that are kept are 
        /// exactly the same as those found in the original source image.
        /// </summary>
        /// <param name="source">
        /// The image in which to remove the particles.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="erosions">
        /// The number of erosions to perform. The default is 2.
        /// </param>
        /// <param name="sizeType">
        /// Specifies whether to keep small particles or large particles.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are 
        /// touching. Set this parameter to Connectivity4 to use connectivity-4 to determine whether particles 
        /// are touching. The default is Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <param name="element">
        /// Describes the structuring element applied to the image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// <para>
        /// Use this method with U8 images.
        /// </para>
        /// 	<para>
        /// The source image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// destination image is not important.
        /// </para>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// 'Find the particles resistant to 3 erosions in the image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.RemoveParticle (imageViewer1.Image, i, 3)
        /// 'Remove all particles with connectivity-4 resistant to 3 erosions in i.
        /// 'Do the operation in place (store the result in i)
        /// Algorithms.RemoveParticle (i, i, 3, SizeToKeep.KeepLarge, Connectivity.Connectivity4)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// //Find the particles resistant to 3 erosions in the image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.RemoveParticle(imageViewer1.Image, i, 3);
        /// //Remove all particles with connectivity-4 resistant to 3 erosions in i.
        /// //Do the operation in place (store the result in i)
        /// Algorithms.RemoveParticle(i, i, 3, SizeToKeep.KeepLarge, Connectivity.Connectivity4);
        /// </code>
        /// </example>

        public static void RemoveParticle(VisionImage source, VisionImage destination, int erosions, SizeToKeep sizeType, Connectivity connectivity, StructuringElement element)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (element == null)
            {
                Utilities.ThrowError(VisionDll.imaqSizeFilter(destination._image, source._image, connectivity, erosions, sizeType, IntPtr.Zero));
            }
            else
            {
                CVI_StructuringElement cviElement = new CVI_StructuringElement();
                cviElement.ConvertFromExternal(element);
                try
                {
                    Utilities.ThrowError(VisionDll.imaqSizeFilter(destination._image, source._image, connectivity, erosions, sizeType, ref cviElement));
                }
                finally
                {
                    cviElement.Dispose();
                }
            }
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological transformations to gray level images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="method">
        /// The type of morphological transformation procedure to use.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this 
        /// method.
        /// <para>
        /// Use this method with U8, I16, and Single images. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> must be the same type of image. 
        /// </para>
        /// 	<para>
        /// This function is optimized for MMX.
        /// </para>
        /// </remarks>

        public static void GrayMorphology(VisionImage source, VisionImage destination, MorphologyMethod method)
        {
            GrayMorphology(source, destination, method, null);
        }
        //==========================================================================================
        /// <summary>
        /// Applies morphological transformations to gray level images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="method">
        /// The type of morphological transformation procedure to use.
        /// </param>
        /// <param name="element">
        /// Describes the structuring element applied to the image. The default is 3 x 3.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this 
        /// method.
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format> and <format type="italics">destination</format> must be the same type of 
        /// image. The calculation modifies the border of the source image. The source image for a morphological 
        /// transformation must have been created with a border capable of supporting the size of the structuring element. 
        /// A 3 x 3 structuring element requires a minimal border of 1, a 5 x 5 structuring element requires a minimal border 
        /// of 2, and so on. The border size of the destination image is not important.
        /// </para>
        /// 	<para>
        /// A structuring element must have odd-sized dimensions so that it contains a central pixel. If one of the dimensions for the structuring element is even, the function does not take into account the odd boundary, farthest out on the matrix. For example, if the input structuring element is 6 ?4 (Width = 6 and Height = 4), the actual processing is performed at 5 x 3. Both the sixth line and the fourth row are ignored. The processing speed is correlated with the size of the structuring element. For example, a 3 x 3 structuring element processes nine pixels, and a 5 ?5 structuring element processes 25 pixels.
        /// </para>
        /// 	<para>
        /// This function is optimized for MMX.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim element As New StructuringElement
        ///  
        /// 'Perform a POpen operation on the image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.GrayMorphology (imageViewer1.Image, i, MorphphologyMethod.POpen, element)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// StructuringElement element = new StructuringElement();
        ///     
        /// //Perform a POpen operation on the image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.GrayMorphology(imageViewer1.Image, i, MorphphologyMethod.POpen, element);
        /// </code>
        /// </example>

        public static void GrayMorphology(VisionImage source, VisionImage destination, MorphologyMethod method, StructuringElement element)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (element == null)
            {
                Utilities.ThrowError(VisionDll.imaqGrayMorphology(destination._image, source._image, method, IntPtr.Zero));
            }
            else
            {
                CVI_StructuringElement cviElement = new CVI_StructuringElement();
                cviElement.ConvertFromExternal(element);
                try
                {
                    Utilities.ThrowError(VisionDll.imaqGrayMorphology(destination._image, source._image, method, ref cviElement));
                }
                finally
                {
                    cviElement.Dispose();
                }
            }
        }
        //==========================================================================================
        /// <summary>Computes the convex envelope for each particle in the source image.
        /// </summary>
        /// <param name="source">The image containing the particles whose convex envelopes the method calculates.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16, and floating point images.
        /// </remarks>

        public static void ConvexHull(VisionImage source, VisionImage destination)
        {
            ConvexHull(source, destination, Connectivity.Connectivity8);
        }
        //==========================================================================================
        /// <summary>Computes the convex envelope for each particle in the source image.
        /// </summary>
        /// <param name="source">The image containing the particles whose convex envelopes the method calculates.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="connectivity">Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are touching. Set this parameter to Connectivity4
        /// to use connectivity-4 to determine whether particles are touching. The default is Connectivity8. Refer to the <format type="italics">NI Vision Concepts Help</format> for more information about connectivity.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16, and floating point images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim j As New VisionImage
        /// ' Threshold the image in Viewer1 and store the results in i
        /// Algorithms.Threshold (imageViewer1.Image, i, new Range(128, 255), True, 255)
        /// ' Find the convex envelope of the particles in i.
        /// ' Store the result in j.
        /// Algorithms.ConvexHull (i, j)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// VisionImage j = new VisionImage();
        /// // Threshold the image in Viewer1 and store the results in i
        /// Algorithms.Threshold(imageViewer1.Image, i, new Range(128, 255), true, 255);
        /// // Find the convex envelope of the particles in i.
        /// // Store the result in j.
        /// Algorithms.ConvexHull(i, j);
        /// </code>
        /// </example>

        public static void ConvexHull(VisionImage source, VisionImage destination, Connectivity connectivity)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqConvexHull(destination._image, source._image, connectivity));
        }
        //==========================================================================================
        /// <summary>
        /// Creates a very accurate distance map based on the Danielsson distance algorithm. The method encodes the pixel value of a particle as a function of the distance of the pixel from the particle perimeter. For a faster but less precise algorithm, use the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Distance" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="source">
        /// The image that the method uses to compute the distance map.
        /// </param>
        /// <param name="destination">
        /// The image that contains the result.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8 and I16 images. This method modifies the border of the source image. The border must be at least one pixel wide.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports NationalInstruments.Vision.WindowForms
        ///  
        /// ' This example assumes you have two viewers on your form.
        /// Dim i As New VisionImage
        ///  
        /// ' Threshold the image in Viewer1 and store the results in i.
        /// Algorithms.Threshold (imageViewer1.Image, i, New Range(128, 255), True, 255)
        ///  
        /// ' Create a distance map of i using the Danielsson algorithm.
        /// ' Store the result in the image in Viewer2.
        /// Algorithms.Danielsson (i, imageViewer2.Image)
        ///  
        /// ' View the results on Viewer2 using a binary palette.
        /// imageViewer2.Palette.Type = PaletteType.Binary
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using NationalInstruments.Vision.WindowForms;
        ///  
        /// // This example assumes you have two viewers on your form.
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// // Threshold the image in Viewer1 and store the results in i.
        /// Algorithms.Threshold(imageViewer1.Image, i, new Range(128, 255), true, 255);
        ///     
        /// // Create a distance map of i using the Danielsson algorithm.
        /// // Store the result in the image in Viewer2.
        /// Algorithms.Danielsson(i, imageViewer2.Image);
        ///     
        /// // View the results on Viewer2 using a binary palette.
        /// imageViewer2.Palette.Type = PaletteType.Binary;
        /// </code>
        /// </example>

        public static void Danielsson(VisionImage source, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqDanielssonDistance(destination._image, source._image));
        }
        //==========================================================================================
        /// <summary>
        /// Separates overlapping circular objects and classifies them based on their radius, surface area, and perimeter. 
        /// Starting from a binary image, this method finds the radius and center of the circular objects even when multiple 
        /// circular objects overlap. In addition, this method can trace the circles in the destination image. It 
        /// constructs and uses a Danielsson distance map to determine the radius of each object.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CircleReport" crefType="Unqualified"/> objects 
        /// containing the measurements for circles found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images. This method operates on circles with radii less than or equal to 256 pixels.
        /// </remarks>

        public static Collection<CircleReport> FindCircles(VisionImage source, VisionImage destination)
        {
            return FindCircles(source, destination, new Range(1, 10));
        }
        //==========================================================================================
        /// <summary>
        /// Separates overlapping circular objects and classifies them based on their radius, surface area, and perimeter. 
        /// Starting from a binary image, this method finds the radius and center of the circular objects even when multiple 
        /// circular objects overlap. In addition, this method can trace the circles in the destination image. It 
        /// constructs and uses a Danielsson distance map to determine the radius of each object.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="radiusRange">
        /// The minimum and maximum values for the radius, in pixels, of the circle to detect. Circles with 
        /// radii outside of this value do not appear in the <format type="italics">destination</format> image 
        /// or in the collection of CircleReports. The default value is (1, 10).
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CircleReport" crefType="Unqualified"/> objects 
        /// containing the measurements for circles found.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images. This method operates on circles with radii less than or equal to 
        /// 256 pixels.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'This example requires you to have a ListBox named List1 on the form.
        /// Dim i As New VisionImage
        /// Dim j As New Integer
        ///  
        /// 'Threshold the image in Viewer1 inplace.
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), True, 255)
        ///  
        /// 'Separate and classify circular objects in the image in Viewer1
        /// 'and store the results in i
        /// Dim Reports As Collection(Of CircleReport) = Algorithms.FindCircles (imageViewer1.Image, i, New Range(1, 100))
        ///  
        /// 'Display the areas in the ListBox
        /// List1.Items.Clear()
        /// For Each Report As CircleReport In Reports
        ///     List1.Items.Add(Report.Area)
        /// Next
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // This example requires you to have a ListBox named List1 on the form.
        /// VisionImage i = new VisionImage();
        /// int j;
        ///  
        /// // Threshold the image in Viewer1 inplace.
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), true, 255);
        ///  
        /// // Separate and classify circular objects in the image in Viewer1
        /// // and store the results in i
        /// Collection&lt;CircleReport&gt; reports = Algorithms.FindCircles(imageViewer1.Image, i, new Range(1, 100));
        ///  
        /// // Display the areas in the ListBox
        /// List1.Items.Clear();
        /// foreach (CircleReport report in reports) {
        ///     List1.Items.Add(report);
        /// }
        /// </code>
        /// </example>

        public static Collection<CircleReport> FindCircles(VisionImage source, VisionImage destination, Range radiusRange)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (radiusRange == null) { throw new ArgumentNullException("radiusRange"); }
            int numCircles;
            IntPtr report = VisionDll.imaqFindCircles(destination._image, source._image, (float)radiusRange.Minimum, (float)radiusRange.Maximum, out numCircles);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToCollection<CircleReport, CVI_CircleReport>(report, numCircles, true); 
        }
        //==========================================================================================
        /// <summary>
        /// Labels the particles in a binary image by applying a unique value to all pixels within a particle. 
        /// This value is encoded in 8 or 16 bits, depending on the image type. The method can label 255 particles 
        /// in an 8-bit image and 65,535 particles in a 16-bit image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <returns>
        /// The number of particles that the method detected.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method. Use this method with U8, I16, and Single images.
        /// </remarks>

        public static Int32 Label(VisionImage source, VisionImage destination)
        {
            return Label(source, destination, Connectivity.Connectivity8);
        }
        //==========================================================================================
        /// <summary>
        /// Labels the particles in a binary image by applying a unique value to all pixels within a particle. 
        /// This value is encoded in 8 or 16 bits, depending on the image type. The method can label 255 particles 
        /// in an 8-bit image and 65,535 particles in a 16-bit image.
        /// </summary>
        /// <param name="source">
        /// The source image. The labeling process modifies the border of the source image. The border must be at 
        /// least one pixel wide if you use Connectivity4 or two pixels wide if you use Connectivity8. 
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="connectivity">Set this parameter to Connectivity8 to use connectivity-8 to determine 
        /// whether particles are touching. Set this parameter to Connectivity4 to use connectivity-4 to 
        /// determine whether particles are touching. The default is Connectivity8. Refer to the 
        /// <format type="italics">NI Vision Concepts Help</format> for more information about connectivity.
        /// </param>
        /// <returns>
        /// The number of particles that the method detected.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method. Use this method with U8, I16, and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports NationalInstruments.Vision.WindowsForms
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Threshold the image in Viewer1
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), True, 255) 
        ///  
        /// 'Label the image in Viewer1 and store the result in i
        /// Algorithms.Label (imageViewer1.Image, i)
        ///  
        /// 'Label the image in Viewer1 using Connectivity4
        /// 'and store the result in the image in Viewer2.
        /// 'View the results in Viewer2 using a binary palette
        /// Algorithms.Label (imageViewer1.Image, imageViewer2.Image, Connectivity.Connectivity4)
        /// imageViewer2.Palette.Type = PaletteType.Binary
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using NationalInstruments.Vision.WindowsForms;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Threshold the image in Viewer1
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), true, 255);
        ///     
        /// //Label the image in Viewer1 and store the result in i
        /// Algorithms.Label(imageViewer1.Image, i);
        ///     
        /// //Label the image in Viewer1 using Connectivity4
        /// //and store the result in the image in Viewer2.
        /// //View the results in Viewer2 using a binary palette
        /// Algorithms.Label(imageViewer1.Image, imageViewer2.Image, Connectivity.Connectivity4);
        /// imageViewer2.Palette.Type = PaletteType.Binary;
        /// </code>
        /// </example>

        public static Int32 Label(VisionImage source, VisionImage destination, Connectivity connectivity)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            int numParticles;
            Utilities.ThrowError(VisionDll.imaqLabel2(destination._image, source._image, connectivity, out numParticles));
            return numParticles;
        }
        //==========================================================================================
        /// <summary>
        /// Applies morphological transformations to binary images. 
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operations.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="method">
        /// The type of morphological transformation to use.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the 
        /// image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method 
        /// before using this method.
        /// <para>
        /// The <format type="italics">source</format> and <format type="italics">destination</format> images must be U8 images.
        /// </para>
        /// 	<para>
        /// The <format type="italics">source</format> image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// <format type="italics">destination</format> image is not important.
        /// </para>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>

        public static void Morphology(VisionImage source, VisionImage destination, MorphologyMethod method)
        {
            Morphology(source, destination, method, null);
        }
        //==========================================================================================
        /// <summary>
        /// Applies morphological transformations to binary images. 
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operations.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="method">
        /// The type of morphological transformation to use.
        /// </param>
        /// <param name="element">
        /// The structuring element used in the operation. Pass null or Nothing for  this parameter if you do not 
        /// want a custom structuring element. 
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the 
        /// image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method 
        /// before using this method.
        /// <para>
        /// The <format type="italics">source</format> and <format type="italics">destination</format> images must be U8 images.
        /// </para>
        /// 	<para>
        /// The <format type="italics">source</format> image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// <format type="italics">destination</format> image is not important.
        /// </para>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports NationalInstruments.Vision.WindowsForms
        ///  
        /// Dim i As New VisionImage
        ///     
        /// 'Threshold the image in Viewer1 to make it a binary image.
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range (128, 255), True, 255)
        ///     
        /// 'Perform a POpen operation on the image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.Morphology (imageViewer1.Image, i, MorphologyMethod.POpen)
        ///     
        /// 'Display the result in Viewer2 using a binary palette.
        /// imageViewer2.Palette.Type = PaletteType.Binary
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using NationalInstruments.Vision.WindowsForms;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Threshold the image in Viewer1 to make it a binary image.
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), true, 255);
        ///     
        /// //Perform a POpen operation on the image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.Morphology(imageViewer1.Image, i, MorphologyMethod.POpen);
        ///     
        /// //Display the result in Viewer2 using a binary palette.
        /// imageViewer2.Palette.Type = PaletteType.Binary;
        /// </code>
        /// </example>

        public static void Morphology(VisionImage source, VisionImage destination, MorphologyMethod method, StructuringElement element)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (element == null)
            {
                Utilities.ThrowError(VisionDll.imaqMorphology(destination._image, source._image, method, IntPtr.Zero));
            }
            else
            {
                CVI_StructuringElement cviElement = new CVI_StructuringElement();
                cviElement.ConvertFromExternal(element);
                try
                {
                    Utilities.ThrowError(VisionDll.imaqMorphology(destination._image, source._image, method, ref cviElement));
                }
                finally
                {
                    cviElement.Dispose();
                }
            }
        }
        //==========================================================================================
        /// <summary>
        /// Starting from a labeled image, calculates the zones of influence between particles. Each labeled 
        /// particle dilates/grows until the particles reach their neighbors, at which time this growth is stopped.
        /// Before calling this method, you must label the particles with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Label" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="source">
        /// The image to segment.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8 and I16 images.
        /// </para>
        /// 	<para>
        /// The segmentation modifies the border of the source image. The border must be at least one pixel wide.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports NationalInstruments.Vision.WindowsForms
        ///  
        /// Dim i As New VisionImage
        /// 'Threshold the image in Viewer1.
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), True, 255)
        ///  
        /// 'Label the image in Viewer1 and store the result in i.
        /// Algorithms.Label (imageViewer1.Image, i)
        ///  
        /// 'Segment the image in i.
        /// 'Do the operation in-place (Store the result in i).
        /// Algorithms.Segmentation (i, i)
        ///  
        /// 'View i in Viewer2 using a binary palette.
        /// imageViewer2.Attach (i)
        /// imageViewer2.Palette.Type = PaletteType.Binary
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using NationalInstruments.Vision.WindowsForms;
        ///  
        /// VisionImage i = new VisionImage();
        /// //Threshold the image in Viewer1.
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), true, 255);
        ///  
        /// //Label the image in Viewer1 and store the result in i.
        /// Algorithms.Label(imageViewer1.Image, i);
        ///  
        /// //Segment the image in i.
        /// //Do the operation in-place (Store the result in i).
        /// Algorithms.Segmentation(i, i);
        ///  
        /// //View i in Viewer2 using a binary palette.
        /// imageViewer2.Attach(i);
        /// imageViewer2.Palette.Type = PaletteType.Binary;
        /// </code>
        /// </example>

        public static void Segmentation(VisionImage source, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqSegmentation(destination._image, source._image));
        }
        //==========================================================================================
        /// <summary>
        /// Separates touching particles, particularly small isthmuses found between particles. After performing 
        /// the erosion, the algorithm reconstructs the image. 
        /// </summary>
        /// <param name="source">
        /// The image containing particles to separate.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// All source and destination images must be U8 images.
        /// </para>
        /// 	<para>
        /// The source image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// destination image is not important.
        /// </para>
        /// </remarks>

        public static void Separation(VisionImage source, VisionImage destination)
        {
            Separation(source, destination, 1, null);
        }
        //==========================================================================================
        /// <summary>
        /// Separates touching particles, particularly small isthmuses found between particles. This method 
        /// performs n erosions (n = <format type="italics">erosions</format>), and then reconstructs the 
        /// final image based on the results of the erosion. If an existing isthmus is broken or removed 
        /// during the erosion process, the particles are reconstructed without the isthmus. The reconstructed 
        /// particles, however, have the same size as the initial particles except that they are separated. If 
        /// no isthmus is broken during the erosion process, the particles are reconstructed as they were 
        /// initially found, and no change is made.
        /// </summary>
        /// <param name="source">
        /// The image containing particles to separate.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="erosions">
        /// The number of erosions to perform. The default is 1.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// All source and destination images must be U8 images.
        /// </para>
        /// 	<para>
        /// The source image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// destination image is not important.
        /// </para>
        /// </remarks>

        public static void Separation(VisionImage source, VisionImage destination, int erosions)
        {
            Separation(source, destination, erosions, null);
        }
        //==========================================================================================
        /// <summary>
        /// Separates touching particles, particularly small isthmuses found between particles. This method 
        /// performs n erosions (n = <format type="italics">erosions</format>), and then reconstructs the 
        /// final image based on the results of the erosion. If an existing isthmus is broken or removed 
        /// during the erosion process, the particles are reconstructed without the isthmus. The reconstructed 
        /// particles, however, have the same size as the initial particles except that they are separated. If 
        /// no isthmus is broken during the erosion process, the particles are reconstructed as they were 
        /// initially found, and no change is made.
        /// </summary>
        /// <param name="source">
        /// The image containing particles to separate.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="erosions">
        /// The number of erosions to perform. The default is 1.
        /// </param>
        /// <param name="element">
        /// The structuring element applied to the image. The method applies a 3 x 3 structuring element if you pass 
        /// null or Nothing for this parameter.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// All source and destination images must be U8 images.
        /// </para>
        /// 	<para>
        /// The source image for a morphological transformation must have been created with a border capable of 
        /// supporting the size of the structuring element. A 3 x 3 structuring element requires a minimal border 
        /// of 1, a 5 x 5 structuring element requires a minimal border of 2, and so on. The border size of the 
        /// destination image is not important.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Threshold the image in Viewer1 to make it a binary image.
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), True, 255)
        ///  
        /// 'Separate the particles in the image on Viewer1.
        /// Algorithms.Separation (imageViewer1.Image, i)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// //Threshold the image in Viewer1 to make it a binary image.
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), true, 255);
        ///  
        /// //Separate the particles in the image on Viewer1.
        /// Algorithms.Separation(imageViewer1.Image, i);
        /// </code>
        /// </example>

        public static void Separation(VisionImage source, VisionImage destination, int erosions, StructuringElement element)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (element == null)
            {
                Utilities.ThrowError(VisionDll.imaqSeparation(destination._image, source._image, erosions, IntPtr.Zero));
            }
            else
            {
                CVI_StructuringElement cviElement = new CVI_StructuringElement();
                cviElement.ConvertFromExternal(element);
                try
                {
                    Utilities.ThrowError(VisionDll.imaqSeparation(destination._image, source._image, erosions, ref cviElement));
                }
                finally
                {
                    cviElement.Dispose();
                }
            }
        }
        //==========================================================================================
        /// <summary>
        /// Creates a distance map. The method encodes the pixel value of a particle as a function of the distance of the pixel from the particle border. For a more precise but slower algorithm, use the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Danielsson" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="source">The image that the method uses to compute the distance map.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method. 
        /// All images must be U8 images. The method modifies the border of the source image. 
        /// The border must be at least half as large as the larger of the structuring element 
        /// dimensions.
        /// </remarks>

        public static void Distance(VisionImage source, VisionImage destination)
        {
            Distance(source, destination, null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a distance map. The method encodes the pixel value of a particle as a function of the distance of the pixel from the particle border. For a more precise but slower algorithm, use the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Danielsson" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="source">The image that the method uses to compute the distance map.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="element">
        /// Describes the structuring element applied to the image. The method uses a 3 x 3 structuring element if you do not set this parameter.
        /// For more information on structuring elements, refer to the <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method. 
        /// All images must be U8 images. The method modifies the border of the source image. 
        /// The border must be at least half as large as the larger of the structuring element 
        /// dimensions.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports NationalInstruments.Vision.WindowsForms
        ///  
        /// Dim i As New VisionImage
        /// Dim structuringElement As New StructuringElement
        ///  
        /// ' Threshold the image in Viewer1 and store the results in i.
        /// Algorithms.Threshold (imageViewer1.Image, i, New Range(128, 255), True, 255)
        ///  
        /// ' Create a distance map of i.
        /// ' Store the result in the image in Viewer2.
        /// Algorithms.Distance (i, imageViewer2.Image)
        ///  
        /// ' View the results on Viewer2 using a binary palette.
        /// imageViewer2.Palette.Type = PaletteType.Binary
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using NationalInstruments.Vision.WindowsForms;
        ///  
        /// VisionImage i = new VisionImage();
        /// StructuringElement structuringElement = new StructuringElement();
        ///     
        /// // Threshold the image in Viewer1 and store the results in i.
        /// Algorithms.Threshold(imageViewer1.Image, i, 128, 255, , 255);
        ///     
        /// // Create a distance map of i.
        /// // Store the result in the image in Viewer2.
        /// Algorithms.Distance(i, imageViewer2.Image);
        ///     
        /// // View the results on Viewer2 using a binary palette.
        /// imageViewer2.Palette.Type = PaletteType.Binary;
        /// </code>
        /// </example>

        public static void Distance(VisionImage source, VisionImage destination, StructuringElement element)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (element == null)
            {
                Utilities.ThrowError(VisionDll.imaqSimpleDistance(destination._image, source._image, IntPtr.Zero));
            }
            else
            {
                CVI_StructuringElement cviElement = new CVI_StructuringElement();
                cviElement.ConvertFromExternal(element);
                try
                {
                    Utilities.ThrowError(VisionDll.imaqSimpleDistance(destination._image, source._image, ref cviElement));
                }
                finally
                {
                    cviElement.Dispose();
                }
            }
        }
        //==========================================================================================
        /// <summary>
        /// Starting from a binary image, calculates the skeletons of the particles within an image or the lines 
        /// delineating the zones of influence of the objects (skeleton of an inverse image).
        /// </summary>
        /// <param name="source">
        /// The image whose skeleton the method derives.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8 images.
        /// </para>
        /// 	<para>
        /// The calculation modifies the border of the source image. The border must be at least one pixel wide.
        /// </para>
        /// </remarks>

        public static void Skeleton(VisionImage source, VisionImage destination)
        {
            Skeleton(source, destination, SkeletonMethod.L);
        }
        //==========================================================================================
        /// <summary>
        /// Starting from a binary image, calculates the skeletons of the particles within an image or the lines 
        /// delineating the zones of influence of the objects (skeleton of an inverse image).
        /// </summary>
        /// <param name="source">
        /// The image whose skeleton the method derives.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="method">
        /// The method to calculate the skeleton. The default is L.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image 
        /// using the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before 
        /// using this method.
        /// <para>
        /// Use this method with U8 images.
        /// </para>
        /// 	<para>
        /// The calculation modifies the border of the source image. The border must be at least one pixel wide.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Threshold the image in Viewer1
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), True, 255)
        ///  
        /// 'Perform a skeleton on the image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.Skeleton (imageViewer1.Image, i, SkeletonMethod.Inverse)
        ///  
        /// imageViewer2.Attach (i)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// //Threshold the image in Viewer1
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), true, 255);
        ///  
        /// //Perform a skeleton on the image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.Skeleton(imageViewer1.Image, i, SkeletonMethod.Inverse);
        ///  
        /// imageViewer2.Attach(i);
        /// </code>
        /// </example>

        public static void Skeleton(VisionImage source, VisionImage destination, SkeletonMethod method)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqSkeleton(destination._image, source._image, method));
        }
        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction to gray scale images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="points">
        /// Points is an array of point-coordinates in a source image that define where the reconstruction starts.
        /// </param>
        /// <param name="operation">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>        
        /// <remarks>
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void GrayMorphologyReconstruct(VisionImage source, VisionImage destination, Collection<PointContour> points, MorphologyReconstructOperation method, Roi roi)
        {
            GrayMorphologyReconstruct(source, destination, points, method, null, roi);
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction to gray scale images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="points">
        /// Points is an array of point-coordinates in a source image that define where the reconstruction starts.
        /// </param>
        /// <param name="operation">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>
        /// <param name="element">
        /// Describes the structuring element applied to the image. The default is 3 x 3.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>        
        /// <remarks>
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// <para>
        /// A structuring element must have odd-sized dimensions so that it contains a central pixel. If one of the dimensions for the structuring element is even, then function throws an error.  The processing speed is correlated with the size of the structuring element. For example, a 3 x 3 structuring element processes nine pixels, and a 5 ?5 structuring element processes 25 pixels.
        /// </para>
        /// </remarks>

        public static void GrayMorphologyReconstruct(VisionImage source, VisionImage destination, Collection<PointContour> points, MorphologyReconstructOperation method, StructuringElement element, Roi roi)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (points == null) { throw new ArgumentNullException("points"); }
            CVI_PointFloat[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointFloat>(points);                        
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            
            if (element == null)
            {
                Utilities.ThrowError(VisionDll.imaqGrayMorphologyReconstruct(destination._image, source._image, IntPtr.Zero, cviPoints, cviPoints.Length, method, IntPtr.Zero, cviRoi));
            }
            else
            {
                CVI_StructuringElement cviElement = new CVI_StructuringElement();
                cviElement.ConvertFromExternal(element);
                try
                {
                    Utilities.ThrowError(VisionDll.imaqGrayMorphologyReconstruct(destination._image, source._image, IntPtr.Zero, cviPoints, cviPoints.Length, method, ref cviElement, cviRoi));
                }
                finally
                {
                    cviElement.Dispose();
                }
            }
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction to gray scale images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="points">
        /// Points is an array of point-coordinates in a source image that define where the reconstruction starts.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>              
        /// <remarks>
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void GrayMorphologyReconstruct(VisionImage source, VisionImage destination, Collection<PointContour> points, MorphologyReconstructOperation method)
        {
            GrayMorphologyReconstruct(source, destination, points, method, (StructuringElement)null);
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction to gray scale images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="points">
        /// Points is an array of point-coordinates in a source image that define where the reconstruction starts.
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param> 
        /// <param name="element">
        /// Describes the structuring element applied to the image. The default is 3 x 3.
        /// </param>               
        /// <remarks>
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// <para>
        /// A structuring element must have odd-sized dimensions so that it contains a central pixel. If one of the dimensions for the structuring element is even, then function throws an error.  The processing speed is correlated with the size of the structuring element. For example, a 3 x 3 structuring element processes nine pixels, and a 5 ?5 structuring element processes 25 pixels.
        /// </para>
        /// </remarks>

        public static void GrayMorphologyReconstruct(VisionImage source, VisionImage destination, Collection<PointContour> points, MorphologyReconstructOperation method, StructuringElement element)
        {
            GrayMorphologyReconstruct(source, destination, points, method, element, (Roi)null);            
        }
        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction to gray scale images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="marker">
        /// Image Marker is a reference to the marker image. 
        /// </param>
        /// <param name="element">
        /// Describes the structuring element applied to the image. The default is 3 x 3.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>      
        /// <remarks>
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void GrayMorphologyReconstruct(VisionImage source, VisionImage destination, VisionImage marker, MorphologyReconstructOperation method, Roi roi)
        {
            GrayMorphologyReconstruct(source, destination, marker, method, null, roi);
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction to gray scale images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="marker">
        /// Image Marker is a reference to the marker image. 
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>
        /// <param name="element">
        /// Describes the structuring element applied to the image. The default is 3 x 3.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>      
        /// <remarks>
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// <para>
        /// A structuring element must have odd-sized dimensions so that it contains a central pixel. If one of the dimensions for the structuring element is even, then function throws an error.  The processing speed is correlated with the size of the structuring element. For example, a 3 x 3 structuring element processes nine pixels, and a 5 ?5 structuring element processes 25 pixels.
        /// </para>
        /// </remarks>

        public static void GrayMorphologyReconstruct(VisionImage source, VisionImage destination, VisionImage marker, MorphologyReconstructOperation method, StructuringElement element, Roi roi)
        {
            const int NoInputPoints = 0; 
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (marker == null) { throw new ArgumentNullException("marker"); }
            marker.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);         
            IntPtr cviRoi = Roi.GetIntPtr(roi);

            if (element == null)
            {
                Utilities.ThrowError(VisionDll.imaqGrayMorphologyReconstruct(destination._image, source._image, marker._image, IntPtr.Zero, NoInputPoints, method, IntPtr.Zero, cviRoi));
            }
            else
            {
                CVI_StructuringElement cviElement = new CVI_StructuringElement();
                cviElement.ConvertFromExternal(element);
                try
                {
                    Utilities.ThrowError(VisionDll.imaqGrayMorphologyReconstruct(destination._image, source._image, marker._image, IntPtr.Zero, NoInputPoints , method, ref cviElement, cviRoi));
                }
                finally
                {
                    cviElement.Dispose();
                }
            }
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction to gray scale images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="marker">
        /// Image Marker is a reference to the marker image. 
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>   
        /// <param name="element">
        /// Describes the structuring element applied to the image. The default is 3 x 3.
        /// </param> 
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>
        /// <remarks>
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void GrayMorphologyReconstruct(VisionImage source, VisionImage destination, VisionImage marker, MorphologyReconstructOperation method)
        {
            GrayMorphologyReconstruct(source, destination, marker, method, (StructuringElement)null);
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction to gray scale images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="marker">
        /// Image Marker is a reference to the marker image. 
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>   
        /// <param name="element">
        /// Describes the structuring element applied to the image. The default is 3 x 3.
        /// </param> 
        /// <remarks>
        /// <para>
        /// Use this method with U8, I16, and Single images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// <para>
        /// A structuring element must have odd-sized dimensions so that it contains a central pixel. If one of the dimensions for the structuring element is even, then function throws an error.  The processing speed is correlated with the size of the structuring element. For example, a 3 x 3 structuring element processes nine pixels, and a 5 ?5 structuring element processes 25 pixels.
        /// </para>
        /// </remarks>

        public static void GrayMorphologyReconstruct(VisionImage source, VisionImage destination, VisionImage marker, MorphologyReconstructOperation method, StructuringElement element)
        {
            GrayMorphologyReconstruct(source, destination, marker, method, element, (Roi)null);             
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction on binary images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="points">
        /// Points is an array of point-coordinates in a source image that define where the reconstruction starts.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>
        /// <remarks>
        /// <para>
        /// Use this method with binary images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void MorphologyReconstruct(VisionImage source, VisionImage destination, Collection<PointContour> points, MorphologyReconstructOperation method, Roi roi)
        {
            MorphologyReconstruct(source, destination, points, method, Connectivity.Connectivity8, roi);
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction on binary images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="points">
        /// Points is an array of point-coordinates in a source image that define where the reconstruction starts.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>
        /// <param name="connectivity">
        /// Connectivity  specifies the type of connectivity used by the algorithm for morphological reconstruction.
        /// </param>
        /// <remarks>
        /// <para>
        /// Use this method with binary images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void MorphologyReconstruct(VisionImage source, VisionImage destination, Collection<PointContour> points, MorphologyReconstructOperation method, Connectivity connectivity, Roi roi)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // no ThrowIfDisposed() method for points
            if (points == null) { throw new ArgumentNullException("points"); }

            CVI_PointFloat[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointFloat>(points);            
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            Utilities.ThrowError(VisionDll.imaqMorphologyReconstruct(destination._image, source._image, IntPtr.Zero, cviPoints, cviPoints.Length, method, connectivity, cviRoi));
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction on binary images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="points">
        /// Points is an array of point-coordinates in a source image that define where the reconstruction starts.
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param> 
        /// <remarks>
        /// <para>
        /// Use this method with binary images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void MorphologyReconstruct(VisionImage source, VisionImage destination, Collection<PointContour> points, MorphologyReconstructOperation method)
        {
            MorphologyReconstruct(source, destination, points, method, Connectivity.Connectivity8);
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction on binary images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="points">
        /// Points is an array of point-coordinates in a source image that define where the reconstruction starts.
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param> 
        /// <param name="connectivity">
        /// Connectivity  specifies the type of connectivity used by the algorithm for morphological reconstruction.
        /// </param>
        /// <remarks>
        /// <para>
        /// Use this method with binary images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void MorphologyReconstruct(VisionImage source, VisionImage destination, Collection<PointContour> points, MorphologyReconstructOperation method, Connectivity connectivity)
        {
            MorphologyReconstruct(source, destination, points, method, connectivity, null);            
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction on binary images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="marker">
        /// Image Marker is a reference to the marker image. 
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>  
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>              
        /// <remarks>
        /// <para>
        /// Use this method with binary images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void MorphologyReconstruct(VisionImage source, VisionImage destination, VisionImage marker, MorphologyReconstructOperation method, Roi roi)
        {
            MorphologyReconstruct(source, destination, marker, method, Connectivity.Connectivity8, roi);
        }
//==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction on binary images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="marker">
        /// Image Marker is a reference to the marker image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor defines the region of interest (ROI) within which the morphological reconstruction is performed.
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>
        /// <param name="connectivity">
        /// Connectivity  specifies the type of connectivity used by the algorithm for morphological reconstruction.
        /// </param>
        /// <remarks>
        /// <para>
        /// Use this method with binary images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void MorphologyReconstruct(VisionImage source, VisionImage destination, VisionImage marker, MorphologyReconstructOperation method, Connectivity connectivity, Roi roi)
        {
            const int NoInputPoints = 0;
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (marker == null) { throw new ArgumentNullException("marker"); }
            marker.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);           
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            Utilities.ThrowError(VisionDll.imaqMorphologyReconstruct(destination._image, source._image, marker._image, IntPtr.Zero, NoInputPoints, method, connectivity, cviRoi));
        }

        //==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction on binary images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="marker">
        /// Image Marker is a reference to the marker image. 
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>        
        /// <remarks>
        /// <para>
        /// Use this method with binary images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void MorphologyReconstruct(VisionImage source, VisionImage destination, VisionImage marker, MorphologyReconstructOperation method)
        {
            MorphologyReconstruct(source, destination, marker, method, Connectivity.Connectivity8);
        }
//==========================================================================================
        /// <summary>
        /// Applies morphological reconstruction on binary images.
        /// </summary>
        /// <param name="source">
        /// The image on which the method performs the morphological operation.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="marker">
        /// Image Marker is a reference to the marker image. 
        /// </param>
        /// <param name="method">
        /// The type of morphological reconstruction procedure to use dilate or erode.
        /// </param>
        /// <param name="connectivity">
        /// Connectivity  specifies the type of connectivity used by the algorithm for morphological reconstruction.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this 
        /// method.
        /// <para>
        /// Use this method with binary images.
        /// </para>
        /// 	<para>
        /// 		<format type="italics">source</format>, <format type="italics">destination</format> and <format type="italics">marker</format> 
        /// images must be the same type.
        /// </para>
        /// <para>
        ///  Marker is specified either though the array of points or using the marker image. If none or both of them are specified then and error is thrown.
        /// </para>  
        /// </remarks>

        public static void MorphologyReconstruct(VisionImage source, VisionImage destination, VisionImage marker, MorphologyReconstructOperation method, Connectivity connectivity)
        {
            MorphologyReconstruct(source, destination, marker, method, connectivity, null);            
        }
        #endregion

        #region Logical functions
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise AND or a bitwise NAND between two images.
        /// </summary>
        /// <param name="sourceA">
        /// The input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32. This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' And image i with the image in imageViewer1.
        /// ' Do the operation inplace (store the result in i)
        /// Algorithms.And(imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // And image i with the image in imageViewer1.
        /// // Do the operation inplace (store the result in i)
        /// Algorithms.And(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void And(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqAnd(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise AND between an image and a constant.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="value">
        /// The pixel value.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32. This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' And operation between a constant and an image in imageViewer1.
        /// ' Store the result in i
        /// Algorithms.And(imageViewer1.Image, New PixelValue(1), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // And operation between a constant and an image in imageViewer1.
        /// // Store the result in i
        /// Algorithms.And(imageViewer1.Image, new PixelValue(1), i);
        /// </code>
        /// </example>

        public static void And(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqAndConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>Performs comparison operations between two images.
        /// </summary>
        /// <param name="sourceA">The first input image.
        /// </param>
        /// <param name="sourceB">The second input image.
        /// </param>
        /// <param name="destination">The resulting image. It can be one of the source images.
        /// </param>
        /// <param name="comparisonFunction">The comparison operator to use.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, and Sgl. All input images must have the same image type.
        /// This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// ' Compare a constant to an image in Viewer1.
        /// ' Store the result in i
        /// Algorithms.Compare (imageViewer1.Image, New PixelValue(50), i, ComparisonFunction.ClearEqual)
        /// ' Compare image i to the image in Viewer1.
        /// ' Do the comparison inplace (store the result in i)
        /// Algorithms.Compare (imageViewer1.Image, i, i, ComparisonFunction.ClearGreater)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// // Compare a constant to an image in Viewer1.
        /// // Store the result in i
        /// Algorithms.Compare(imageViewer1.Image, new PixelValue(50), i, ComparisonFunction.ClearEqual);
        /// // Compare image i to the image in Viewer1.
        /// // Do the comparison inplace (store the result in i)
        /// Algorithms.Compare(imageViewer1.Image, i, i, ComparisonFunction.ClearGreater);
        /// 	</code>
        /// </example>

        public static void Compare(VisionImage sourceA, VisionImage sourceB, VisionImage destination, ComparisonFunction comparisonFunction)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqCompare(destination._image, sourceA._image, sourceB._image, comparisonFunction));
        }
        //==========================================================================================
        /// <summary>Performs comparison operations between an image and a constant.
        /// </summary>
        /// <param name="source">The input image.
        /// </param>
        /// <param name="value">The pixel value.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="comparisonFunction">The comparison operator to use.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, and Sgl. This method is optimized for MMX.
        /// </remarks>

        public static void Compare(VisionImage source, PixelValue value, VisionImage destination, ComparisonFunction comparisonFunction)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqCompareConstant(destination._image, source._image, value.CVI_PixValue, comparisonFunction));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise logical difference (A AND NOT B) between two images. 
        /// </summary>
        /// <param name="sourceA">
        /// The first source image. 
        /// </param>
        /// <param name="sourceB">
        /// The second source image, which must be the same type of image as <format type="italics">sourceA</format>.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Find the Logical Difference of image i with the image in Viewer1. 
        /// 'Store the result in i
        /// Algorithms.LogicalDifference (imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Find the Logical Difference of image i with the image in Viewer1. 
        /// //Store the result in i
        /// Algorithms.LogicalDifference(imageViewer1.Image, i, i); 
        /// </code>
        /// </example>

        public static void LogicalDifference(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqLogicalDifference(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Performs a bitwise logical difference (A AND NOT B) between an image and a constant. 
        /// </summary>
        /// <param name="source">
        /// The source image. 
        /// </param>
        /// <param name="value">
        /// The value to AND NOT to the source image.
        /// </param>
        /// <param name="destination">
        /// The destination image. 
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Logical Difference operation between a constant and an image in Viewer1.
        /// 'Store the result in i
        /// Algorithms.LogicalDifference (imageViewer1.Image, New PixelValue (128), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Logical Difference operation between a constant and an image in Viewer1.
        /// //Store the result in i
        /// Algorithms.LogicalDifference(imageViewer1.Image, new PixelValue(128), i);
        /// </code>
        /// </example>

        public static void LogicalDifference(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqLogicalDifferenceConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise NAND between two images.
        /// </summary>
        /// <param name="sourceA">
        /// The input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image, which must be the same type of image as <format type="italics">sourceA</format>
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' NAND image i with the image in imageViewer1.
        /// ' Do the operation inplace (store the result in i)
        /// Algorithms.Nand(imageViewer1.Image, i, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // NAND image i with the image in imageViewer1.
        /// // Do the operation inplace (store the result in i)
        /// Algorithms.Nand(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Nand(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqNand(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise NAND between an image and a constant.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="value">
        /// The pixel value to NAND with the source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>

        public static void Nand(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqNandConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise NOR between two images.
        /// </summary>
        /// <param name="sourceA">
        /// The input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image, which must be the same type of image as <format type="italics">sourceA</format>
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' NOR image i with the image in Viewer1.
        /// ' Do the operation inplace (store the result in i)
        /// Algorithms.Nor (imageViewer1.Image, i, i)
        ///  </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///    
        /// // NOR image i with the image in Viewer1.
        /// // Do the operation inplace (store the result in i)
        /// Algorithms.Nor(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Nor(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqNor(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise NOR between an image and a constant.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="value">
        /// The pixel value to NOR with the source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// ' NOR operation between a constant and an image in Viewer1.
        /// ' Store the result in i
        /// Algorithms.Nor (imageViewer1.Image, New PixelValue (1), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // NOR operation between a constant and an image in Viewer1.
        /// // Store the result in i
        /// Algorithms.Nor(imageViewer1.Image, new PixelValue(1), i);
        /// </code>
        /// </example>

        public static void Nor(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqNorConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise OR between two images.
        /// </summary>
        /// <param name="sourceA">
        /// The input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image, which must be the same type of image as <format type="italics">sourceA</format>
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' OR image i with the image in Viewer1.
        /// ' Do the operation inplace (store the result in i)
        /// Algorithms.Or (imageViewer1.Image, i, i)
        ///  </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///    
        /// // OR image i with the image in Viewer1.
        /// // Do the operation inplace (store the result in i)
        /// Algorithms.Or(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Or(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqOr(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise OR between an image and a constant.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="value">
        /// The pixel value to OR with the source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// ' OR operation between a constant and an image in Viewer1.
        /// ' Store the result in i
        /// Algorithms.Or (imageViewer1.Image, New PixelValue (1), i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // OR operation between a constant and an image in Viewer1.
        /// // Store the result in i
        /// Algorithms.Or(imageViewer1.Image, new PixelValue(1), i);
        /// </code>
        /// </example>

        public static void Or(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqOrConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise XNOR between two images.
        /// </summary>
        /// <param name="sourceA">
        /// The input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image, which must be the same type of image as <format type="italics">sourceA</format>
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' XNOR image i with the image in Viewer1.
        /// ' Do the operation inplace (store the result in i)
        /// Algorithms.Xnor (imageViewer1.Image, i, i)
        ///  </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///    
        /// // XNOR image i with the image in Viewer1.
        /// // Do the operation inplace (store the result in i)
        /// Algorithms.Xnor(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Xnor(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqXnor(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Performs a bitwise XNOR between an image and a constant.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="value">
        /// The pixel value to XNOR with the source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>

        public static void Xnor(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqXnorConstant(destination._image, source._image, value.CVI_PixValue));
        }
        //==========================================================================================
        /// <summary>
        /// Computes a bitwise XOR between two images.
        /// </summary>
        /// <param name="sourceA">
        /// The input image.
        /// </param>
        /// <param name="sourceB">
        /// The second input image, which must be the same type of image as <format type="italics">sourceA</format>
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' XOR image i with the image in Viewer1.
        /// ' Do the operation inplace (store the result in i)
        /// Algorithms.Xor (imageViewer1.Image, i, i)
        ///  </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///    
        /// // XOR image i with the image in Viewer1.
        /// // Do the operation inplace (store the result in i)
        /// Algorithms.Xor(imageViewer1.Image, i, i);
        /// </code>
        /// </example>

        public static void Xor(VisionImage sourceA, VisionImage sourceB, VisionImage destination)
        {
            if (sourceA == null) { throw new ArgumentNullException("sourceA"); }
            sourceA.ThrowIfDisposed();
            if (sourceB == null) { throw new ArgumentNullException("sourceB"); }
            sourceB.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqXor(destination._image, sourceA._image, sourceB._image));
        }
        //==========================================================================================
        /// <summary>
        /// Performs a bitwise XOR between an image and a constant.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="value">
        /// The pixel value to XOR with the source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8, I16, Rgb32, and Hsl32.
        /// </remarks>

        public static void Xor(VisionImage source, PixelValue value, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            // Make sure the PixelValue is of the right type.
            source.ThrowIfWrongType(value);
            Utilities.ThrowError(VisionDll.imaqXorConstant(destination._image, source._image, value.CVI_PixValue));
        }
        #endregion

        #region Particle Analysis functions
        //==========================================================================================
        /// <summary>
        /// Filters (keeps or removes) particles in an image according to their morphological measurements.
        /// </summary>
        /// <param name="source">
        /// The image on which to perform the particle filter.
        /// </param>
        /// <param name="destination">
        /// The result of the processing, containing only the filtered particles.
        /// </param>
        /// <param name="criteria">
        /// The options to apply to the particles in the source image.
        /// </param>
        /// <returns>
        /// The number of particles remaining in the destination image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method. Use 
        /// this method with U8, I16 and Single images.
        /// </remarks>

        public static Int32 ParticleFilter(VisionImage source, VisionImage destination, Collection<ParticleFilterCriteria> criteria)
        {
            return ParticleFilter(source, destination, criteria, new ParticleFilterOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Filters (keeps or removes) particles in an image according to their morphological measurements.
        /// </summary>
        /// <param name="source">
        /// The image on which to perform the particle filter.
        /// </param>
        /// <param name="destination">
        /// The result of the processing, containing only the filtered particles.
        /// </param>
        /// <param name="criteria">
        /// The criteria to apply to the particles in the source image.
        /// </param>
        /// <param name="options">
        /// The options used by the method to filter binary particles.
        /// </param>
        /// <returns>
        /// The number of particles remaining in the destination image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method. Use 
        /// this method with U8, I16 and Single images.
        /// </remarks>

        public static Int32 ParticleFilter(VisionImage source, VisionImage destination, Collection<ParticleFilterCriteria> criteria, ParticleFilterOptions options)
        {
            return ParticleFilter(source, destination, criteria, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Filters (keeps or removes) particles in an image according to their morphological measurements.
        /// </summary>
        /// <param name="source">
        /// The image on which to perform the particle filter.
        /// </param>
        /// <param name="destination">
        /// The result of the processing, containing only the filtered particles.
        /// </param>
        /// <param name="criteria">
        /// The criteria to apply to the particles in the source image.
        /// </param>
        /// <param name="options">
        /// The options used by the method to filter binary particles.
        /// </param>
        /// <param name="roi">
        /// The ROI whose contours a particle must be contained in to avoid being filtered out. If 
        /// <see cref="NationalInstruments.Vision.Analysis.ParticleFilterOptions.RejectBorder" crefType="Unqualified"/> is True in the
        /// <format type="italics">options</format> parameter, any particle touching the border of a contour in roi will also 
        /// be filtered out. Pass null or Nothing for this parameter to filter particles in the entire image based on the 
        /// <format type="italics">criteria</format> parameter. 
        /// </param>
        /// <returns>
        /// The number of particles remaining in the destination image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method. Use 
        /// this method with U8, I16 and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        ///  
        /// Dim i As New VisionImage
        /// Dim criteria As New Collection(Of ParticleFilterCriteria)
        /// ' Select particles that contain less than 100 pixels or
        /// ' more than 300 pixels and remove them.
        /// criteria.Add (New ParticleFilterCriteria (MeasurementType.Area, New Range(100,300))
        /// ' Perform the particle filter operation and
        /// ' store the result in i
        /// Algorithms.ParticleFilter (imageViewer1.Image, i, criteria)
        /// </code>
        /// 	<code lang="C#">
        /// [C#]
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage i = new VisionImage();
        /// Collection&lt;ParticleFilterCriteria&gt; criteria = new Collection&lt;ParticleFilterCriteria&gt;();
        /// // Select particles that contain less than 100 pixels or
        /// // more than 300 pixels and remove them.
        /// criteria.Add(new ParticleFilterCriteria(MeasurementType.Area, new Range(100, 300));
        /// // Perform the particle filter operation and
        /// // store the result in i
        /// Algorithms.ParticleFilter(imageViewer1.Image, i, criteria);
        /// </code>
        /// </example>

        public static Int32 ParticleFilter(VisionImage source, VisionImage destination, Collection<ParticleFilterCriteria> criteria, ParticleFilterOptions options, Roi roi)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (criteria == null) { throw new ArgumentNullException("criteria"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            CVI_ParticleFilterCriteria2[] cviCriteria = Utilities.ConvertCollectionToArray<ParticleFilterCriteria, CVI_ParticleFilterCriteria2>(criteria);
            CVI_ParticleFilterOptions2 cviOptions = new CVI_ParticleFilterOptions2();
            cviOptions.ConvertFromExternal(options);
            Int32 numParticles;
            Utilities.ThrowError(VisionDll.imaqParticleFilter4(destination._image, source._image, cviCriteria, cviCriteria.Length, ref cviOptions, cviRoi, out numParticles));
            return numParticles;
        }

        //==========================================================================================
        /// <summary>
        /// Takes a measurement on all particles in a binary image.
        /// </summary>
        /// <param name="image">
        /// The image containing the particle to get information about. 
        /// </param>
        /// <param name="measurements">
        /// The measurement to make on the particle. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ParticleMeasurementsReport" crefType="Unqualified"/> object containing the value of the measurements on each particle in the image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images.
        /// </remarks>

        public static ParticleMeasurementsReport ParticleMeasurements(VisionImage image, Collection<MeasurementType> measurements)
        {
            return ParticleMeasurements(image, measurements, Connectivity.Connectivity8, ParticleMeasurementsCalibrationMode.Pixel);
        }
        //==========================================================================================
        /// <summary>
        /// Takes a measurement on all particles in a binary image.
        /// </summary>
        /// <param name="image">
        /// The image containing the particle to get information about. 
        /// </param>
        /// <param name="measurements">
        /// The measurement to make on the particle. 
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are touching. Set this
        /// parameter to Connectivity4 to use connectivity-4 to determine whether particles are touching. The default is 
        /// Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ParticleMeasurementsReport" crefType="Unqualified"/> object containing the value of the measurements on each particle in the image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images.
        /// </remarks>

        public static ParticleMeasurementsReport ParticleMeasurements(VisionImage image, Collection<MeasurementType> measurements, Connectivity connectivity)
        {
            return ParticleMeasurements(image, measurements, connectivity, ParticleMeasurementsCalibrationMode.Pixel);
        }
        //==========================================================================================
        /// <summary>
        /// Takes a measurement on all particles in a binary image.
        /// </summary>
        /// <param name="image">
        /// The image containing the particle to get information about. 
        /// </param>
        /// <param name="measurements">
        /// The measurement to make on the particle. 
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are touching. Set this
        /// parameter to Connectivity4 to use connectivity-4 to determine whether particles are touching. The default is 
        /// Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <param name="calibrationMode">
        /// Specifies whether to return pixel measurements, real-world measurements, or both.  The default is Pixel.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ParticleMeasurementsReport" crefType="Unqualified"/> object containing the value of the measurements on each particle in the image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim Measurements As New Collection(Of MeasurementType)
        /// ' Measure area, perimeter, and Heywood circularity factor in pixel and real-world measurements.
        /// Measurements.Add (MeasurementType.Area)
        /// Measurements.Add (MeasurementType.Perimeter)
        /// Measurements.Add (MeasurementType.HeywoodCircularityFactor)
        /// Dim Report As ParticleMeasurementsReport = Algorithms.ParticleMeasurements (imageViewer1.Image, 
        /// Measurements, Connectivity.Connectivity8, ParticleMeasurementsCalibrationMode.Both)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// Collection&lt;MeasurementType&gt; measurements = new Collection&lt;MeasurementType&gt;();
        /// // Measure area, perimeter, and Heywood circularity factor in pixel and real-world measurements.
        /// measurements.Add(MeasurementType.Area);
        /// measurements.Add(MeasurementType.Perimeter);
        /// measurements.Add(MeasurementType.HeywoodCircularityFactor);
        /// ParticleMeasurementsReport report = Algorithms.ParticleMeasurements(imageViewer1.Image, 
        /// measurements, Connectivity.Connectivity8, ParticleMeasurementsCalibrationMode.Both);
        /// </code>
        /// </example>

        public static ParticleMeasurementsReport ParticleMeasurements(VisionImage image, Collection<MeasurementType> measurements, Connectivity connectivity, ParticleMeasurementsCalibrationMode calibrationMode)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (measurements == null) { throw new ArgumentNullException("measurements"); }
            MeasurementType[] cviMeasurements = Utilities.ConvertCollectionToArray(measurements);
            // Since we're using the real CVI interface, we have to call imaqCountParticles() first.
            int unusedNumParticles;
            Utilities.ThrowError(VisionDll.imaqCountParticles(image._image, connectivity, out unusedNumParticles));
            IntPtr report = VisionDll.imaqMeasureParticles(image._image, calibrationMode, cviMeasurements, (IntPtr)cviMeasurements.Length);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ParticleMeasurementsReport, CVI_MeasureParticlesReport>(report, true);
        }

        //==========================================================================================
        /// <summary>
        /// Detects and returns commonly-used properties of particles in a binary image.
        /// </summary>
        /// <param name="image">
        /// The image on which to get information about the particles.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ParticleReport" crefType="Unqualified"/> containing information about the particles in the image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images.
        /// </remarks>

        public static Collection<ParticleReport> ParticleReport(VisionImage image)
        {
            return ParticleReport(image, Connectivity.Connectivity8, false);
        }
        //==========================================================================================
        /// <summary>
        /// Detects and returns commonly-used properties of particles in a binary image.
        /// </summary>
        /// <param name="image">
        /// The image on which to get information about the particles.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are touching. Set this
        /// parameter to Connectivity4 to use connectivity-4 to determine whether particles are touching. The default is 
        /// Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ParticleReport" crefType="Unqualified"/> containing information about the particles in the image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images.
        /// </remarks>

        public static Collection<ParticleReport> ParticleReport(VisionImage image, Connectivity connectivity)
        {
            return ParticleReport(image, connectivity, false);
        }
        //==========================================================================================
        /// <summary>
        /// Detects and returns commonly-used properties of particles in a binary image.
        /// </summary>
        /// <param name="image">
        /// The image on which to get information about the particles.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are touching. Set this
        /// parameter to Connectivity4 to use connectivity-4 to determine whether particles are touching. The default is 
        /// Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <param name="calibrated">
        /// Specifies whether to take calibrated measurements on the particles in the image. The default is False.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ParticleReport" crefType="Unqualified"/> containing information about the particles in the image.
        /// </returns>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using 
        /// the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8, I16 and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// ' Threshold the image in viewer1 in place.
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), True, 255)
        ///  
        /// ' Do the particle report.
        /// Dim Reports As Collection(Of ParticleReport) = Algorithms.ParticleReport (ImageViewer1.Image)
        ///  
        /// 'For each particle, display Area and the Bounding Rectangle.
        /// For Each Report As ParticleReport In Reports
        ///     Dim Text As String = "A: " + CStr(Report.Area)
        ///     imageViewer1.Image.Overlays.Default.AddRectangle (Report.BoundingRect, Rgb32Value.GreenColor, DrawingMode.DrawValue)
        ///     Dim TextOptions As New OverlayTextOptions("Arial", 16, HorizontalTextAlignment.Center)
        ///     Dim RotatedRect As New RotatedRectangleContour(Report.BoundingRect)
        ///     imageViewer1.Image.Overlays.Default.AddText (Text, RotatedRect.Center, Rgb32Value.BlueColor, TextOptions)
        /// Next
        ///  
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Threshold the image in viewer1 in place.
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, New Range(128, 255), true, 255);
        ///  
        /// // Do the particle report.
        /// Collection&lt;ParticleReport&gt; reports = Algorithms.ParticleReport(ImageViewer1.Image);
        ///  
        /// // For each particle, display Area and the Bounding Rectangle.
        /// foreach (ParticleReport report in reports) {
        ///     string text = "A: " + report.Area.ToString();
        ///     imageViewer1.Image.Overlays.Default.AddRectangle(report.BoundingRect, Rgb32Value.GreenColor, DrawingMode.DrawValue);
        ///     OverlayTextOptions textOptions = new OverlayTextOptions("Arial", 16, HorizontalTextAlignment.Center);
        ///     RotatedRectangleContour rotatedRect = new RotatedRectangleContour(report.BoundingRect);
        ///     imageViewer1.Image.Overlays.Default.AddText(text, rotatedRect.Center, Rgb32Value.BlueColor, textOptions);
        /// }
        /// </code>
        /// </example>

        public static Collection<ParticleReport> ParticleReport(VisionImage image, Connectivity connectivity, bool calibrated)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Array1D array1D;
            VisionDll.Priv_InitArray1D(out array1D);
            Utilities.ThrowError(VisionDll.Priv_Particle(image._image, (Int32)connectivity, calibrated ? 1 : 0, out array1D));
            Collection<ParticleReport> report = Utilities.ConvertArray1DToCollection<ParticleReport, VB_ParticleReport>(array1D);
            VisionDll.Priv_DisposeArray1DBytes(ref array1D);
            return report;
        }
        #endregion

        #region Analytic Geometry functions
        //==========================================================================================
        /// <summary>
        /// Builds a reference for any arbitrary coordinate system with respect to the image plane. The reference of the coordinate system is specified as the position of the origin of the coordinate system, the orientation of its x-axis with respect to that of the image plane, and the direction of the axis.
        /// </summary>
        /// <param name="points">
        /// A collection of points that define the coordinate system. If two points are specified, these points are assumed to lie along the 
        /// x-axis of the coordinate system and the first point is used as the origin of the coordinate axis, as shown in the following figure.
        /// <para>
        /// 		<img src="TwoPointCoordSystem.gif"/>
        /// 	</para>
        /// 	<para>
        /// If three points are specified, the first two points are assumed to be along the x-axis, and the third point is assumed to be on the 
        /// y-axis of the coordinate system, as shown in the following figure.
        /// </para>
        /// 	<para>
        /// 		<img src="ThreePointCoordSystem.gif"/>
        /// 	</para>
        /// </param>
        /// <returns>
        /// The resulting coordinate system. On failure, an exception is thrown.
        /// </returns>

        public static CoordinateSystem BuildCoordinateSystem(Collection<PointContour> points)
        {
            return BuildCoordinateSystem(points, AxisOrientation.Direct);
        }
        //==========================================================================================
        /// <summary>
        /// Builds a reference for any arbitrary coordinate system with respect to the image plane. The reference of the coordinate system is specified as the position of the origin of the coordinate system, the orientation of its x-axis with respect to that of the image plane, and the direction of the axis.
        /// </summary>
        /// <param name="points">
        /// A collection of points that define the coordinate system. If two points are specified, these points are assumed to lie along the 
        /// x-axis of the coordinate system and the first point is used as the origin of the coordinate axis, as shown in the following figure.
        /// <para>
        /// 		<img src="TwoPointCoordSystem.gif"/>
        /// 	</para>
        /// 	<para>
        /// If three points are specified, the first two points are assumed to be along the x-axis, and the third point is assumed to be on the 
        /// y-axis of the coordinate system, as shown in the following figure.
        /// </para>
        /// 	<para>
        /// 		<img src="ThreePointCoordSystem.gif"/>
        /// 	</para>
        /// </param>
        /// <returns>
        /// The resulting coordinate system. On failure, an exception is thrown.
        /// </returns>

        public static CoordinateSystem BuildCoordinateSystem(Roi points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return BuildCoordinateSystem(Utilities.ConvertRoiToPoints(points), AxisOrientation.Direct);
        }
        //==========================================================================================
        /// <summary>
        /// Builds a reference for any arbitrary coordinate system with respect to the image plane. The reference of the coordinate system is specified as the position of the origin of the coordinate system, the orientation of its x-axis with respect to that of the image plane, and the direction of the axis.
        /// </summary>
        /// <param name="points">
        /// A collection of points that define the coordinate system. If two points are specified, these points are assumed to lie along the 
        /// x-axis of the coordinate system and the first point is used as the origin of the coordinate axis, as shown in the following figure.
        /// <para>
        /// 		<img src="TwoPointCoordSystem.gif"/>
        /// 	</para>
        /// 	<para>
        /// If three points are specified, the first two points are assumed to be along the x-axis, and the third point is assumed to be on the 
        /// y-axis of the coordinate system, as shown in the following figure.
        /// </para>
        /// 	<para>
        /// 		<img src="ThreePointCoordSystem.gif"/>
        /// 	</para>
        /// </param>
        /// <param name="orientation">
        /// Specifies the direction of the coordinate system. The <format type="italics">orientation</format> can be <see cref="NationalInstruments.Vision.AxisOrientation.Direct" crefType="Unqualified"/> or <see cref="NationalInstruments.Vision.AxisOrientation.Indirect" crefType="Unqualified"/>.
        /// </param>
        /// <returns>
        /// The resulting coordinate system. On failure, an exception is thrown.
        /// </returns>

        public static CoordinateSystem BuildCoordinateSystem(Roi points, AxisOrientation orientation)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return BuildCoordinateSystem(Utilities.ConvertRoiToPoints(points), orientation);
        }
        //==========================================================================================
        /// <summary>
        /// Builds a reference for any arbitrary coordinate system with respect to the image plane. The reference of the coordinate system is specified as the position of the origin of the coordinate system, the orientation of its x-axis with respect to that of the image plane, and the direction of the axis.
        /// </summary>
        /// <param name="points">
        /// A collection of points that define the coordinate system. If two points are specified, these points are assumed to lie along the 
        /// x-axis of the coordinate system and the first point is used as the origin of the coordinate axis, as shown in the following figure.
        /// <para>
        /// 		<img src="TwoPointCoordSystem.gif"/>
        /// 	</para>
        /// 	<para>
        /// If three points are specified, the first two points are assumed to be along the x-axis, and the third point is assumed to be on the 
        /// y-axis of the coordinate system, as shown in the following figure.
        /// </para>
        /// 	<para>
        /// 		<img src="ThreePointCoordSystem.gif"/>
        /// 	</para>
        /// </param>
        /// <param name="orientation">
        /// Specifies the direction of the coordinate system. The <format type="italics">orientation</format> can be <see cref="NationalInstruments.Vision.AxisOrientation.Direct" crefType="Unqualified"/> or <see cref="NationalInstruments.Vision.AxisOrientation.Indirect" crefType="Unqualified"/>.
        /// </param>
        /// <returns>
        /// The resulting coordinate system. On failure, an exception is thrown.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim points1 As New Collection(Of PointContour)
        /// Dim points2 As New Collection(Of PointContour)
        /// Dim coordinateSystem1 As New CoordinateSystem
        /// Dim coordinateSystem2 As New CoordinateSystem
        ///  
        /// 'Build a direct coordinate system whose origin is at (0,0)
        /// 'and the x-axis passes through (10,10)
        /// points1.Add(New PointContour(0, 0))
        /// points1.Add(New PointContour(10, 10))
        /// coordinateSystem1 = Algorithms.BuildCoordinateSystem (points1, AxisOrientation.Direct)
        ///  
        /// 'Build an indirect coordinate system whose
        /// 'x-axis passes through (1,5) and (10,-25) and
        /// 'y-axis passes through (20,2)
        /// points2.Add(New PointContour(1,5))
        /// points2.Add(New PointContour(10,-25))
        /// points2.Add(New PointContour(20,2))
        /// coordinateSystem2 = Algorithms.BuildCoordinateSystem (points2, AxisOrientation.Indirect)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// Collection&lt;PointContour&gt; points1 = new Collection&lt;PointContour&gt;();
        /// Collection&lt;PointContour&gt; points2 = new Collection&lt;PointContour&gt;();
        /// CoordinateSystem coordinateSystem1, coordinateSystem2;
        ///  
        /// // Build a direct coordinate system whose origin is at (0,0)
        /// // and the x-axis passes through (10,10)
        /// points1.Add(new PointContour(0, 0))
        /// points1.Add(new PointContour(10, 10))
        /// coordinateSystem1 = Algorithms.BuildCoordinateSystem (points1, AxisOrientation.Direct)
        ///  
        /// // Build an indirect coordinate system whose
        /// // x-axis passes through (1,5) and (10,-25) and
        /// // y-axis passes through (20,2)
        /// points2.Add(new PointContour(1,5))
        /// points2.Add(new PointContour(10,-25))
        /// points2.Add(new PointContour(20,2))
        /// coordinateSystem2 = Algorithms.BuildCoordinateSystem (points2, AxisOrientation.Indirect)
        /// </code>
        /// </example>

        public static CoordinateSystem BuildCoordinateSystem(Collection<PointContour> points, AxisOrientation orientation)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            // Make sure we have enough points.
            if (points.Count < 2) { throw new VisionException(ErrorCode.InsfPoints); }
            CVI_Point[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_Point>(points);
            CVI_CoordinateSystem cviSystem = new CVI_CoordinateSystem();
            Utilities.ThrowError(VisionDll.imaqBuildCoordinateSystem(cviPoints, (cviPoints.Length > 2) ? CVI_ReferenceMode.CoordXY : CVI_ReferenceMode.CoordOriginX, orientation, out cviSystem));
            return cviSystem.ConvertToExternal();
        }
        //==========================================================================================
        /// <summary>
        /// Finds the circle that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The coordinates of the points to use for the fit. This collection must have at least three items.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitCircleReport" crefType="Unqualified"/> object containing information
        /// about the circle that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// You must supply at least three non-colinear points to fit to the edge of the circle.
        /// </remarks>

        public static FitCircleReport FitCircle(Collection<PointContour> points)
        {
            return FitCircle(points, new FitCircleOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds the circle that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The ROI containing the points to use for the fit.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitCircleReport" crefType="Unqualified"/> object containing information
        /// about the circle that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// You must supply at least three non-colinear points to fit to the edge of the circle.
        /// </remarks>

        public static FitCircleReport FitCircle(Roi points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return FitCircle(Utilities.ConvertRoiToPoints(points), new FitCircleOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds the circle that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The ROI containing the points to use for the fit.
        /// </param>
        /// <param name="options">
        /// The options the method uses.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitCircleReport" crefType="Unqualified"/> object containing information
        /// about the circle that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// You must supply at least three non-colinear points to fit to the edge of the circle.
        /// </remarks>

        public static FitCircleReport FitCircle(Roi points, FitCircleOptions options)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return FitCircle(Utilities.ConvertRoiToPoints(points), options);
        }
        //==========================================================================================
        /// <summary>
        /// Finds the circle that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The coordinates of the points to use for the fit. This collection must have at least three items.
        /// </param>
        /// <param name="options">
        /// The options the method uses.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitCircleReport" crefType="Unqualified"/> object containing information
        /// about the circle that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// You must supply at least three non-colinear points to fit to the edge of the circle.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim points As New Collection(Of PointContour)
        ///  
        /// 'Find the circle that passes through
        /// '(100,100), (100,-100), (-100,100)
        /// points.Add (New PointContour (100, 100))
        /// points.Add (New PointContour (100, -100))
        /// points.Add (New PointContour (-100, 100))
        ///  
        /// Dim Report As FitCircleReport = Algorithms.FitCircle (points)
        /// 'Overlay the circle on the image in Viewer1.
        /// imageViewer1.Image.Overlays.Default.AddOval (Report.Circle)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// Collection&lt;PointContour&gt; points = new Collection&lt;PointContour&gt;();
        ///  
        /// // Find the circle that passes through
        /// // (100,100), (100,-100), (-100,100)
        /// points.Add(new PointContour(100, 100));
        /// points.Add(new PointContour(100, -100));
        /// points.Add(new PointContour(-100, 100));
        ///  
        /// FitCircleReport report = Algorithms.FitCircle(points);
        /// // Overlay the circle on the image in Viewer1.
        /// imageViewer1.Image.Overlays.Default.AddOval(report.Circle);
        /// </code>
        /// </example>

        public static FitCircleReport FitCircle(Collection<PointContour> points, FitCircleOptions options)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_PointFloat[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointFloat>(points);
            CVI_FitCircleOptions cviOptions = new CVI_FitCircleOptions();
            cviOptions.ConvertFromExternal(options);
            IntPtr report = VisionDll.imaqFitCircle2(cviPoints, cviPoints.Length, ref cviOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<FitCircleReport, CVI_BestCircle2, Collection<PointContour>>(report, points, true);
        }
        //==========================================================================================
        /// <summary>
        /// Finds the ellipse that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The coordinates of the points to use for the fit. This collection must have at least six items.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitEllipseReport" crefType="Unqualified"/> object containing information
        /// about the ellipse that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// You must supply at least six non-colinear points to fit to the edge of the ellipse.
        /// </remarks>

        public static FitEllipseReport FitEllipse(Collection<PointContour> points)
        {
            return FitEllipse(points, new FitEllipseOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds the ellipse that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The ROI containing the points to use for the fit.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitEllipseReport" crefType="Unqualified"/> object containing information
        /// about the ellipse that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// You must supply at least six non-colinear points to fit to the edge of the ellipse.
        /// </remarks>

        public static FitEllipseReport FitEllipse(Roi points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return FitEllipse(Utilities.ConvertRoiToPoints(points), new FitEllipseOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds the ellipse that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The ROI containing the points to use for the fit.
        /// </param>
        /// <param name="options">
        /// The options the method uses.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitEllipseReport" crefType="Unqualified"/> object containing information
        /// about the ellipse that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// You must supply at least six non-colinear points to fit to the edge of the ellipse.
        /// </remarks>

        public static FitEllipseReport FitEllipse(Roi points, FitEllipseOptions options)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return FitEllipse(Utilities.ConvertRoiToPoints(points), options);
        }
        //==========================================================================================
        /// <summary>
        /// Finds the ellipse that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The coordinates of the points to use for the fit. This collection must have at least six items.
        /// </param>
        /// <param name="options">
        /// The options the method uses.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitEllipseReport" crefType="Unqualified"/> object containing information
        /// about the ellipse that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// You must supply at least six non-colinear points to fit to the edge of the ellipse.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim points As New Collection(Of PointContour)
        ///  
        /// 'Find the ellipse that passes through
        /// '(1,1), (-1,1), (1,-1), (-1,-1), (1.414,0), (0,1.414)
        /// points.Add (New PointContour (1, 1))
        /// points.Add (New PointContour (-1, 1))
        /// points.Add (New PointContour (1, -1))
        /// points.Add (New PointContour (-1, -1))
        /// points.Add (New PointContour (1.414, 0))
        /// points.Add (New PointContour (0, 1.414))
        ///  
        /// Dim Report As FitEllipseReport = Algorithms.FitEllipse (points)
        /// 'Overlay the axes of the ellipse on the image in Viewer1.
        /// imageViewer1.Image.Overlays.Default.AddLine (Report.MajorAxis)
        /// imageViewer1.Image.Overlays.Default.AddLine (Report.MinorAxis)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// Collection&lt;PointContour&gt; points = new Collection&lt;PointContour&gt;();
        ///  
        /// // Find the ellipse that passes through
        /// // (1,1), (-1,1), (1,-1), (-1,-1), (1.414,0), (0,1.414)
        /// points.Add(new PointContour(1, 1));
        /// points.Add(new PointContour(-1, 1));
        /// points.Add(new PointContour(1, -1));
        /// points.Add(new PointContour(-1, -1));
        /// points.Add(new PointContour(1.414, 0));
        /// points.Add(new PointContour(0, 1.414));
        ///  
        /// FitEllipseReport report = Algorithms.FitEllipse(points);
        /// // Overlay the axes of the ellipse on the image in Viewer1.
        /// imageViewer1.Image.Overlays.Default.AddLine(report.MajorAxis);
        /// imageViewer1.Image.Overlays.Default.AddLine(report.MinorAxis);
        /// </code>
        /// </example>

        public static FitEllipseReport FitEllipse(Collection<PointContour> points, FitEllipseOptions options)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_PointFloat[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointFloat>(points);
            CVI_FitEllipseOptions cviOptions = new CVI_FitEllipseOptions();
            cviOptions.ConvertFromExternal(options);
            IntPtr report = VisionDll.imaqFitEllipse2(cviPoints, cviPoints.Length, ref cviOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<FitEllipseReport, CVI_BestEllipse2, Collection<PointContour>>(report, points, true);
        }
        //==========================================================================================
        /// <summary>
        /// Finds the line that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The coordinates of the points to use for the fit. This collection must contain two or more point items 
        /// corresponding to two or more points on the line.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitLineReport" crefType="Unqualified"/> object containing information
        /// about the line that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// The resulting line may only take into account a subset of the input points.
        /// </remarks>

        public static FitLineReport FitLine(Collection<PointContour> points)
        {
            return FitLine(points, new FitLineOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds the line that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The ROI containing the points to use for the fit. This ROI must contain two or more PointContours
        /// corresponding to two or more points on the line.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitLineReport" crefType="Unqualified"/> object containing information
        /// about the line that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// The resulting line may only take into account a subset of the input points.
        /// </remarks>

        public static FitLineReport FitLine(Roi points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return FitLine(Utilities.ConvertRoiToPoints(points), new FitLineOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Finds the line that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The ROI containing the points to use for the fit. This ROI must contain two or more PointContours
        /// corresponding to two or more points on the line.
        /// </param>
        /// <param name="options">
        /// The options the method uses.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitLineReport" crefType="Unqualified"/> object containing information
        /// about the line that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// The resulting line may only take into account a subset of the input points.
        /// </remarks>

        public static FitLineReport FitLine(Roi points, FitLineOptions options)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return FitLine(Utilities.ConvertRoiToPoints(points), options);
        }
        //==========================================================================================
        /// <summary>
        /// Finds the line that best represents a set of points.
        /// </summary>
        /// <param name="points">
        /// The coordinates of the points to use for the fit. This collection must contain two or more point items 
        /// corresponding to two or more points on the line.
        /// </param>
        /// <param name="options">
        /// The options the method uses.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.FitLineReport" crefType="Unqualified"/> object containing information
        /// about the line that best represents the set of points.
        /// </returns>
        /// <remarks>
        /// The resulting line may only take into account a subset of the input points.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim points As New Collection(Of PointContour)
        ///  
        /// 'Find the line that passes through
        /// '(1,1), (-1,1)
        /// points.Add (New PointContour (1, 1))
        /// points.Add (New PointContour (-1, 1))
        ///  
        /// Dim Report As FitLineReport = Algorithms.FitLine (points)
        /// 'Overlay the line on the image in Viewer1.
        /// imageViewer1.Image.Overlays.Default.AddLine (Report.LineSegment)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// Collection&lt;PointContour&gt; points = new Collection&lt;PointContour&gt;();
        ///  
        /// // Find the line that passes through
        /// // (1,1), (-1,1)
        /// points.Add(new PointContour(1, 1));
        /// points.Add(new PointContour(-1, 1));
        ///  
        /// FitLineReport report = Algorithms.FitLine(points);
        /// // Overlay the line on the image in Viewer1.
        /// imageViewer1.Image.Overlays.Default.AddLine(report.LineSegment);
        /// </code>
        /// </example>

        public static FitLineReport FitLine(Collection<PointContour> points, FitLineOptions options)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_PointFloat[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointFloat>(points);
            CVI_FitLineOptions cviOptions = new CVI_FitLineOptions();
            cviOptions.ConvertFromExternal(options);
            IntPtr report = VisionDll.imaqFitLine(cviPoints, cviPoints.Length, ref cviOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<FitLineReport, CVI_BestLine, Collection<PointContour>>(report, points, true);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the angles formed by sets of four points in an image.
        /// </summary>
        /// <param name="points">
        /// The coordinates for angle computation.
        /// </param>
        /// <returns>
        /// A collection containing the computed angles in degrees.
        /// </returns>

        public static Collection<double> GetAngles(Collection<PointContour> points)
        {
            return GetAngles(points, null);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the angles formed by sets of four points in an image.
        /// </summary>
        /// <param name="points">
        /// The ROI containing the points for angle computation.
        /// </param>
        /// <returns>
        /// A collection containing the computed angles in degrees.
        /// </returns>

        public static Collection<double> GetAngles(Roi points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return GetAngles(Utilities.ConvertRoiToPoints(points), null);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the angles formed by sets of two points and a common vertex.
        /// </summary>
        /// <param name="points">
        /// The ROI containing the points for angle computation.
        /// </param>
        /// <param name="vertex">
        /// The coordinate of the vertex.
        /// </param>
        /// <returns>
        /// A collection containing the computed angles in degrees.
        /// </returns>

        public static Collection<double> GetAngles(Roi points, PointContour vertex)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return GetAngles(Utilities.ConvertRoiToPoints(points), vertex);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the angles formed by sets of two points and a common vertex.
        /// </summary>
        /// <param name="points">
        /// The coordinates for angle computation.
        /// </param>
        /// <param name="vertex">
        /// The coordinate of the vertex.
        /// </param>
        /// <returns>
        /// A collection containing the computed angles in degrees.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        ///     
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Find the edge coordinates along a path defined by regions
        /// 'on Viewer1 and display the coordinates on the image.
        /// Dim points As New Collection(Of PointContour)
        /// Dim vertex As New PointContour
        /// Dim anglesWithoutVertex As Double
        /// Dim anglesWithVertex As Double 
        ///  
        /// 'Initialize the points and the vertex
        /// points.Add (New PointContour (10, 100))
        /// points.Add (New PointContour (20, 50))
        /// points.Add (New PointContour (30, 10))
        /// points.Add (New PointContour (40, 15))
        /// vertex.Initialize(10, 10)
        ///  
        /// 'Find the angles without a vertex
        /// anglesWithoutVertex = Algorithms.GetAngles (points)
        /// 'Find the angles with a vertex
        /// anglesWithVertex = Algorithms.GetAngles (points, vertex)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Find the edge coordinates along a path defined by regions
        /// // on Viewer1 and display the coordinates on the image.
        /// Collection&lt;PointContour&gt; points = new Collection&lt;PointContour&gt;();
        /// PointContour vertex = new PointContour();
        /// double anglesWithoutVertex, anglesWithVertex;
        ///  
        /// // Initialize the points and the vertex
        /// points.Add(new PointContour(10, 100));
        /// points.Add(new PointContour(20, 50));
        /// points.Add(new PointContour(30, 10));
        /// points.Add(new PointContour(40, 15));
        /// vertex.Initialize(10, 10);
        ///  
        /// // Find the angles without a vertex
        /// anglesWithoutVertex = Algorithms.GetAngles(points);
        /// // Find the angles with a vertex
        /// anglesWithVertex = Algorithms.GetAngles(points, vertex);
        /// </code>
        /// </example>

        public static Collection<double> GetAngles(Collection<PointContour> points, PointContour vertex)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            Array1D pointsArray = Utilities.ConvertCollectionToArray1D<PointContour, CVI_PointFloat>(points);
            Array1D anglesInDegreesArray = new Array1D();
            VisionDll.Priv_InitArray1D(out anglesInDegreesArray);
            try
            {
                if (vertex == null)
                {
                    Utilities.ThrowError(VisionDll.Priv_GetAngles2(ref pointsArray, ref anglesInDegreesArray, IntPtr.Zero, IntPtr.Zero));
                }
                else
                {
                    CVI_PointFloat cviVertex = new CVI_PointFloat();
                    cviVertex.ConvertFromExternal(vertex);
                    Utilities.ThrowError(VisionDll.Priv_GetAngles2(ref pointsArray, ref anglesInDegreesArray, IntPtr.Zero, ref cviVertex));
                }
                return Utilities.ConvertArray1DToCollection<double, float>(anglesInDegreesArray, delegate(float f) { return f; });
            }
            finally
            {
                Marshal.FreeCoTaskMem(pointsArray.Ptr);
                VisionDll.Priv_DisposeArray1DBytes(ref anglesInDegreesArray);
            }
        }
        //==========================================================================================
        /// <summary>
        /// Computes a line that bisects two lines.
        /// </summary>
        /// <param name="line1">
        /// Defines the first line.
        /// </param>
        /// <param name="line2">
        /// Defines the second line.
        /// </param>
        /// <returns>
        /// The line that bisects <format type="italics">line1</format> and <format type="italics">line2</format>. On failure, an exception is thrown.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim line1 As New LineContour(New PointContour(10, 10), New PointContour(100, 100))
        /// Dim line2 As New LineContour(New PointContour(-40, -100), New PointContour(20, 30))
        ///  
        /// 'Find the bisecting line
        /// Dim BisectingLine As LineContour = Algorithms.FindBisectingLine (line1, line2)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// LineContour line1 = new LineContour(new PointContour(10, 10), new PointContour(100, 100));
        /// LineContour line2 = new LineContour(new PointContour(-40, 100), new PointContour(20, 30));
        ///     
        /// //Find the bisecting line
        /// LineContour bisectingLine = Algorithms.FindBisectingLine(line1, line2);
        /// </code>
        /// </example>

        public static LineContour FindBisectingLine(LineContour line1, LineContour line2)
        {
            if (line1 == null) { throw new ArgumentNullException("line1"); }
            if (line2 == null) { throw new ArgumentNullException("line2"); }
            CVI_PointFloat start1 = new CVI_PointFloat();
            start1.ConvertFromExternal(line1.Start);
            CVI_PointFloat end1 = new CVI_PointFloat();
            end1.ConvertFromExternal(line1.End);
            CVI_PointFloat start2 = new CVI_PointFloat();
            start2.ConvertFromExternal(line2.Start);
            CVI_PointFloat end2 = new CVI_PointFloat();
            end2.ConvertFromExternal(line2.End);
            CVI_PointFloat bisectStart = new CVI_PointFloat();
            CVI_PointFloat bisectEnd = new CVI_PointFloat();
            Utilities.ThrowError(VisionDll.imaqGetBisectingLine(start1, end1, start2, end2, out bisectStart, out bisectEnd));
            return new LineContour(bisectStart.ConvertToExternal(), bisectEnd.ConvertToExternal());
        }
        //==========================================================================================
        /// <summary>
        /// Computes the distance, in pixels, between consecutive pairs of points.
        /// </summary>
        /// <param name="points">
        /// Specifies the points that the method uses to find the distances. This parameter must contain two or more PointContours.
        /// </param>
        /// <returns>
        /// A collection of floating-point numbers containing the computed distances. The method computes distance i between
        /// points i + 1 and i + 2.
        /// </returns>

        public static Collection<double> FindPointDistances(Roi points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return FindPointDistances(Utilities.ConvertRoiToPoints(points));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the distance, in pixels, between consecutive pairs of points.
        /// </summary>
        /// <param name="points">
        /// Specifies the points the method uses to find the distances.
        /// </param>
        /// <returns>
        /// A collection of floating-point numbers containing the computed distances. The method computes distance i between
        /// points i + 1 and i + 2.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim points As New Collection(Of PointContour)
        ///  
        /// 'Initialize points
        /// points.Add (New PointContour (10, 20))
        /// points.Add (New PointContour (100, 30))
        /// points.Add (New PointContour (50, 40))
        ///  
        /// 'Find the distances between consecutive points
        /// Dim Distances As Collection(Of Double) = Algorithms.FindPointDistances (points)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// Collection&lt;PointContour&gt; points = new Collection&lt;PointContour&gt;();
        ///  
        /// //Initialize points
        /// points.Add(new PointContour(10, 20));
        /// points.Add(new PointContour(100, 30));
        /// points.Add(new PointContour(50, 40));
        ///  
        /// //Find the distances between consecutive points
        /// Collection&lt;double&gt; distances = Algorithms.FindPointDistances(points);
        /// </code>
        /// </example>

        public static Collection<double> FindPointDistances(Collection<PointContour> points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            Array1D pointsArray = Utilities.ConvertCollectionToArray1D<PointContour, CVI_PointFloat>(points);
            Array1D distancesArray = new Array1D();
            VisionDll.Priv_InitArray1D(out distancesArray);
            try
            {
                Utilities.ThrowError(VisionDll.Priv_FindPointDistances(ref pointsArray, ref distancesArray));
                return Utilities.ConvertArray1DToCollection<double, float>(distancesArray, delegate(float f) { return f; });
            }
            finally
            {
                Marshal.FreeCoTaskMem(pointsArray.Ptr);
                VisionDll.Priv_DisposeArray1DBytes(ref distancesArray);
            }
        }
        //==========================================================================================
        /// <summary>
        /// Computes the intersection of two lines.
        /// </summary>
        /// <param name="line1">
        /// Defines the start point and end point of the first line.
        /// </param>
        /// <param name="line2">
        /// Defines the start point and end point of the second line.
        /// </param>
        /// <returns>
        /// The coordinate location of the intersection of the two lines.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim line1 As New LineContour(New PointContour(10, 10), New PointContour(100, 100))
        /// Dim line2 As New LineContour(New PointContour(-40, -100), New PointContour(20, 30))
        ///  
        /// 'Find the intersection
        /// Dim intersection As PointContour = Algorithms.FindIntersectionPoint (line1, line2)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// LineContour line1 = new LineContour(new PointContour(10, 10), new PointContour(100, 100));
        /// LineContour line2 = new LineContour(new PointContour(-40, -100), new PointContour(20, 30));
        ///    
        /// //Find the intersection
        /// PointContour intersection = Algorithms.FindIntersectionPoint(line1, line2);
        /// </code>
        /// </example>

        public static PointContour FindIntersectionPoint(LineContour line1, LineContour line2)
        {
            if (line1 == null) { throw new ArgumentNullException("line1"); }
            if (line2 == null) { throw new ArgumentNullException("line2"); }
            CVI_PointFloat start1 = new CVI_PointFloat();
            start1.ConvertFromExternal(line1.Start);
            CVI_PointFloat end1 = new CVI_PointFloat();
            end1.ConvertFromExternal(line1.End);
            CVI_PointFloat start2 = new CVI_PointFloat();
            start2.ConvertFromExternal(line2.Start);
            CVI_PointFloat end2 = new CVI_PointFloat();
            end2.ConvertFromExternal(line2.End);
            CVI_PointFloat intersectionPoint = new CVI_PointFloat();
            Utilities.ThrowError(VisionDll.imaqGetIntersection(start1, end1, start2, end2, out intersectionPoint));
            return intersectionPoint.ConvertToExternal();
        }
        //==========================================================================================
        /// <summary>
        /// Computes the mid line between a point and a reference line. The mid line is the line that is parallel to 
        /// the reference line and lies midway between the point and the reference line.
        /// </summary>
        /// <param name="line">
        /// Defines the reference line.
        /// </param>
        /// <param name="point">
        /// Defines the point.
        /// </param>
        /// <returns>
        /// The line that is parallel to the reference line and lies midway between the point and the reference line.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim line As New LineContour
        /// Dim point As New PointContour
        ///  
        /// 'Initialize the line and the point
        /// line.Start.Initialize (10, 10)
        /// line.End.Initialize (100, 100)
        /// point.Initialize(50, -50)
        ///  
        /// 'Find the midline
        /// Dim MidLine As LineContour = Algorithms.FindMidLine (line, point)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// LineContour line = new LineContour();
        /// PointContour point = new PointContour();
        ///     
        /// //Initialize the line and the point
        /// line.Start.Initialize(10, 10);
        /// line.End.Initialize(100, 100);
        /// point.Initialize(50, -50);
        ///     
        /// //Find the midline
        /// LineContour midLine = Algorithms.FindMidLine(line, point);
        /// </code>
        /// </example>

        public static LineContour FindMidLine(LineContour line, PointContour point)
        {
            if (line == null) { throw new ArgumentNullException("line"); }
            if (point == null) { throw new ArgumentNullException("point"); }
            CVI_PointFloat lineStart = new CVI_PointFloat();
            lineStart.ConvertFromExternal(line.Start);
            CVI_PointFloat lineEnd = new CVI_PointFloat();
            lineEnd.ConvertFromExternal(line.End);
            CVI_PointFloat cviPoint = new CVI_PointFloat();
            cviPoint.ConvertFromExternal(point);
            CVI_PointFloat midLineStart = new CVI_PointFloat();
            CVI_PointFloat midLineEnd = new CVI_PointFloat();
            Utilities.ThrowError(VisionDll.imaqGetMidLine(lineStart, lineEnd, cviPoint, out midLineStart, out midLineEnd));
            return new LineContour(midLineStart.ConvertToExternal(), midLineEnd.ConvertToExternal());
        }
        //==========================================================================================
        /// <summary>
        /// Computes a line that passes through a point and is perpendicular to a reference line.
        /// </summary>
        /// <param name="line">
        /// Defines the reference line.
        /// </param>
        /// <param name="point">
        /// Defines the point.
        /// </param>
        /// <returns>
        /// The coordinate location of the perpendicular line.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim line As New LineContour
        /// Dim point As New PointContour
        ///  
        /// 'Initialize the line and the point
        /// line.Start.Initialize (10, 10)
        /// line.End.Initialize (100, 100)
        /// point.Initialize(50, -50)
        ///  
        /// 'Find the line that passes through point and
        /// 'is perpendicular to line
        /// Dim perpendicular As LineContour = Algorithms.FindPerpendicularLine (line, point)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// LineContour line = new LineContour();
        /// PointContour point = new PointContour();
        ///     
        /// //Initialize the line and the point
        /// line.Start.Initialize(10, 10);
        /// line.End.Initialize(100, 100);
        /// point.Initialize(50, -50);
        ///     
        /// //Find the line that passes through point and
        /// //is perpendicular to line
        /// LineContour perpendicular = Algorithms.FindPerpendicularLine(line, point);
        /// </code>
        /// </example>

        public static LineContour FindPerpendicularLine(LineContour line, PointContour point)
        {
            if (line == null) { throw new ArgumentNullException("line"); }
            if (point == null) { throw new ArgumentNullException("point"); }
            CVI_PointFloat lineStart = new CVI_PointFloat();
            lineStart.ConvertFromExternal(line.Start);
            CVI_PointFloat lineEnd = new CVI_PointFloat();
            lineEnd.ConvertFromExternal(line.End);
            CVI_PointFloat cviPoint = new CVI_PointFloat();
            cviPoint.ConvertFromExternal(point);
            CVI_PointFloat perpLineStart = new CVI_PointFloat();
            CVI_PointFloat perpLineEnd = new CVI_PointFloat();
            Utilities.ThrowError(VisionDll.imaqGetPerpendicularLine(lineStart, lineEnd, cviPoint, out perpLineStart, out perpLineEnd, IntPtr.Zero));
            return new LineContour(perpLineStart.ConvertToExternal(), perpLineEnd.ConvertToExternal());
        }
        //==========================================================================================
        /// <summary>
        /// Finds the number of edge segments in an image and returns the coordinates of the pixels in each segment. 
        /// Any pixel that is greater than zero is considered as an edge location.
        /// </summary>
        /// <param name="image">
        /// The source image.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.SegmentReport" crefType="Unqualified"/> object containing information 
        /// about the edge segments the method found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// <para>
        /// This method joins adjoining edge pixels into edge segments. An edge segment is considered closed if it forms a 
        /// loop. Each edge segment is given a weight based on the pixel gray values along that edge. An edge segment with 
        /// high gray values has a higher weight.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Get the contours on the image in Viewer1.
        /// Dim Contours As Collection(Of SegmentReport)
        /// Contours = Algorithms.GetPointsOnContour (imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Get the contours on the image in Viewer1.
        /// Collection&lt;SegmentReport&gt; contours;
        /// contours = Algorithms.GetPointsOnContour(imageViewer1.Image);
        /// </code>
        /// </example>

        public static Collection<SegmentReport> GetPointsOnContour(VisionImage image)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            int numSegments;
            IntPtr report = VisionDll.imaqGetPointsOnContour(image._image, out numSegments);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToCollection<SegmentReport, CVI_SegmentInfo>(report, numSegments, true);
        }
        //==========================================================================================
        /// <summary>
        /// Given the endpoints of a line, returns an collection of all the points comprising the line.
        /// </summary>
        /// <param name="line">
        /// The ROI containing the line whose points the method returns.
        /// </param>
        /// <returns>
        /// The points on the line.
        /// </returns>

        public static Collection<PointContour> GetPointsOnLine(Roi line)
        {
            if (line == null) { throw new ArgumentNullException("line"); }
            line.ThrowIfDisposed();
            Utilities.ThrowIfNotSingleLine(line);
            return GetPointsOnLine((LineContour)line[0].Shape);
        }
        //==========================================================================================
        /// <summary>
        /// Given the endpoints of a line, returns an collection of all the points comprising the line.
        /// </summary>
        /// <param name="line">
        /// Specifies the line whose points the method returns.
        /// </param>
        /// <returns>
        /// The points on the line.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim line As New LineContour
        ///  
        /// 'Initialize the line
        /// line.Start (10, 10)
        /// line.End (100, 100)
        ///  
        /// 'Find the points on the line
        /// Algorithms.GetPointsOnLine (line)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// LineContour line = new LineContour();
        ///     
        /// //Initialize the line
        /// line.Start(10, 10);
        /// line.End(100, 100);
        ///     
        /// //Find the points on the line
        /// Algorithms.GetPointsOnLine(line);
        /// </code>
        /// </example>

        public static Collection<PointContour> GetPointsOnLine(LineContour line)
        {
            if (line == null) { throw new ArgumentNullException("line"); }
            CVI_Point cviStart = new CVI_Point();
            cviStart.ConvertFromExternal(line.Start);
            CVI_Point cviEnd = new CVI_Point();
            cviEnd.ConvertFromExternal(line.End);
            int numPoints;
            IntPtr points = VisionDll.imaqGetPointsOnLine(cviStart, cviEnd, out numPoints);
            Utilities.ThrowError(points);
            return Utilities.ConvertIntPtrToCollection<PointContour, CVI_Point>(points, numPoints, true);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the area of a polygon described by the coordinates of its vertices.
        /// </summary>
        /// <param name="polygon">
        /// Defines the vertices of the polygon. This polygon must have three or more items corresponding to 
        /// three or more points located at the vertices of the polygon.
        /// </param>
        /// <returns>
        /// The area of the polygon.
        /// </returns>
        /// <remarks>
        /// The polygon must contain three or more vertices.
        /// </remarks>

        public static double FindPolygonArea(PolygonContour polygon)
        {
            if (polygon == null) { throw new ArgumentNullException("polygon"); }
            return FindPolygonArea(polygon.Points);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the area of a polygon described by the coordinates of its vertices.
        /// </summary>
        /// <param name="points">
        /// Defines the vertices of the polygon. This polygon must have three or more items corresponding to 
        /// three or more points located at the vertices of the polygon.
        /// </param>
        /// <returns>
        /// The area of the polygon.
        /// </returns>
        /// <remarks>
        /// The polygon must contain three or more vertices.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim points As New Collection(Of PointContour)
        ///  
        /// 'Find the area of a triangle defined by the points
        /// '(0,0), (0,30) and (30,0)
        /// points.Add (New PointContour (0, 0))
        /// points.Add (New PointContour (0, 30))
        /// points.Add (New PointContour (30, 0))
        ///  
        /// Dim area As Double = Algorithms.FindPolygonArea (points)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// Collection&lt;PointContour&gt; points = new Collection&lt;PointContour&gt;();
        ///  
        /// //Find the area of a triangle defined by the points
        /// //(0,0), (0,30) and (30,0)
        /// points.Add(new PointContour(0, 0));
        /// points.Add(new PointContour(0, 30));
        /// points.Add(new PointContour(30, 0));
        ///  
        /// double area = Algorithms.FindPolygonArea(points);
        /// </code>
        /// </example>

        public static double FindPolygonArea(Collection<PointContour> points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            CVI_PointFloat[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointFloat>(points);
            float area;
            Utilities.ThrowError(VisionDll.imaqGetPolygonArea(cviPoints, cviPoints.Length, out area));
            return area;
        }
        //==========================================================================================
        /// <summary>
        /// Resamples an array of pixels from an image using multiple interpolation functions. Use this method to 
        /// perform subpixel analysis of pixel profiles in the image.
        /// </summary>
        /// <param name="image">
        /// The image containing the values to interpolate.
        /// </param>
        /// <param name="points">
        /// The ROI containing the points over which the method performs the interpolation.
        /// </param>
        /// <returns>
        /// The resampled pixel data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Rgb32 images. You can use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.RoiProfile" crefType="Unqualified"/> method to obtain 
        /// the coordinate points that you need to specify to this method.
        /// </remarks>

        public static Collection<PixelValue> InterpolatePoints(VisionImage image, Roi points)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return InterpolatePoints(image, Utilities.ConvertRoiToPoints(points), new SubPixelOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Resamples an array of pixels from an image using multiple interpolation functions. Use this method to 
        /// perform subpixel analysis of pixel profiles in the image.
        /// </summary>
        /// <param name="image">
        /// The image containing the values to interpolate.
        /// </param>
        /// <param name="points">
        /// The coordinates of points over which the method performs the interpolation. This parameter must only contain PointContours.
        /// </param>
        /// <returns>
        /// The resampled pixel data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Rgb32 images. You can use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.RoiProfile" crefType="Unqualified"/> method to obtain 
        /// the coordinate points that you need to specify to this method.
        /// </remarks>

        public static Collection<PixelValue> InterpolatePoints(VisionImage image, Collection<PointContour> points)
        {
            return InterpolatePoints(image, points, new SubPixelOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Resamples an array of pixels from an image using multiple interpolation functions. Use this method to 
        /// perform subpixel analysis of pixel profiles in the image.
        /// </summary>
        /// <param name="image">
        /// The image containing the values to interpolate.
        /// </param>
        /// <param name="points">
        /// The ROI containing the points over which the method performs the interpolation.
        /// </param>
        /// <param name="options">
        /// The options to use to perform the interpolation for subpixel measurements.
        /// </param>
        /// <returns>
        /// The resampled pixel data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Rgb32 images. You can use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.RoiProfile" crefType="Unqualified"/> method to obtain 
        /// the coordinate points that you need to specify to this method.
        /// </remarks>

        public static Collection<PixelValue> InterpolatePoints(VisionImage image, Roi points, SubPixelOptions options)
        {
            if (points == null) { throw new ArgumentNullException("points"); }
            points.ThrowIfDisposed();
            return InterpolatePoints(image, Utilities.ConvertRoiToPoints(points), options);
        }
        //==========================================================================================
        /// <summary>
        /// Resamples an array of pixels from an image using multiple interpolation functions. Use this method to 
        /// perform subpixel analysis of pixel profiles in the image.
        /// </summary>
        /// <param name="image">
        /// The image containing the values to interpolate.
        /// </param>
        /// <param name="points">
        /// The coordinates of points over which the method performs the interpolation. This parameter must only contain PointContours.
        /// </param>
        /// <param name="options">
        /// The options to use to perform the interpolation for subpixel measurements.
        /// </param>
        /// <returns>
        /// The resampled pixel data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Rgb32 images. You can use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.RoiProfile" crefType="Unqualified"/> method to obtain 
        /// the coordinate points that you need to specify to this method.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim ProfileReport As RoiProfileReport
        ///  
        /// 'First get the points along the path using RoiProfile
        /// ProfileReport = Algorithms.RoiProfile (imageViewer1.Image, imageViewer1.Roi)
        ///  
        /// 'Resample the pixels in image in Viewer1 using
        /// 'a quadratic interpolation with one-fourth subpixel accuracy.
        /// Dim Options As New SubPixelOptions (SubPixelAccuracy.OneFourth, InterpolationMethod.Quadratic)
        /// Algorithms.InterpolatePoints (imageViewer1.Image, ProfileReport.Pixels, Options)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// RoiProfileReport profileReport;
        ///  
        /// //First get the points along the path using RoiProfile
        /// profileReport = Algorithms.RoiProfile(imageViewer1.Image, imageViewer1.Roi);
        ///  
        /// //Resample the pixels in image in Viewer1 using
        /// //a quadratic interpolation with one-fourth subpixel accuracy.
        /// SubPixelOptions options = new SubPixelOptions(SubPixelAccuracy.OneFourth, InterpolationMethod.Quadratic);
        /// Algorithms.InterpolatePoints(imageViewer1.Image, profileReport.Pixels, options);
        /// </code>
        /// </example>

        public static Collection<PixelValue> InterpolatePoints(VisionImage image, Collection<PointContour> points, SubPixelOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (points == null) { throw new ArgumentNullException("points"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_Point[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_Point>(points);
            int count;
            IntPtr result = VisionDll.imaqInterpolatePoints(image._image, cviPoints, cviPoints.Length, options.Type, options.Accuracy, out count);
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToCollection<PixelValue, float>(result, count, true, delegate(float f) { return new PixelValue(f); });
        }
        #endregion

        #region Clipboard functions
        //==========================================================================================
        /// <summary>
        /// Reads an image from the clipboard.
        /// </summary>
        /// <param name="image">
        /// The image to copy the clipboard image into.
        /// </param>
        /// <remarks>
        /// Use this method with image types U8 and Rgb32.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void ClipboardToImage(VisionImage image)
        {
            ClipboardToImage(image, false);
        }

        //==========================================================================================
        /// <summary>
        /// Reads an image from the clipboard.
        /// </summary>
        /// <param name="image">
        /// The image to copy the clipboard image into.
        /// </param>
        /// <param name="readPalette">
        /// Determines whether to read the palette from the clipboard image.
        /// </param>
        /// <returns>
        /// If requested, returns the palette from the clipboard.
        /// </returns>
        /// <remarks>
        /// Use this method with image types U8 and Rgb32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Copy the image in clipboard to the image in viewer1.
        /// 'Also, copy the palette from the clipboard.
        /// imageViewer1.Palette = Algorithms.ClipboardToImage (imageViewer1.Image, True)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Copy the image in clipboard to the image in viewer1.
        /// //Also, copy the palette from the clipboard.
        /// imageViewer1.Palette = Algorithms.ClipboardToImage(imageViewer1.Image, True)
        /// 	</code>
        /// </example>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static Palette ClipboardToImage(VisionImage image, bool readPalette)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (readPalette)
            {
                Rgb32Value[] colorTableArray = new Rgb32Value[256];
                Utilities.ThrowError(VisionDll.imaqClipboardToImage(image._image, colorTableArray));
                image.OnReadFile();
                return new Palette(Utilities.ConvertArrayToCollection<Rgb32Value>(colorTableArray, 256));
            }
            else
            {
                Utilities.ThrowError(VisionDll.imaqClipboardToImage(image._image, null));
                image.OnReadFile();
                return null;
            }
        }
        //==========================================================================================
        /// <summary>
        /// Copies an image onto the clipboard.
        /// </summary>
        /// <param name="image">
        /// The image to copy to the clipboard.
        /// </param>
        /// <remarks>
        /// Use this method with U8 and Rgb32 images.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void ImageToClipboard(VisionImage image)
        {
            ImageToClipboard(image, null);
        }
        //==========================================================================================
        /// <summary>
        /// Copies an image onto the clipboard.
        /// </summary>
        /// <param name="image">
        /// The image to copy to the clipboard.
        /// </param>
        /// <param name="palette">
        /// A palette to associate with 8-bit images. It must point to an array of 256 colors, which represent the color 
        /// palette that the method associates with the image. If you pass null or Nothing for this parameter is NULL, the 
        /// method associates a grayscale palette with the image. 
        /// </param>
        /// <remarks>
        /// Use this method with U8 and Rgb32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Copy the image in Viewer1 to the clipboard.
        /// Algorithms.ImageToClipboard (imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Copy the image in Viewer1 to the clipboard.
        /// Algorithms.ImageToClipboard(imageViewer1.Image);
        /// </code>
        /// </example>

        [EditorBrowsable(EditorBrowsableState.Advanced)]
        public static void ImageToClipboard(VisionImage image, Palette palette)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            System.Security.Permissions.UIPermission ui = new System.Security.Permissions.UIPermission(UIPermissionWindow.NoWindows, UIPermissionClipboard.AllClipboard);
            ui.Demand();
            if (palette != null)
            {
                // We need this to be exactly 256 entries long.
                Rgb32Value[] colorTableArray = new Rgb32Value[256];
                int i;
                for (i = 0; i < palette.Entries.Count && i < 256; ++i)
                {
                    colorTableArray[i] = palette.Entries[i].Rgb32Value;
                }
                for (int j = i; j < 256; ++j)
                {
                    colorTableArray[j] = new Rgb32Value((byte)j, (byte)j, (byte)j);
                }
                Utilities.ThrowError(VisionDll.imaqImageToClipboard(image._image, colorTableArray));
            }
            else
            {
                Utilities.ThrowError(VisionDll.imaqImageToClipboard(image._image, null));
            }
        }
        #endregion

        #region Color Processing functions
        //==========================================================================================
        /// <summary>
        /// Maps the value of a color in one color mode into the value of the same color in another color mode.
        /// </summary>
        /// <param name="source">
        /// The source color space used for the operation.
        /// </param>
        /// <param name="destinationSpace">
        /// The destination color space used for the operation.
        /// </param>
        /// <returns>
        /// The value of the source color as represented in the destination color space. On failure, an exception is thrown.
        /// </returns>

        public static ColorValue ConvertColorValue(ColorValue source, ColorMode destinationSpace)
        {
            return ConvertColorValue(source, destinationSpace, 0.0);
        }
        //==========================================================================================
        /// <summary>
        /// Maps the value of a color in one color mode into the value of the same color in another color mode.
        /// </summary>
        /// <param name="source">
        /// The source color space used for the operation.
        /// </param>
        /// <param name="destinationSpace">
        /// The destination color space used for the operation.
        /// </param>
        /// <param name="offset">Offset, in degrees, to use when converting to HSL.
        /// </param>
        /// <returns>
        /// The value of the source color as represented in the destination color space. On failure, an exception is thrown.
        /// </returns>

        public static ColorValue ConvertColorValue(ColorValue source, ColorMode destinationSpace, double offset)
        {
            CVI_Color2 cviSource = source.CVI_Color2;
            CVI_Color2 cviResult = VisionDll.imaqChangeColorSpace2(ref cviSource, source.ColorMode, destinationSpace, offset, IntPtr.Zero);
            // Unfortunately we could think cviResult is an error when it's really a success, so just
            // call imaqGetLastError() and let it decide.
            Utilities.ThrowError();
            return new ColorValue(cviResult, destinationSpace);
        }
        //==========================================================================================
        /// <summary>
        /// Maps the value of a color in one color mode into the value of the same color in another color mode.
        /// </summary>
        /// <param name="source">
        /// The source color space used for the operation.
        /// </param>
        /// <param name="destinationSpace">
        /// The destination color space used for the operation.
        /// </param>
        /// <param name="offset">Offset, in degrees, to use when converting to HSL.
        /// </param>
        /// <param name="whiteReference">White reference value to use when converting to CIE L*a*b* format.
        /// </param>
        /// <returns>
        /// The value of the source color as represented in the destination color space. On failure, an exception is thrown.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// ' Find the HSL value equivalent to RGB color (128, 128, 128)
        /// Dim HslValue As Hsl32Value
        /// HslValue = Algorithms.ConvertColorValue(New ColorValue(New Rgb32Value(128, 128, 128)), ColorMode.Hsl).Hsl32
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Find the HSL value equivalent to RGB color (128, 128, 128)
        /// Hsl32Value hslValue;
        /// hslValue = Algorithms.ConvertColorValue(new ColorValue(new Rgb32Value(128, 128, 128)), ColorMode.Hsl).Hsl32;
        /// </code>
        /// </example>

        public static ColorValue ConvertColorValue(ColorValue source, ColorMode destinationSpace, double offset, CieXyzValue whiteReference)
        {
            CVI_Color2 cviSource = source.CVI_Color2;
            CVI_Color2 cviResult = VisionDll.imaqChangeColorSpace2(ref cviSource, source.ColorMode, destinationSpace, offset, ref whiteReference);
            // Unfortunately we could think cviResult is an error when it's really a success, so just
            // call imaqGetLastError() and let it decide.
            Utilities.ThrowError();
            return new ColorValue(cviResult, destinationSpace);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel distribution, of a color image.
        /// </summary>
        /// <param name="image">
        /// The color image used to compute the histogram.
        /// </param>
        /// <returns>
        /// A ColorHistogramReport. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.</remarks>

        public static ColorHistogramReport ColorHistogram(VisionImage image)
        {
            return ColorHistogram(image, 256, ColorMode.Rgb, null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel distribution, of a color image.
        /// </summary>
        /// <param name="image">
        /// The color image used to compute the histogram.
        /// </param>
        /// <param name="classes">
        /// The number of classes into which the method separates the pixels.
        /// The default is 256.
        /// </param>
        /// <returns>
        /// A ColorHistogramReport. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.</remarks>

        public static ColorHistogramReport ColorHistogram(VisionImage image, Int32 classes)
        {
            return ColorHistogram(image, classes, ColorMode.Rgb, null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel distribution, of a color image.
        /// </summary>
        /// <param name="image">
        /// The color image used to compute the histogram.
        /// </param>
        /// <param name="classes">
        /// The number of classes into which the method separates the pixels.
        /// The default is 256.
        /// </param>
        /// <param name="mode">
        /// The color space in which to perform the histogram. The default is ColorMode.Rgb.
        /// </param>
        /// <returns>
        /// A ColorHistogramReport. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.</remarks>

        public static ColorHistogramReport ColorHistogram(VisionImage image, Int32 classes, ColorMode mode)
        {
            return ColorHistogram(image, classes, mode, null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel distribution, of a color image.
        /// </summary>
        /// <param name="image">
        /// The color image used to compute the histogram.
        /// </param>
        /// <param name="classes">
        /// The number of classes into which the method separates the pixels. The default is 256.
        /// </param>
        /// <param name="mode">
        /// The color space in which to perform the histogram. The default is ColorMode.Rgb.
        /// </param>
        /// <param name="mask">
        /// The region to use for computing the histogram. The method calculates the histogram using only those pixels in the image whose corresponding pixels in the mask are non-zero. Pass null or Nothing for this parameter if you want to perform a histogram on the entire image.
        /// </param>
        /// <returns>
        /// A ColorHistogramReport. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.</remarks>

        public static ColorHistogramReport ColorHistogram(VisionImage image, Int32 classes, ColorMode mode, VisionImage mask)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            IntPtr result = VisionDll.imaqColorHistogram2(image._image, classes, mode, IntPtr.Zero, VisionImage.GetIntPtr(mask));
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToStructure<ColorHistogramReport, CVI_ColorHistogramReport>(result, true);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel distribution, of a color image.
        /// </summary>
        /// <param name="image">
        /// The color image used to compute the histogram.
        /// </param>
        /// <param name="classes">
        /// The number of classes into which the method separates the pixels. The default is 256.
        /// </param>
        /// <param name="mode">
        /// The color space in which to perform the histogram. The default is ColorMode.Rgb.
        /// </param>
        /// <param name="mask">
        /// The region to use for computing the histogram. The method calculates the histogram using only those pixels in the image whose corresponding pixels in the mask are non-zero. Pass null or Nothing for this parameter if you want to perform a histogram on the entire image.
        /// </param>
        /// <param name="whiteReference">
        /// The white reference value to use when the <see cref="NationalInstruments.Vision.ColorMode.CieLab" crefType="Unqualified"/> mode is selected.
        /// </param>
        /// <returns>
        /// A ColorHistogramReport. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim Report As ColorHistogramReport
        /// Dim MaskImage As New VisionImage
        /// ' Find the histogram of a portion of the image in Viewer1
        /// ' defined by the regions on Viewer1.
        /// Algorithms.RoiToMask (MaskImage, imageViewer1.Roi)
        /// 'Compute the histogram for each color plane
        /// Report = Algorithms.ColorHistogram (imageViewer1.Image, 256, ColorMode.Rgb, MaskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// ColorHistogramReport report;
        /// VisionImage maskImage = new VisionImage();
        /// // Find the histogram of a portion of the image in Viewer1
        /// // defined by the regions on Viewer1.
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// // Compute the histogram for each color plane
        /// report = Algorithms.ColorHistogram(imageViewer1.Image, 256, ColorMode.Rgb, MaskImage);
        /// </code>
        /// </example>

        public static ColorHistogramReport ColorHistogram(VisionImage image, Int32 classes, ColorMode mode, VisionImage mask, CieXyzValue whiteReference)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            IntPtr result = VisionDll.imaqColorHistogram2(image._image, classes, mode, ref whiteReference, VisionImage.GetIntPtr(mask));
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToStructure<ColorHistogramReport, CVI_ColorHistogramReport>(result, true);
        }
        //==========================================================================================
        /// <summary>
        /// Applies brightness, contrast, and gamma correction to each plane of a color image.
        /// </summary>
        /// <param name="source">
        /// The source image to transform.
        /// </param>
        /// <param name="destination">
        /// The result of performing the transform.
        /// </param>
        /// <param name="options">
        /// The options to use to transform the image.
        /// </param>
        /// <remarks>
        /// Use this method on Rgb32 images.
        /// </remarks>

        public static void ColorBcgTransform(VisionImage source, VisionImage destination, BcgOptions options)
        {
            if (options == null) { throw new ArgumentNullException("options"); }
            ColorBcgTransform(source, destination, options, options, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Applies brightness, contrast, and gamma correction to each plane of a color image.
        /// </summary>
        /// <param name="source">
        /// The source image to transform.
        /// </param>
        /// <param name="destination">
        /// The result of performing the transform.
        /// </param>
        /// <param name="options">
        /// The options to use to transform the image.
        /// </param>
        /// <param name="mask">
        /// The mask applied to the source image. It indicates the region of the image where the ColorBcgTransform is applied. Only pixels in the original image that correspond to a nonzero pixel in the mask are used. A ColorBcgTransform on the complete image occurs if no mask image is passed in.
        /// </param>
        /// <remarks>
        /// Use this method on Rgb32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim MaskImage As New VisionImage
        /// Dim RedOptions As New BcgOptions
        /// Dim BlueOptions As New BcgOptions
        /// 'Create a mask image from the regions on Viewer1
        /// Algorithms.RoiToMask (MaskImage, imageViewer1.Roi, New PixelValue(0), imageViewer1.Image)
        /// 'Set up the options for the ColorBcgTransform
        /// 'Decrease the brightness of the red plane
        /// RedOptions.Brightness = 100
        /// 'Increase the contrast and gamma of the blue plane
        /// BlueOptions.Contrast = 90
        /// BlueOptions.Gamma = 2
        /// 'Perform a ColorBcgTransform on a portion of Image using the mask
        /// 'Store the results in the image in Viewer2.
        /// imageViewer2.Image.Type = ImageType.Rgb32
        /// Algorithms.ColorBcgTransform (imageViewer1.Image, imageViewer2.Image, RedOptions, New BcgOptions(), BlueOptions, MaskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage MaskImage = new VisionImage();
        /// BcgOptions RedOptions = new BcgOptions();
        /// BcgOptions BlueOptions = new BcgOptions();
        /// //Create a mask image from the regions on Viewer1
        /// Algorithms.RoiToMask(MaskImage, imageViewer1.Roi, new PixelValue(0), imageViewer1.Image);
        /// //Set up the options for the ColorBcgTransform
        /// //Decrease the brightness of the red plane
        /// RedOptions.Brightness = 100;
        /// //Increase the contrast and gamma of the blue plane
        /// BlueOptions.Contrast = 90;
        /// BlueOptions.Gamma = 2;
        /// //Perform a ColorBcgTransform on a portion of Image using the mask
        /// //Store the results in the image in Viewer2.
        /// imageViewer2.Image.Type = ImageType.Rgb32;
        /// Algorithms.ColorBcgTransform(imageViewer1.Image, imageViewer2.Image, RedOptions, new BcgOptions(), BlueOptions, MaskImage);
        /// </code>
        /// </example>

        public static void ColorBcgTransform(VisionImage source, VisionImage destination, BcgOptions options, VisionImage mask)
        {
            if (options == null) { throw new ArgumentNullException("options"); }
            ColorBcgTransform(source, destination, options, options, options, mask);
        }
        //==========================================================================================
        /// <summary>
        /// Applies brightness, contrast, and gamma correction to each plane of a color image.
        /// </summary>
        /// <param name="source">
        /// The source image to transform.
        /// </param>
        /// <param name="destination">
        /// The result of performing the transform.
        /// </param>
        /// <param name="redOptions">
        /// The options to use to transform the red plane of the image.
        /// </param>
        /// <param name="greenOptions">
        /// The options to use to transform the green plane of the image.
        /// </param>
        /// <param name="blueOptions">
        /// The options to use to transform the blue plane of the image.
        /// </param>
        /// <remarks>
        /// Use this method on Rgb32 images.
        /// </remarks>

        public static void ColorBcgTransform(VisionImage source, VisionImage destination, BcgOptions redOptions, BcgOptions greenOptions, BcgOptions blueOptions)
        {
            ColorBcgTransform(source, destination, redOptions, greenOptions, blueOptions, null);
        }
        //==========================================================================================
        /// <summary>
        /// Applies brightness, contrast, and gamma correction to each plane of a color image.
        /// </summary>
        /// <param name="source">
        /// The source image to transform.
        /// </param>
        /// <param name="destination">
        /// The result of performing the transform.
        /// </param>
        /// <param name="redOptions">
        /// The options to use to transform the red plane of the image.
        /// </param>
        /// <param name="greenOptions">
        /// The options to use to transform the green plane of the image.
        /// </param>
        /// <param name="blueOptions">
        /// The options to use to transform the blue plane of the image.
        /// </param>
        /// <param name="mask">
        /// The mask applied to the source image. It indicates the region of the image where the ColorBcgTransform is applied. Only pixels in the original image that correspond to a nonzero pixel in the mask are used. A ColorBcgTransform on the complete image occurs if no mask image is passed in.
        /// </param>
        /// <remarks>
        /// Use this method on Rgb32 images.
        /// </remarks>

        public static void ColorBcgTransform(VisionImage source, VisionImage destination, BcgOptions redOptions, BcgOptions greenOptions, BcgOptions blueOptions, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (redOptions == null) { throw new ArgumentNullException("redOptions"); }
            if (greenOptions == null) { throw new ArgumentNullException("greenOptions"); }
            if (blueOptions == null) { throw new ArgumentNullException("blueOptions"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            CVI_BCGOptions cviRed = new CVI_BCGOptions();
            cviRed.ConvertFromExternal(redOptions);
            CVI_BCGOptions cviGreen = new CVI_BCGOptions();
            cviGreen.ConvertFromExternal(greenOptions);
            CVI_BCGOptions cviBlue = new CVI_BCGOptions();
            cviBlue.ConvertFromExternal(blueOptions);
            Utilities.ThrowError(VisionDll.imaqColorBCGTransform(destination._image, source._image, ref cviRed, ref cviGreen, ref cviBlue, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram of the luminance plane or all the planes of a color image and redistributes pixel values across the desired range while maintaining pixel value groupings.
        /// </summary>
        /// <param name="source">
        /// The color image to equalize.
        /// </param>
        /// <param name="destination">
        /// The resulting color image.
        /// </param>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.
        /// </remarks>

        public static void ColorEqualize(VisionImage source, VisionImage destination)
        {
            ColorEqualize(source, destination, false);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram of the luminance plane or all the planes of a color image and redistributes pixel values across the desired range while maintaining pixel value groupings.
        /// </summary>
        /// <param name="source">
        /// The color image to equalize.
        /// </param>
        /// <param name="destination">
        /// The resulting color image.
        /// </param>
        /// <param name="equalizeAllPlanes">
        /// Set this parameter to True to equalize each plane. Set this parameter to False to equalize the luminance plane only.
        /// This parameter has a default value of False.
        /// </param>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// i.Type = ImageType.Rgb32
        /// 'Equalize the luminance plane of the image in Viewer1 and the
        /// 'store the results in i
        /// Algorithms.ColorEqualize (imageViewer1.Image, i)
        /// 'Equalize all the planes of the image in Viewer1.
        /// 'Do this operation inplace (store the results in the image in Viewer1)
        /// Algorithms.ColorEqualize (imageViewer1.Image, imageViewer1.Image, True)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// i.Type = ImageType.Rgb32;
        /// //Equalize the luminance plane of the image in Viewer1 and the
        /// //store the results in i
        /// Algorithms.ColorEqualize(imageViewer1.Image, i);
        /// //Equalize all the planes of the image in Viewer1.
        /// //Do this operation inplace (store the results in the image in Viewer1)
        /// Algorithms.ColorEqualize(imageViewer1.Image, imageViewer1.Image, true);
        ///  
        /// </code>
        /// </example>

        public static void ColorEqualize(VisionImage source, VisionImage destination, bool equalizeAllPlanes)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqColorEqualize(destination._image, source._image, equalizeAllPlanes ? 1 : 0));
        }
        //==========================================================================================
        /// <summary>
        /// Performs a transformation on an image by replacing each pixel value in a given color plane with the lookup table entry corresponding to that value.
        /// </summary>
        /// <param name="source">
        /// The image on which to apply the lookup operation.
        /// </param>
        /// <param name="destination">
        /// The resulting color image.
        /// </param>
        /// <param name="mode">The color space in which to apply the lookup operation.
        /// </param>
        /// <param name="plane1">The lookup table to apply to the red or hue color plane. This array can contain up to 256 elements. If you specify fewer than 256 elements, the method fills the remaining elements with grayscale values. Pass null or Nothing for this parameter if you do not want to replace pixel values in this plane.
        /// </param>
        /// <param name="plane2">The lookup table to apply to the green or saturation color plane. This array can contain up to 256 elements. If you specify fewer than 256 elements, the method fills the remaining elements with grayscale values. Pass null or Nothing for this parameter if you do not want to replace pixel values in this plane.
        /// </param>
        /// <param name="plane3">The lookup table to apply to the blue, luminance, value, or intensity color plane. This array can contain up to 256 elements. If you specify fewer than 256 elements, the method fills the remaining elements with grayscale values. Pass null or Nothing for this parameter  if you do not want to replace pixel values in this plane.
        /// </param>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.
        /// </remarks>

        public static void ColorUserLookup(VisionImage source, VisionImage destination, ColorMode mode, Collection<Int16> plane1, Collection<Int16> plane2, Collection<Int16> plane3)
        {
            ColorUserLookup(source, destination, mode, plane1, plane2, plane3, null);
        }
        //==========================================================================================
        /// <summary>
        /// Performs a transformation on an image by replacing each pixel value in a given color plane with the lookup table entry corresponding to that value.
        /// </summary>
        /// <param name="source">
        /// The image on which to apply the lookup operation.
        /// </param>
        /// <param name="destination">
        /// The resulting color image.
        /// </param>
        /// <param name="mode">The color space in which to apply the lookup operation.
        /// </param>
        /// <param name="plane1">The lookup table to apply to the red or hue color plane. This array can contain up to 256 elements. If you specify fewer than 256 elements, the method fills the remaining elements with grayscale values. Pass null or Nothing for this parameter if you do not want to replace pixel values in this plane.
        /// </param>
        /// <param name="plane2">The lookup table to apply to the green or saturation color plane. This array can contain up to 256 elements. If you specify fewer than 256 elements, the method fills the remaining elements with grayscale values. Pass null or Nothing for this parameter if you do not want to replace pixel values in this plane.
        /// </param>
        /// <param name="plane3">The lookup table to apply to the blue, luminance, value, or intensity color plane. This array can contain up to 256 elements. If you specify fewer than 256 elements, the method fills the remaining elements with grayscale values. Pass null or Nothing for this parameter  if you do not want to replace pixel values in this plane.
        /// </param>
        /// <param name="mask">
        /// The region in the image to use for the color user lookup. The method processes only those pixels in the image whose corresponding pixels in the mask are non-zero. Pass null or Nothing for this parameter if you want to transform the entire image.
        /// </param>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim i As New VisionImage
        /// Dim redPlaneLookupTable As New Collection(Of Int16)
        /// Dim greenPlaneLookupTable As New Collection(Of Int16)
        /// 'Do something to populate redPlaneLookupTable and greenPlaneLookupTable
        /// 'Perform the ColorUserLookup on the image in Viewer1 and
        /// 'store the result in i
        /// i.Type = ImageType.Rgb32
        /// Algorithms.ColorUserLookup (imageViewer1.Image, i, ColorMode.Rgb, redPlaneLookupTable, greenPlaneLookupTable, Nothing)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage i = new VisionImage();
        /// Collection&lt;Int16&gt; redPlaneLookupTable = new Collection&lt;Int16&gt;();
        /// Collection&lt;Int16&gt; greenPlaneLookupTable = new Collection&lt;Int16&gt;();
        /// //Do something to populate redPlaneLookupTable and greenPlaneLookupTable
        /// //Perform the ColorUserLookup on the image in Viewer1 and
        /// //store the result in i
        /// i.Type = ImageType.Rgb32;
        /// Algorithms.ColorUserLookup(imageViewer1.Image, i, ColorMode.Rgb, redPlaneLookupTable, greenPlaneLookupTable, null);
        /// </code>
        /// </example>

        public static void ColorUserLookup(VisionImage source, VisionImage destination, ColorMode mode, Collection<Int16> plane1, Collection<Int16> plane2, Collection<Int16> plane3, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            int tableSize = 256;
            Int16[] cviTable1, cviTable2, cviTable3;
            // Optimization - if they pass an empty table, pass NULL to the function.
            if (plane1 == null || plane1.Count == 0)
            {
                cviTable1 = null;
            } else {
                // If the table is the correct size, just do the conversion; otherwise, pad it first.
                if (plane1.Count >= tableSize)
                {
                    cviTable1 = Utilities.ConvertCollectionToArray<Int16>(plane1);
                }
                else
                {
                    cviTable1 = new Int16[tableSize];
                    for (int i = 0; i < plane1.Count; ++i)
                    {
                        cviTable1[i] = plane1[i];
                    }
                    for (int i = plane1.Count; i < tableSize; ++i)
                    {
                        cviTable1[i] = (Int16)i;
                    }
                }
            }
            // Optimization - if they pass an empty table, pass NULL to the function.
            if (plane2 == null || plane2.Count == 0)
            {
                cviTable2 = null;
            } else {
                // If the table is the correct size, just do the conversion; otherwise, pad it first.
                if (plane2.Count >= tableSize)
                {
                    cviTable2 = Utilities.ConvertCollectionToArray<Int16>(plane2);
                }
                else
                {
                    cviTable2 = new Int16[tableSize];
                    for (int i = 0; i < plane2.Count; ++i)
                    {
                        cviTable2[i] = plane2[i];
                    }
                    for (int i = plane2.Count; i < tableSize; ++i)
                    {
                        cviTable2[i] = (Int16)i;
                    }
                }
            }
            // Optimization - if they pass an empty table, pass NULL to the function.
            if (plane3 == null || plane3.Count == 0)
            {
                cviTable3 = null;
            } else {
                // If the table is the correct size, just do the conversion; otherwise, pad it first.
                if (plane3.Count >= tableSize)
                {
                    cviTable3 = Utilities.ConvertCollectionToArray<Int16>(plane3);
                }
                else
                {
                    cviTable3 = new Int16[tableSize];
                    for (int i = 0; i < plane3.Count; ++i)
                    {
                        cviTable3[i] = plane3[i];
                    }
                    for (int i = plane3.Count; i < tableSize; ++i)
                    {
                        cviTable3[i] = (Int16)i;
                    }
                }
            }
            Utilities.ThrowError(VisionDll.imaqColorLookup(destination._image, source._image, mode, VisionImage.GetIntPtr(mask), cviTable1, cviTable2, cviTable3));
        }
        //==========================================================================================
        /// <summary>
        /// Thresholds a color image. The method selects a pixel if all three color components fall within the specified range. The method replaces the value of selected pixels with the given replacement value and sets the value of unselected pixels to 0.
        /// </summary>
        /// <param name="source">The color image to threshold.
        /// </param>
        /// <param name="destination">The resulting image. This image must be a U8 image except when it is the same image passed to <format type="italics">source</format>. In the case where the same image is passed to <format type="italics">source</format> and <format type="italics">destination</format>, the image is converted to a U8 image and the result is stored in this image.
        /// </param>
        /// <param name="mode">The color space in which to perform the threshold operation.
        /// </param>
        /// <param name="replaceValue">The value to which the method sets selected pixels.
        /// The default is 1.</param>
        /// <param name="plane1Range">The range in the red or the hue plane. The range is from 0 to 255.
        /// </param>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.
        /// </remarks>

        public static void ColorThreshold(VisionImage source, VisionImage destination, ColorMode mode, Int32 replaceValue, Range plane1Range)
        {
            ColorThreshold(source, destination, mode, replaceValue, plane1Range, new Range(0, 255), new Range(0, 255));
        }
        //==========================================================================================
        /// <summary>
        /// Thresholds a color image. The method selects a pixel if all three color components fall within the specified range. The method replaces the value of selected pixels with the given replacement value and sets the value of unselected pixels to 0.
        /// </summary>
        /// <param name="source">The color image to threshold.
        /// </param>
        /// <param name="destination">The resulting image. This image must be a U8 image except when it is the same image passed to <format type="italics">source</format>. In the case where the same image is passed to <format type="italics">source</format> and <format type="italics">destination</format>, the image is converted to a U8 image and the result is stored in this image.
        /// </param>
        /// <param name="mode">The color space in which to perform the threshold operation.
        /// </param>
        /// <param name="replaceValue">The value to which the method sets selected pixels.
        /// The default is 1.</param>
        /// <param name="plane1Range">The range in the red or the hue plane. The range is from 0 to 255.
        /// </param>
        /// <param name="plane2Range">The range in the green or the saturation plane. The range is from 0 to 255.
        /// </param>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.
        /// </remarks>

        public static void ColorThreshold(VisionImage source, VisionImage destination, ColorMode mode, Int32 replaceValue, Range plane1Range, Range plane2Range)
        {
            ColorThreshold(source, destination, mode, replaceValue, plane1Range, plane2Range, new Range(0, 255));
        }
        //==========================================================================================
        /// <summary>
        /// Thresholds a color image. The method selects a pixel if all three color components fall within the specified range. The method replaces the value of selected pixels with the given replacement value and sets the value of unselected pixels to 0.
        /// </summary>
        /// <param name="source">The color image to threshold.
        /// </param>
        /// <param name="destination">The resulting image. This image must be a U8 image except when it is the same image passed to <format type="italics">source</format>. In the case where the same image is passed to <format type="italics">source</format> and <format type="italics">destination</format>, the image is converted to a U8 image and the result is stored in this image.
        /// </param>
        /// <param name="mode">The color space in which to perform the threshold operation.
        /// </param>
        /// <param name="replaceValue">The value to which the method sets selected pixels.
        /// The default is 1.</param>
        /// <param name="plane1Range">The range in the red or the hue plane. The range is from 0 to 255.
        /// </param>
        /// <param name="plane2Range">The range in the green or the saturation plane. The range is from 0 to 255.
        /// </param>
        /// <param name="plane3Range">The range in the blue, the luminence, the value, or the intensity plane. The range is from 0 to 255.
        /// </param>
        /// <remarks>
        /// Use this method with image types Rgb32 and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// 'Perform a color threshold on the image in Viewer1 and
        /// 'store the results in i
        /// Algorithms.ColorThreshold (imageViewer1.Image, i, ColorMode.Hsl, 255, new Range(128, 255))
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// //Perform a color threshold on the image in Viewer1 and
        /// //store the results in i
        /// Algorithms.ColorThreshold(imageViewer1.Image, i, ColorMode.Hsl, 255, new Range(128, 255));
        /// </code>
        /// </example>

        public static void ColorThreshold(VisionImage source, VisionImage destination, ColorMode mode, Int32 replaceValue, Range plane1Range, Range plane2Range, Range plane3Range)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (plane1Range == null) { throw new ArgumentNullException("plane1Range"); }
            if (plane2Range == null) { throw new ArgumentNullException("plane2Range"); }
            if (plane3Range == null) { throw new ArgumentNullException("plane3Range"); }
            CVI_Range cviPlane1 = new CVI_Range();
            cviPlane1.ConvertFromExternal(plane1Range);
            CVI_Range cviPlane2 = new CVI_Range();
            cviPlane2.ConvertFromExternal(plane2Range);
            CVI_Range cviPlane3 = new CVI_Range();
            cviPlane3.ConvertFromExternal(plane3Range);
            Utilities.ThrowError(VisionDll.imaqColorThreshold(destination._image, source._image, replaceValue, mode, ref cviPlane1, ref cviPlane2, ref cviPlane3));
        }

        #endregion

        #region Transform functions
        //==========================================================================================
        /// <summary>
        /// Applies brightness, contrast, and gamma correction to an image by computing and applying a lookup table. The method computes the lookup table based on the brightness, contrast, and gamma values.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The result of performing the BcgTransform.
        /// </param>
        /// <param name="options">
        /// Specifies the options to use to perform the transform.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void BcgTransform(VisionImage source, VisionImage destination, BcgOptions options)
        {
            BcgTransform(source, destination, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Applies brightness, contrast, and gamma correction to an image by computing and applying a lookup table. The method computes the lookup table based on the brightness, contrast, and gamma values.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The result of performing the BcgTransform.
        /// </param>
        /// <param name="options">
        /// Specifies the options to use to perform the transform.
        /// </param>
        /// <param name="mask">
        /// The mask applied to the source image. <format type="italics">mask</format> specifies the region of the source image in which the method applies the brightness, contrast, and gamma correction. The method applies the correction to only those source pixels whose corresponding mask pixels are non-zero. Do not set this parameter if you want the method to correct the entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim mask As New VisionImage 
        /// Dim options As New BcgOptions
        ///  
        /// 'Increase the brightness of the image in Viewer1 and
        /// 'store the results in the image in Viewer2.
        /// BcgOptions.Brightness = 200
        /// Algortihms.BcgTransform (imageViewer1.Image, imageViewer2.Image, options)
        ///  
        /// 'Create a mask image from the regions on Viewer1
        /// Algorithms.Mask (mask, imageViewer1.Roi, imageViewer1.Image)
        ///  
        /// 'Perform a BCGTransform on a portion of Image using the mask
        /// 'Do this operation inplace (store the results in the image in Viewer2)
        /// BcgOptions.Brightness = 200
        /// BcgOptions.Contrast = 20
        /// BcgOptions.Gamma = 0.7
        /// Algorithms.BCGTransform (imageViewer2.Image, imageViewer2.Image, options, mask) 
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage mask = new VisionImage();
        /// BcgOptions options = new BcgOptions();
        ///  
        /// //Increase the brightness of the image in Viewer1 and
        /// //store the results in the image in Viewer2.
        /// BcgOptions.Brightness = 200;
        /// Algortihms.BcgTransform(imageViewer1.Image, imageViewer2.Image, options);
        ///         
        /// //Create a mask image from the regions on Viewer1
        /// Algorithms.Mask(mask, imageViewer1.Roi, imageViewer1.Image);
        ///         
        /// //Perform a BCGTransform on a portion of Image using the mask
        /// //Do this operation inplace (store the results in the image in Viewer2)
        /// BcgOptions.Brightness = 200;
        /// BcgOptions.Contrast = 20;
        /// BcgOptions.Gamma = 0.7;
        /// Algorithms.BCGTransform(imageViewer2.Image, imageViewer2.Image, options, mask);
        /// </code>
        /// </example>

        public static void BcgTransform(VisionImage source, VisionImage destination, BcgOptions options, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            CVI_BCGOptions cviOptions = new CVI_BCGOptions();
            cviOptions.ConvertFromExternal(options);
            Utilities.ThrowError(VisionDll.imaqBCGTransform(destination._image, source._image, ref cviOptions, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram of an image and redistributes pixel values across the appropriate range to maintain the 
        /// same pixel value distribution. Pixels whose values are the same before the redistribution also have common pixel 
        /// values after the redistribution.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static void Equalize(VisionImage source, VisionImage destination)
        {
            Equalize(source, destination, null, null, null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram of an image and redistributes pixel values across the appropriate range to maintain the 
        /// same pixel value distribution. Pixels whose values are the same before the redistribution also have common pixel 
        /// values after the redistribution.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="histogramReport">
        /// The histogram of the source image that is supplied from the output of the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Histogram" crefType="Unqualified"/> method. It is necessary to
        /// input the same image to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Histogram" crefType="Unqualified"/> method
        /// and this method. If you pass null or Nothing for <format type="italics">histogramReport</format>, the method computes the histogram from the source image
        /// to equalize the image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static void Equalize(VisionImage source, VisionImage destination, HistogramReport histogramReport)
        {
            Equalize(source, destination, histogramReport, null, null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram of an image and redistributes pixel values across the appropriate range to maintain the 
        /// same pixel value distribution. Pixels whose values are the same before the redistribution also have common pixel 
        /// values after the redistribution.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="histogramReport">
        /// The histogram of the source image that is supplied from the output of the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Histogram" crefType="Unqualified"/> method. It is necessary to
        /// input the same image to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Histogram" crefType="Unqualified"/> method
        /// and this method. If you pass null or Nothing for <format type="italics">histogramReport</format>, the method computes the histogram from the source image
        /// to equalize the image.
        /// </param>
        /// <param name="pixelRange">
        /// The minimum and maximum values for the range of pixel values to equalize.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static void Equalize(VisionImage source, VisionImage destination, HistogramReport histogramReport, Range pixelRange)
        {
            Equalize(source, destination, histogramReport, pixelRange, null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram of an image and redistributes pixel values across the appropriate range to maintain the 
        /// same pixel value distribution. Pixels whose values are the same before the redistribution also have common pixel 
        /// values after the redistribution.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="histogramReport">
        /// The histogram of the source image that is supplied from the output of the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Histogram" crefType="Unqualified"/> method. It is necessary to
        /// input the same image to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Histogram" crefType="Unqualified"/> method
        /// and this method. If you pass null or Nothing for <format type="italics">histogramReport</format>, the method computes the histogram from the source image
        /// to equalize the image.
        /// </param>
        /// <param name="pixelRange">
        /// The minimum and maximum values for the range of pixel values to equalize.
        /// </param>
        /// <param name="mask">
        /// The region in which the method performs the equalization. The method processes only those pixels in the image 
        /// whose corresponding pixels in the mask are non-zero. Pass null or Nothing for this parameter if you want to 
        /// calculate a histogram equalization on the entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim HistogramReport As New HistogramReport
        /// Dim MaskImage As New VisionImage
        ///     
        /// 'Equalize a portion of the image in Viewer1
        /// 'defined by the regions on Viewer1 and store the result in i.
        /// Algorithms.RoiToMask (MaskImage, imageViewer1.Roi)
        ///     
        /// Algorithms.Equalize (imageViewer1.Image, i, Nothing, New Range(0, 255), MaskImage)
        ///     
        /// 'Calculate the histogram of the image in Viewer1 and then use
        /// 'this histogram to perform the equalization.
        /// 'Do the equalization inplace. Store the results in the image in Viewer1.
        /// HistogramReport = Algorithms.Histogram (imageViewer1.Image, 128, New Range(0, 255), MaskImage)
        ///  
        /// Algorithms.Equalize (imageViewer1.Image, imageViewer1.Image, HistogramReport)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// HistogramReport HistogramReport = new HistogramReport();
        /// VisionImage MaskImage = new VisionImage();
        ///     
        /// // Equalize a portion of the image in Viewer1
        /// // defined by the regions on Viewer1 and store the result in i.
        /// Algorithms.RoiToMask(MaskImage, imageViewer1.Roi);
        ///     
        /// Algorithms.Equalize(imageViewer1.Image, i, null, new Range(0, 255), MaskImage);
        ///     
        /// // Calculate the histogram of the image in Viewer1 and then use
        /// // this histogram to perform the equalization.
        /// // Do the equalization inplace. Store the results in the image in Viewer1.
        /// HistogramReport = Algorithms.Histogram (imageViewer1.Image, 128, new Range(0, 255), MaskImage);
        ///     
        /// Algorithms.Equalize (imageViewer1.Image, imageViewer1.Image, HistogramReport);
        /// </code>
        /// </example>

        public static void Equalize(VisionImage source, VisionImage destination, HistogramReport histogramReport, Range pixelRange, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            // Since the order of parameters is weird here, allow null's where we otherwise wouldn't.
            float min = pixelRange != null ? (float)pixelRange.Minimum : 0;
            float max = pixelRange != null ? (float)pixelRange.Maximum : 0;
            if (histogramReport != null)
            {
                Array1D realHistogram = Utilities.ConvertCollectionToArray1D<int>(histogramReport.Histogram);
                Priv_HistographReport privReport = new Priv_HistographReport();
                privReport.ConvertFromExternal(histogramReport, realHistogram);
                try {
                    Utilities.ThrowError(VisionDll.Priv_Equalize(destination._image, source._image, ref privReport, min, max, VisionImage.GetIntPtr(mask)));
                } finally {
                    Marshal.FreeCoTaskMem(realHistogram.Ptr);
                }
            }
            else
            {
                Utilities.ThrowError(VisionDll.Priv_Equalize(destination._image, source._image, IntPtr.Zero, min, max, VisionImage.GetIntPtr(mask)));
            }

        }
        //==========================================================================================
        /// <summary>
        /// Inverts the pixel intensities of an image using the following equation:
        /// <para>
        /// f(p) = dynamicMax - p + dynamicMin
        /// </para>
        /// 	<para>
        /// where
        /// </para>
        /// 	<list type="bullet">
        /// 		<item>
        /// 			<description>
        /// p represents the value of a pixel.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// dynamicMin represents 0 (8-bit images) or the smallest pixel value in the source image 
        /// (16-bit and floating point images).
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// dynamicMax represents 255 (8-bit images) or the largest pixel value in the source image 
        /// (16-bit and floating point images).
        /// </description>
        /// 		</item>
        /// 	</list>
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The results of the operation.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static void Inverse(VisionImage source, VisionImage destination)
        {
            Inverse(source, destination, null);
        }
        //==========================================================================================
        /// <summary>
        /// Inverts the pixel intensities of an image using the following equation:
        /// <para>
        /// f(p) = dynamicMax - p + dynamicMin
        /// </para>
        /// 	<para>
        /// where
        /// </para>
        /// 	<list type="bullet">
        /// 		<item>
        /// 			<description>
        /// p represents the value of a pixel.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// dynamicMin represents 0 (8-bit images) or the smallest pixel value in the source image 
        /// (16-bit and floating point images).
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// dynamicMax represents 255 (8-bit images) or the largest pixel value in the source image 
        /// (16-bit and floating point images).
        /// </description>
        /// 		</item>
        /// 	</list>
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The results of the operation.
        /// </param>
        /// <param name="mask">
        /// The mask to apply to the source image. It indicates the region of the image where the method applies 
        /// the inverse. The method processes only those pixels in the image whose corresponding pixels in the 
        /// mask are non-zero. Pass null or Nothing for this parameter if you want to invert the pixel intensities 
        /// for the entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim maskImage As New VisionImage
        ///  
        /// 'Invert a portion of the image in Viewer1 defined by the ROI on
        /// 'Viewer1 and store the result in i.
        /// Algorithms.RoiToMask (maskImage, imageViewer1.Roi)
        /// Algorithms.Inverse (imageViewer1.Image, i, maskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// VisionImage maskImage = new VisionImage();
        ///     
        /// //Invert a portion of the image in Viewer1 defined by the ROI on
        /// //Viewer1 and store the result in i.
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// Algorithms.Inverse(imageViewer1.Image, i, maskImage);
        /// </code>
        /// </example>

        public static void Inverse(VisionImage source, VisionImage destination, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            Utilities.ThrowError(VisionDll.imaqInverse(destination._image, source._image, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Performs a user-defined lookup table transformation by remapping the pixel values in an image.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="table">
        /// The lookup table. For 8-bit images, the lookup table must contain 256 elements. The method replaces each pixel value 
        /// v with <format type="italics">table</format>(v). For 16-bit images, the lookup table must contain 65,536 elements. The method replaces each 
        /// non-negative pixel value v with <format type="italics">table</format>(v) and replaces each negative pixel value v 
        /// with <format type="italics">table</format>(65536+v).
        /// </param>
        /// <remarks>
        /// Use this method with U8 and I16 images.
        /// </remarks>

        public static void UserLookup(VisionImage source, VisionImage destination, Collection<Int16> table)
        {
            UserLookup(source, destination, table, null);
        }
        //==========================================================================================
        /// <summary>
        /// Performs a user-defined lookup table transformation by remapping the pixel values in an image.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="table">
        /// The lookup table. For 8-bit images, the lookup table must contain 256 elements. The method replaces each pixel value 
        /// v with <format type="italics">table</format>(v). For 16-bit images, the lookup table must contain 65,536 elements. The method replaces each 
        /// non-negative pixel value v with <format type="italics">table</format>(v) and replaces each negative pixel value v 
        /// with <format type="italics">table</format>(65536+v).
        /// </param>
        /// <param name="mask">
        /// The region in the image to use for the user lookup. The method processes only those pixels in the 
        /// image whose corresponding pixels in the mask are non-zero. Pass null of Nothing for this parameter 
        /// if you want to transform the entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 and I16 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim LookupTable As New Collection(Of Short)
        /// Dim i As New VisionImage
        /// Dim maskImage As New VisionImage
        ///  
        /// ' Create a lookup table
        /// For i1 As Integer = 0 To 255
        ///     LookupTable.Add(Math.Abs(128 - i1))
        /// Next
        ///  
        /// ' Perform a UserLookup on the entire image in Viewer1.
        /// ' Store the result in i.
        /// Algorithms.UserLookup (imageViewer1.Image, i, LookupTable)
        ///  
        /// ' Perform a UserLookup on the portion of the image
        /// ' in Viewer1 based on the ROI selected on Viewer1.
        /// ' Do the operation inplace.
        /// Algorithms.RoiToMask (maskImage, imageViewer1.Roi)
        /// Algorithms.UserLookup (imageViewer1.Image, imageViewer1.Image, LookupTable, maskImage)
        ///  
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// Collection&lt;short&gt; lookupTable = new Collection&lt;short&gt;();
        /// VisionImage i = new VisionImage();
        /// VisionImage maskImage = new VisionImage();
        ///  
        /// // Create a lookup table
        /// for (int i1 = 0; i &lt;= 255; ++i) {
        ///     lookupTable.Add(Math.Abs(128-i1));
        /// }
        ///  
        /// // Perform a UserLookup on the entire image in Viewer1.
        /// // Store the result in i.
        /// Algorithms.UserLookup(imageViewer1.Image, i, lookupTable);
        ///  
        /// // Perform a UserLookup on the portion of the image
        /// // in Viewer1 based on the ROI selected on Viewer1.
        /// // Do the operation inplace.
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// Algorithms.UserLookup(imageViewer1.Image, imageViewer1.Image, lookupTable, maskImage);
        /// </code>
        /// </example>

        public static void UserLookup(VisionImage source, VisionImage destination, Collection<Int16> table, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (table == null) { throw new ArgumentNullException("table"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            int tableSize = 256;
            if (source.Type == ImageType.I16) { tableSize = 65536; }
            // If the table is the correct size, just do the conversion; otherwise, pad it first.
            Int16[] cviTable;
            if (table.Count >= tableSize)
            {
                cviTable = Utilities.ConvertCollectionToArray<Int16>(table);
            }
            else
            {
                cviTable = new Int16[tableSize];
                for (int i = 0; i < table.Count; ++i)
                {
                    cviTable[i] = table[i];
                }
                for (int i = table.Count; i < tableSize; ++i)
                {
                    cviTable[i] = (i <= 32768) ? (Int16)i : (Int16)(i - 65536);
                }
            }
            Utilities.ThrowError(VisionDll.imaqLookup(destination._image, source._image, cviTable, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Converts the pixel values of an image by replacing them with values from a defined lookup table. 
        /// This method modifies the dynamic range of either part of an image or the complete image, depending 
        /// on the type of curve chosen.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="mathOperator">
        /// The transform function to use.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static void MathLookup(VisionImage source, VisionImage destination, MathLookupOperator mathOperator)
        {
            MathLookup(source, destination, mathOperator, 0, new Range(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Converts the pixel values of an image by replacing them with values from a defined lookup table. 
        /// This method modifies the dynamic range of either part of an image or the complete image, depending 
        /// on the type of curve chosen.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="mathOperator">
        /// The transform function to use.
        /// </param>
        /// <param name="x">
        /// Specifies the power to which the method raises the pixel value.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static void MathLookup(VisionImage source, VisionImage destination, MathLookupOperator mathOperator, double x)
        {
            MathLookup(source, destination, mathOperator, x, new Range(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Converts the pixel values of an image by replacing them with values from a defined lookup table. 
        /// This method modifies the dynamic range of either part of an image or the complete image, depending 
        /// on the type of curve chosen.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="mathOperator">
        /// The transform function to use.
        /// </param>
        /// <param name="x">
        /// Specifies the power to which the method raises the pixel value.
        /// </param>
        /// <param name="pixelRange">
        /// The range of pixel values on which the method applies the transform. If this range is [0,0], the method will use the minimum and maximum for the image type.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static void MathLookup(VisionImage source, VisionImage destination, MathLookupOperator mathOperator, double x, Range pixelRange)
        {
            MathLookup(source, destination, mathOperator, x, pixelRange, null);
        }
        //==========================================================================================
        /// <summary>
        /// Converts the pixel values of an image by replacing them with values from a defined lookup table. 
        /// This method modifies the dynamic range of either part of an image or the complete image, depending 
        /// on the type of curve chosen.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="mathOperator">
        /// The transform function to use.
        /// </param>
        /// <param name="x">
        /// Specifies the power to which the method raises the pixel value.
        /// </param>
        /// <param name="pixelRange">
        /// The range of pixel values on which the method applies the transform. If this range is [0,0], the method will use the minimum and maximum for the image type.
        /// </param>
        /// <param name="mask">
        /// Specifies the region in the image to use for the math lookup. The method processes only those pixels 
        /// in the image whose corresponding pixels in the mask are non-zero. Pass null or Nothing for this 
        /// parameter if you want to apply the transfer function to the entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim maskImage As New VisionImage
        ///  
        /// 'Apply a transformation function to a portion of the image in Viewer1
        /// 'defined by regions selected on Viewer1.
        /// 'Store the result in i.
        /// Algorithms.RoiToMask (maskImage, imageViewer1.Roi)
        /// Algorithms.MathLookup (imageViewer1.Image, i, MathLookupOperator.Pow1X, 5, New Range (), maskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// VisionImage maskImage = new VisionImage();
        ///     
        /// //Apply a transformation function to a portion of the image in Viewer1
        /// //defined by regions selected on Viewer1.
        /// //Store the result in i.
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// Algorithms.MathLookup (imageViewer1.Image, i, MathLookupOperator.Pow1X, 5, new Range(), maskImage);
        /// </code>
        /// </example>

        public static void MathLookup(VisionImage source, VisionImage destination, MathLookupOperator mathOperator, double x, Range pixelRange, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (pixelRange == null) { throw new ArgumentNullException("pixelRange"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            Utilities.ThrowError(VisionDll.imaqMathTransform(destination._image, source._image, mathOperator, (float)pixelRange.Minimum, (float)pixelRange.Maximum, (float)x, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the watershed transform of an image. The watershed transfrom outputs an image in which the same number 
        /// (label) is assigned to all the pixels that belong to the same catchment basin of the topographic surface of the 
        /// input image, and 0 is assigned to all the pixels in the watershed line.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <returns>
        /// The number of zones detected in the image. A zone is a region of the image in which all pixels belong to the same 
        /// catchment basin. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 and I16 images. <format type="italics">destination</format> must be a U8 or I16 image. 
        /// If you use a U8 <format type="italics">destination</format> image, the method can store up to 255 unique labels 
        /// not including the watershed line value of 0. If you use a I16 <format type="italics">destination</format> image, 
        /// the method can store up to 32,767 unique labels not including the watershed line value of 0.
        /// </remarks>

        public static Int32 WatershedTransform(VisionImage source, VisionImage destination)
        {
            return WatershedTransform(source, destination, Connectivity.Connectivity8);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the watershed transform of an image. The watershed transfrom outputs an image in which the same number 
        /// (label) is assigned to all the pixels that belong to the same catchment basin of the topographic surface of the 
        /// input image, and 0 is assigned to all the pixels in the watershed line.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether adjacent pixels belong to the same 
        /// catchment basin or watershed line. Set this parameter to Connectivity4 to use connectivity-4 to determine whether 
        /// adjacent pixels belong to the same catchment basin or watershed line. The default is Connectivity8. 
        /// For more information about connectivity, refer to the <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <returns>
        /// The number of zones detected in the image. A zone is a region of the image in which all pixels belong to the same 
        /// catchment basin. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 and I16 images. <format type="italics">destination</format> must be a U8 or I16 image. 
        /// If you use a U8 <format type="italics">destination</format> image, the method can store up to 255 unique labels 
        /// not including the watershed line value of 0. If you use a I16 <format type="italics">destination</format> image, 
        /// the method can store up to 32,767 unique labels not including the watershed line value of 0.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Perform a watershed transformation on the image in viewer1 to i.
        /// ' Use connectivity-4
        /// Algorithms.WatershedTransform (imageViewer1.Image, i, connectivity.Connectivity4)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // Perform a watershed transformation on the image in viewer1 to i.
        /// // Use connectivity-4
        /// Algorithms.WatershedTransform(imageViewer1.Image, i, connectivity.Connectivity4);
        /// </code>
        /// </example>

        public static Int32 WatershedTransform(VisionImage source, VisionImage destination, Connectivity connectivity)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            int zoneCount;
            Utilities.ThrowError(VisionDll.imaqWatershedTransform(destination._image, source._image, connectivity, out zoneCount));
            return zoneCount;
        }
        #endregion

        #region Utilities functions
        //==========================================================================================
        /// <summary>
        /// This property allows you to specify how many processors all of your vision functions should take advantage of. The default is to use as many cores as possible. You may use this to get/set the number of cores to use. If a custom value is set to 0, this indicates use as many as possible, and the user is only allowed to specify as many cores as available by the operating system at maximum.
        /// </summary>
        /// <value>
        /// </value>

        [CLSCompliant(false)]
        public static UInt32 CoresUsed
        {
            get
            {
                UInt32 numCores = 0;
                Utilities.ThrowError(VisionDll.imaqMulticoreOptions(CVI_MulticoreOperation.GetCores, ref numCores));
                return numCores;
            }
            set
            {
                Utilities.ThrowError(VisionDll.imaqMulticoreOptions(CVI_MulticoreOperation.SetCores, ref value));
            }
        }
        #endregion

        #region Image Manipulation functions
        //==========================================================================================
        /// <summary>
        /// Changes the type of an image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="type">
        /// The new type for the image.
        /// </param>
        /// <remarks>
        /// 	<para>
        /// This method operates with all image types.
        /// </para>
        /// 	<para>
        /// This method can perform the change directly on the source image, or it can leave the source image unchanged and instead copy the source image to a destination image and then convert the destination image. If the destination image is the same as the source image, the method changes the type of the source image. Otherwise, the method resizes the destination image to the size of source image and then copies the pixels.
        /// </para>
        /// 	<para>
        /// To change the type of an image in-place you can set its <see cref="NationalInstruments.Vision.VisionImage.Type" crefType="Unqualified"/> to the required type. If the source image type and the type parameter are the same, the method copies pixels unmodified. You can also use the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method to copy pixels without modifying them. 
        /// </para>
        /// </remarks>

        public static void Cast(VisionImage source, VisionImage destination, ImageType type)
        {
            CommonAlgorithms.Cast(source, destination, type);
        }
        //==========================================================================================
        /// <summary>
        /// Changes the type of an image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="type">
        /// The new type for the image.
        /// </param>
        /// <param name="lookupTable">A lookup table. Refer to the Remarks section for a description of how the method uses the lookup table.
        /// </param>
        /// <remarks>
        /// 	<para>
        /// This method operates with all image types.
        /// </para>
        /// 	<para>
        /// This method can perform the change directly on the source image, or it can leave the source image unchanged and instead copy the source image to a destination image and then convert the destination image. If the destination image is the same as the source image, the method changes the type of the source image. Otherwise, the method resizes the destination image to the size of source image and then copies the pixels.
        /// </para>
        /// 	<para>
        /// To change the type of an image in-place without supplying a shift value or a lookup table, you can set the <see cref="NationalInstruments.Vision.VisionImage.Type" crefType="Unqualified"/> property to the required type. If the source image type and the <format type="bold">Type</format> crefType="Unqualified" /&gt; property are the same, the method copies pixels unmodified. You can also use the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method to copy pixels without modifying them.  If the source image type and the <format type="bold">Type</format> property are not the same, the method casts the pixel values to the new type as follows:
        /// </para>
        /// 	<list type="table">
        /// 		<listheader>
        /// 			<term>source Type</term>
        /// 			<description>type Parameter</description>
        /// 		</listheader>
        /// 		<item>
        /// 			<term><list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 				</list></term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// If you provide a lookup table, the destination pixel will have the lookup value of the source pixel. If you do not provide a lookup table, the method copies the source value to the destination unmodified.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Rgb32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// Each color component of the destination is set to the source value. If the source value is greater than 255, the method sets each color component to 255. If the source value is less than 0, the method sets each color component to 0.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Hsl32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the luminance component of the destination to the source value. If the source value is greater than 255, the method sets the luminance to 255. If the source value is less than 0, the method sets the luminance to 0. The method sets hue and saturation component of the destination to 0. If the source image has a specified bit depth, the method uses the bit depth when performing this conversion.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Complex
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the real component of the destination to the source value. The method sets the imaginary component of the destination to 0.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method right-shifts the source value by the given shift value (divides each source pixel value by 2 shifts) and stores the value in the destination. If the shifted value is greater than 255, the method sets the destination value to 255. If the shift value is zero, the method uses the specified bit depth of the source image.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.RgbU64
        /// </description>
        /// 					</item>
        /// 				</list>
        /// Each color component of the destination is set to the source value. If the source value is greater than 65535, the method sets each color component to 65535. If the source value is less than 0, the method sets each color component to 0.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.RgbU64
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method shifts the source value to the 8-bit range using the specified bit depth of the source image. Then the method sets the destination value to the average of the three color components of the source.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.RgbU64
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the destination value to the average of the three color components of the source. If the average of the source color components is out of the range of the destination, the method coerces the average to the range.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.RgbU64
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Rgb32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method shifts the source value to the 8-bit range using the specified bit depth of the source image. Then the method sets each color component in the destination value to the corresponding component in the source value.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.RgbU64
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Hsl32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method shifts the source value to the 8-bit range using the specified bit depth of the source image. Then the method converts each pixel from the RGB color space to the HSL color space.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// If you provide a lookup table, the destination pixel will have the lookup value of the source pixel. If you do not provide a lookup table, the method copies the source value to the destination unmodified.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the destination value to the source value. If the source value is out of the range of the destination, the method coerces the source to the range.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Rgb32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the destination value to the average of the three color components of the source.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Rgb32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Hsl32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method converts each pixel from the RGB color space to the HSL color space.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Rgb32
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.RgbU64
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Complex
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the real portion of the destination value to the average of the three color components of the source, and it sets the imaginary portion of the destination to 0.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Hsl32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the destination value to the luminance component of the source value.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Hsl32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Rgb32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method converts each pixel from the HSL color space to the RGB color space.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Hsl32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Complex
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the real portion of the destination value to the value of the luminance component of the source, and it sets the imaginary portion of the destination to 0. If the source value is out of the range of the destination, the method coerces the source to the range.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Complex
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U8
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.Single
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the destination value to the magnitude of the source value. If the source value is out of the range of the destination, the method coerces the source to the range.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Complex
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Rgb32
        /// </description>
        /// 					</item>
        /// 					<item>
        /// 						<description>
        /// ImageType.RgbU64
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets each color component of the destination value to the magnitude of the source value. If the source value is out of the range of the destination, the method coerces the source to the range.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Complex
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.Hsl32
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the luminance component of the destination value to the magnitude of the source value, and it sets the hue and saturation components to 0. If the source value is out of the range of the destination, the method coerces the source to the range.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the destination value to the source value. If the source value is out of the range of the destination, the method coerces the source to the range.</description>
        /// 		</item>
        /// 		<item>
        /// 			<term>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.I16
        /// </description>
        /// 					</item>
        /// 				</list>
        /// 			</term>
        /// 			<description>
        /// 				<list type="bullet">
        /// 					<item>
        /// 						<description>
        /// ImageType.U16
        /// </description>
        /// 					</item>
        /// 				</list>
        /// The method sets the destination value to the source value. If the source value is out of the range of the destination, the method coerces the source to the range.</description>
        /// 		</item>
        /// 	</list>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim i As New VisionImage
        /// Dim lookupTable As New Collection(Of Single)
        /// Dim index As Integer
        ///  
        /// ' Change the image in Viewer1 to an Rgb32 image and 
        /// ' store the results in i
        /// Algorithms.Cast (imageViewer1.Image, i, ImageType.Rgb32)
        ///  
        /// ' Populate the lookupTable with a form of inverse transformation
        /// For index = 0 to 255
        ///     lookupTable.Add(32767 - (index * 16))
        /// Next
        ///  
        /// ' Change the image in Viewer1 to a 16-bit image using the lookup table
        /// Algorithms.Cast (imageViewer1.Image, imageViewer1.Image, ImageType.I16, lookupTable)
        ///  
        /// ' Change the image in Viewer1 to an Single image
        /// ' You can use the Type property on the image since there is no need
        /// ' for a lookup table or shift value.
        /// imageViewer1.Image.Type = ImageType.Single
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage i = new VisionImage();
        /// Collection&lt;float&gt; lookupTable = new Collection&lt;float&gt;();
        ///  
        /// // Change the image in imageViewer1 to an Rgb32 image and
        /// // store the results in i
        /// Algorithms.Cast (imageViewer1.Image, i, ImageType.Rgb32);
        ///  
        /// // Populate the lookupTable with a form of inverse transformation
        /// for (int index = 0; index &lt;= 255; ++index) {
        ///     lookupTable.Add(32767 - (index * 16));
        /// }
        ///  
        /// // Change the image in imageViewer1 to a 16-bit image using the lookup table
        /// Algorithms.Cast (imageViewer1.Image, imageViewer1.Image, ImageType.I16, lookupTable);
        ///  
        /// // Change the image in imageViewer1 to a Single image
        /// // You can use the Type property on the image since there is no need
        /// // for a lookup table or shift value.
        /// imageViewer1.Image.Type = ImageType.Single;
        /// </code>
        /// </example>

        public static void Cast(VisionImage source, VisionImage destination, ImageType type, Collection<float> lookupTable)
        {
            CommonAlgorithms.Cast(source, destination, type, lookupTable);
        }
        //==========================================================================================
        /// <summary>
        /// Changes the type of an image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="type">
        /// The new type for the image.
        /// </param>
        /// <param name="numberOfShifts">
        /// The shift value for converting 16-bit images to 8-bit images. The method ignores this value for all other conversions. The method executes this conversion by shifting the 16-bit pixel values to the right by the specified number of shift operations, up to a maximum of 8 shift operations, and then truncating to get an 8-bit value. Enter a value of ? to ignore the bit depth and shift 0. Enter a value of 0 to use the bit depth to cast the image. Refer to the Remarks section for a description of how the function employs the shift value.
        /// This parameter has a default value of 0.
        /// </param>
        /// <remarks>
        /// 	<para>
        /// This method operates with all image types.
        /// </para>
        /// 	<para>
        /// This method can perform the change directly on the source image, or it can leave the source image unchanged and instead copy the source image to a destination image and then convert the destination image. If the destination image is the same as the source image, the method changes the type of the source image. Otherwise, the method resizes the destination image to the size of source image and then copies the pixels.
        /// </para>
        /// 	<para>
        /// To change the type of an image in-place without supplying a shift value or a lookup table, you can set the <format type="italics">type</format> to the required type. If the source image type and the <format type="italics">type</format> parameter are the same, the method copies pixels unmodified. You can also use the Copy method to copy pixels without modifying them. 
        /// </para>
        /// </remarks>

        public static void Cast(VisionImage source, VisionImage destination, ImageType type, Int32 numberOfShifts)
        {
            CommonAlgorithms.Cast(source, destination, type, numberOfShifts);
        }
        //==========================================================================================
        /// <summary>
        /// Copies the source image to the destination image, including the border size and calibration information. To copy an area of one image to an area of another image, use the <see cref="NationalInstruments.Vision.Analysis.Algorithms.Extract" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="source">
        /// The source image to copy.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with all image types. The method copies the complete definition of the source image and its pixel data to the destination image. The method also modifies the border of the destination image so that it becomes the same size as the border of the source image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// ' Copy the image in ImageViewer1 into i
        /// Algorithms.Copy (ImageViewer1.Image, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// // Copy the image in imageViewer1 into i
        /// Algorithms.Copy(imageViewer1.Image, i);
        /// </code>
        /// </example>

        public static void Copy(VisionImage source, VisionImage destination)
        {
            CommonAlgorithms.Copy(source, destination);
        }

        //==========================================================================================
        /// <summary>
        /// Extracts (reduces) an image or part of an image with adjustment of the horizontal and vertical resolution.
        /// </summary>
        /// <param name="source">
        /// The image to extract.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="rectangle">
        /// The portion of the image that the method extracts. Pass RectanlgeContour.None to operate on the entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void Extract(VisionImage source, VisionImage destination, RectangleContour rectangle)
        {
            Extract(source, destination, rectangle, 1, 1);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts (reduces) an image or part of an image with adjustment of the horizontal and vertical resolution.
        /// </summary>
        /// <param name="source">
        /// The image to extract.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="rectangle">
        /// The portion of the image that the method extracts. This parameter must contain 0 or 1 contours. 
        /// If it contains 1 contour, it must be a RectangleContour. Pass RectanlgeContour.None to operate on the entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void Extract(VisionImage source, VisionImage destination, Roi rectangle)
        {
            Extract(source, destination, rectangle, 1, 1);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts (reduces) an image or part of an image with adjustment of the horizontal and vertical resolution.
        /// </summary>
        /// <param name="source">
        /// The image to extract.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="rectangle">
        /// The portion of the image that the method extracts. Pass RectanlgeContour.None to operate on the entire image.
        /// </param>
        /// <param name="xSubsample">
        /// The horizontal sampling step. This parameter defines the columns to extract (the horizontal reduction ratio). 
        /// For example, set this parameter to 3 to extract one out of every three columns from the 
        /// <format type="italics">source</format> image into the <format type="italics">destination</format> image. 
        /// The method copies the pixel columns to the <format type="italics">destination</format> image without subsampling if you use the default value.
        /// This parameter has a default value of 1.
        /// </param>
        /// <param name="ySubsample">
        /// The vertical sampling step. This parameter defines the rows to extract (the vertical reduction ratio). 
        /// For example, set this parameter to 3 to extract one out of every three rows from the 
        /// <format type="italics">source</format> image into the <format type="italics">destination</format> image. 
        /// The method copies the pixel rows to the <format type="italics">destination</format> image without subsampling if you use the default value. This 
        /// parameter has a default value of 1.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim r As New RectangleContour
        ///  
        /// ' Reduce a 50 x 50 rectangular portion of the image in Viewer1
        /// ' starting at (20,30) by a factor of 2.
        /// ' Store the result in i.
        /// r.Initialize (20, 30, 50, 50)
        /// Algorithms.Extract (imageViewer1.Image, i, r, 2, 2)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// RectangleContour r = new RectangleContour();
        ///     
        /// // Reduce a 50 x 50 rectangular portion of the image in Viewer1
        /// // starting at (20,30) by a factor of 2.
        /// // Store the result in i.
        /// r.Initialize(20, 30, 50, 50);
        /// Algorithms.Extract(imageViewer1.Image, i, r, 2, 2);
        /// </code>
        /// </example>

        public static void Extract(VisionImage source, VisionImage destination, RectangleContour rectangle, Int32 xSubsample, Int32 ySubsample)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (rectangle == null) { throw new ArgumentNullException("rectangle"); }
            CVI_Rectangle cviRect = new CVI_Rectangle();
            cviRect.ConvertFromExternal(rectangle);
            Utilities.ThrowError(VisionDll.imaqScale(VisionImage.GetIntPtr(destination), source._image, xSubsample, ySubsample, CVI_ScalingMode.Smaller, cviRect));
        }
        //==========================================================================================
        /// <summary>
        /// Extracts (reduces) an image or part of an image with adjustment of the horizontal and vertical resolution.
        /// </summary>
        /// <param name="source">
        /// The image to extract.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="rectangle">
        /// The portion of the image that the method extracts. This parameter must contain 0 or 1 contours. 
        /// If it contains 1 contour, it must be a RectangleContour. Pass RectanlgeContour.None to operate on the entire image.
        /// </param>
        /// <param name="xSubsample">
        /// The horizontal sampling step. This parameter defines the columns to extract (the horizontal reduction ratio). 
        /// For example, set this parameter to 3 to extract one out of every three columns from the 
        /// <format type="italics">source</format> image into the <format type="italics">destination</format> image. 
        /// The method copies the pixel columns to the <format type="italics">destination</format> image without subsampling if you use the default value.
        /// This parameter has a default value of 1.
        /// </param>
        /// <param name="ySubsample">
        /// The vertical sampling step. This parameter defines the rows to extract (the vertical reduction ratio). 
        /// For example, set this parameter to 3 to extract one out of every three rows from the 
        /// <format type="italics">source</format> image into the <format type="italics">destination</format> image. 
        /// The method copies the pixel rows to the <format type="italics">destination</format> image without subsampling if you use the default value. This 
        /// parameter has a default value of 1.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void Extract(VisionImage source, VisionImage destination, Roi rectangle, Int32 xSubsample, Int32 ySubsample)
        {
            Roi.ThrowIfNonNullAndDisposed(rectangle);
            Extract(source, destination, Utilities.ConvertRoiToRectangle(rectangle), xSubsample, ySubsample);
        }

        //==========================================================================================
        /// <summary>
        /// Copies a small image into a larger image, which is useful for making thumbnail sketches from multiple miniature 
        /// images.
        /// </summary>
        /// <param name="source">
        /// The source image to copy.
        /// </param>
        /// <param name="destination">
        /// The destination image into which the method copies the source image.
        /// </param>
        /// <remarks>
        /// This function operates on all image types. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> images must be of the same type.
        /// </remarks>

        public static void ImageToImage(VisionImage source, VisionImage destination)
        {
            ImageToImage(source, destination, new PointContour(), RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Copies a small image into a larger image, which is useful for making thumbnail sketches from multiple miniature 
        /// images.
        /// </summary>
        /// <param name="source">
        /// The source image to copy.
        /// </param>
        /// <param name="destination">
        /// The destination image into which the method copies the source image.
        /// </param>
        /// <param name="destinationPoint">
        /// The coordinates in the <format type="italics">destination</format> image where the method copies the 
        /// <format type="italics">source</format> image.
        /// </param>
        /// <remarks>
        /// This function operates on all image types. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> images must be of the same type.
        /// </remarks>

        public static void ImageToImage(VisionImage source, VisionImage destination, PointContour destinationPoint)
        {
            ImageToImage(source, destination, destinationPoint, RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Copies a small image into a larger image, which is useful for making thumbnail sketches from multiple miniature 
        /// images.
        /// </summary>
        /// <param name="source">
        /// The source image to copy.
        /// </param>
        /// <param name="destination">
        /// The destination image into which the method copies the source image.
        /// </param>
        /// <param name="destinationPoint">
        /// The coordinates in the <format type="italics">destination</format> image where the method copies the 
        /// <format type="italics">source</format> image.
        /// </param>
        /// <param name="sourceRect">
        /// Specifies the region of the small image that will be copied.
        /// </param>
        /// <remarks>
        /// This function operates on all image types. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> images must be of the same type.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// 'This example copies a 50 x 100 square with pixel value 255
        /// 'into the image in Viewer1.
        ///  
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Populate i
        /// i.SetSize (50, 100)
        ///  
        /// 'Copy i into the image in Viewer1
        /// i.FillImage (New PixelValue (255))
        /// Algorithms.ImageToImage (i, imageViewer1.Image, New PointContour(20, 40))
        /// </code>
        /// 	<code lang="C#">
        /// //This example copies a 50 x 100 square with pixel value 255
        /// //into the image in Viewer1.
        ///  
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Populate i
        /// i.SetSize(50, 100);
        ///     
        /// //Copy i into the image in Viewer1
        /// i.FillImage(new PixelValue(255));
        /// Algorithms.ImageToImage(i, imageViewer1.Image, new PointContour(20, 40));
        /// </code>
        /// </example>

        public static void ImageToImage(VisionImage source, VisionImage destination, PointContour destinationPoint, RectangleContour sourceRect)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (destinationPoint == null) { throw new ArgumentNullException("destinationPoint"); }
            if (sourceRect  == null) { throw new ArgumentNullException("sourceRect"); }
            CVI_Rectangle cviRect = new CVI_Rectangle();
            cviRect.ConvertFromExternal(sourceRect);
            CVI_Point cviPoint = new CVI_Point();
            cviPoint.ConvertFromExternal(destinationPoint);
            Utilities.ThrowError(VisionDll.imaqCopyRect(destination._image, source._image, cviRect, cviPoint));
        }

        //==========================================================================================
        /// <summary>
        /// Resamples an image to a user-defined size. You can use this method to display a reduced or enlarged image.
        /// </summary>
        /// <param name="source">
        /// The image to resample.
        /// </param>
        /// <param name="destination">
        /// The image into which the method places the resampled data. This image may be the same as 
        /// <format type="italics">source</format>.
        /// </param>
        /// <param name="newWidth">
        /// The width of the resampled area. 
        /// </param>
        /// <param name="newHeight">
        /// The height of the resampled area. 
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Hsl, and Rgb32 images. <format type="italics">source</format> 
        /// and <format type="italics">destination</format> must be the same type of image.
        /// </remarks>

        public static void Resample(VisionImage source, VisionImage destination, Int32 newWidth, Int32 newHeight)
        {
            Resample(source, destination, newWidth, newHeight, InterpolationMethod.ZeroOrder, RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Resamples an image to a user-defined size. You can use this method to display a reduced or enlarged image.
        /// </summary>
        /// <param name="source">
        /// The image to resample.
        /// </param>
        /// <param name="destination">
        /// The image into which the method places the resampled data. This image may be the same as 
        /// <format type="italics">source</format>.
        /// </param>
        /// <param name="newWidth">
        /// The width of the resampled area. 
        /// </param>
        /// <param name="newHeight">
        /// The height of the resampled area. 
        /// </param>
        /// <param name="method">
        /// Specifies the type of interpolation used to resample the image. The default is ZeroOrder.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Hsl, and Rgb32 images. <format type="italics">source</format> 
        /// and <format type="italics">destination</format> must be the same type of image.
        /// </remarks>

        public static void Resample(VisionImage source, VisionImage destination, Int32 newWidth, Int32 newHeight, InterpolationMethod method)
        {
            Resample(source, destination, newWidth, newHeight, method, RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Resamples an image to a user-defined size. You can use this method to display a reduced or enlarged image.
        /// </summary>
        /// <param name="source">
        /// The image to resample.
        /// </param>
        /// <param name="destination">
        /// The image into which the method places the resampled data. This image may be the same as 
        /// <format type="italics">source</format>.
        /// </param>
        /// <param name="newWidth">
        /// The width of the resampled area. 
        /// </param>
        /// <param name="newHeight">
        /// The height of the resampled area. 
        /// </param>
        /// <param name="method">
        /// Specifies the type of interpolation used to resample the image. The default is ZeroOrder.
        /// </param>
        /// <param name="rectangle">
        /// Specifies the region of interest to resample. Pass null or Nothing for this parameter to resample the 
        /// entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Hsl, and Rgb32 images. <format type="italics">source</format> 
        /// and <format type="italics">destination</format> must be the same type of image.
        /// </remarks>

        public static void Resample(VisionImage source, VisionImage destination, Int32 newWidth, Int32 newHeight, InterpolationMethod method, Roi rectangle)
        {
            Roi.ThrowIfNonNullAndDisposed(rectangle);
            Resample(source, destination, newWidth, newHeight, method, Utilities.ConvertRoiToRectangle(rectangle));
        }
        //==========================================================================================
        /// <summary>
        /// Resamples an image to a user-defined size. You can use this method to display a reduced or enlarged image.
        /// </summary>
        /// <param name="source">
        /// The image to resample.
        /// </param>
        /// <param name="destination">
        /// The image into which the method places the resampled data. This image may be the same as 
        /// <format type="italics">source</format>.
        /// </param>
        /// <param name="newWidth">
        /// The width of the resampled area. 
        /// </param>
        /// <param name="newHeight">
        /// The height of the resampled area. 
        /// </param>
        /// <param name="method">
        /// Specifies the type of interpolation used to resample the image. The default is ZeroOrder.
        /// </param>
        /// <param name="rectangle">
        /// Specifies the rectangular area to resample. Pass null or Nothing for this parameter to resample the 
        /// entire image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Hsl, and Rgb32 images. <format type="italics">source</format> 
        /// and <format type="italics">destination</format> must be the same type of image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// 'Enlarge the image in Viewer1 to twice its original size.
        /// 'Store the result in i.
        /// Algorithms.Resample (imageViewer1.Image, i, imageViewer1.Image.Width * 2, imageViewer1.Image.Height * 2, InterpolationMethod.BiLinear)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// //Enlarge the image in Viewer1 to twice its original size.
        /// //Store the result in i.
        /// Algorithms.Resample(imageViewer1.Image, i, imageViewer1.Image.Width * 2, imageViewer1.Image.Height * 2, InterpolationMethod.BiLinear);
        /// </code>
        /// </example>

        public static void Resample(VisionImage source, VisionImage destination, Int32 newWidth, Int32 newHeight, InterpolationMethod method, RectangleContour rectangle)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (rectangle == null) { throw new ArgumentNullException("rectangle"); }
            CVI_Rectangle cviRect = new CVI_Rectangle();
            cviRect.ConvertFromExternal(rectangle);
            Utilities.ThrowError(VisionDll.imaqResample(VisionImage.GetIntPtr(destination), source._image, newWidth, newHeight, (Int32)method, cviRect));
        }
        //==========================================================================================
        /// <summary>
        /// Transforms an image through its symmetry.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void Symmetry(VisionImage source, VisionImage destination)
        {
            Symmetry(source, destination, SymmetryOperation.Horizontal);
        }
        //==========================================================================================
        /// <summary>
        /// Transforms an image through its symmetry.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="operation">
        /// Specifies the type of symmetry to use. The default is Horizontal.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///     
        /// 'Transform the image in Viewer1 about its center and
        /// 'store the result in i.
        /// Algorithms.Symmetry (imageViewer1.Image, i, SymmetryOperation.Central)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Transform the image in Viewer1 about its center and
        /// //store the result in i.
        /// Algorithms.Symmetry(imageViewer1.Image, i, SymmetryOperation.Central);
        /// </code>
        /// </example>

        public static void Symmetry(VisionImage source, VisionImage destination, SymmetryOperation operation)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqFlip(destination._image, source._image, operation));
        }
        //==========================================================================================
        /// <summary>
        /// Copies the source image to the destination image in the following manner: If a pixel in the mask has a 
        /// value of 0, the function sets the corresponding source pixel to 0. Otherwise, the function copies the 
        /// corresponding source pixel to the destination image. 
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="mask">
        /// The binary image that contains the mask applied to the source image. All non-zero pixel values are on, 
        /// and all pixel values of 0 are off.
        /// </param>
        /// <remarks>
        /// Use this method with all image types. <format type="italics">source</format> and 
        /// <format type="italics">source</format> must be the same type. <format type="italics">mask</format> 
        /// must be a U8 image. This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim maskImage As New VisionImage
        ///  
        /// 'Mask the image in Viewer1 and store the result in i
        /// Algorithms.RoiToMask (maskImage, imageViewer1.Roi)
        /// Algorithms.Mask (imageViewer1.Image, i, maskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// VisionImage maskImage = new VisionImage();
        ///    
        /// //Mask the image in Viewer1 and store the result in i
        /// Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        /// Algorithms.Mask(imageViewer1.Image, i, maskImage);
        /// </code>
        /// </example>

        public static void Mask(VisionImage source, VisionImage destination, VisionImage mask) {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (mask == null) { throw new ArgumentNullException("mask"); }
            mask.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqMask(destination._image, source._image, mask._image));
        }
        //==========================================================================================
        /// <summary>
        /// Rotates an image counterclockwise.
        /// </summary>
        /// <param name="source">
        /// The image to rotate.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="angle">
        /// The angle, in degrees, to rotate the image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images.
        /// </remarks>

        public static void Rotate(VisionImage source, VisionImage destination, double angle)
        {
            // We have to check source for null here since we need the image type.
            if (source == null) { throw new ArgumentNullException("source"); }
            Rotate(source, destination, angle, new PixelValue(source.Type), InterpolationMethod.Bilinear, true);
        }
        //==========================================================================================
        /// <summary>
        /// Rotates an image counterclockwise.
        /// </summary>
        /// <param name="source">
        /// The image to rotate.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="angle">
        /// The angle, in degrees, to rotate the image.
        /// </param>
        /// <param name="fill">
        /// The value with which the method fills the image pixels not covered by the rotated image.
        /// This parameter has a default value of 0.0.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images.
        /// </remarks>

        public static void Rotate(VisionImage source, VisionImage destination, double angle, PixelValue fill)
        {
            Rotate(source, destination, angle, fill, InterpolationMethod.Bilinear, true);
        }
        //==========================================================================================
        /// <summary>
        /// Rotates an image counterclockwise.
        /// </summary>
        /// <param name="source">
        /// The image to rotate.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="angle">
        /// The angle, in degrees, to rotate the image.
        /// </param>
        /// <param name="fill">
        /// The value with which the method fills the image pixels not covered by the rotated image.
        /// This parameter has a default value of 0.0.
        /// </param>
        /// <param name="method">
        /// The method of interpolation. Valid interpolation methods for rotation are InterpolationMethod.ZeroOrder 
        /// and InterpolationMethod.Bilinear. The default is InterpolationMethod.Bilinear.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images.
        /// </remarks>

        public static void Rotate(VisionImage source, VisionImage destination, double angle, PixelValue fill, InterpolationMethod method)
        {
            Rotate(source, destination, angle, fill, method, true);
        }
        //==========================================================================================
        /// <summary>
        /// Rotates an image counterclockwise.
        /// </summary>
        /// <param name="source">
        /// The image to rotate.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="angle">
        /// The angle, in degrees, to rotate the image.
        /// </param>
        /// <param name="fill">
        /// The value with which the method fills the image pixels not covered by the rotated image.
        /// This parameter has a default value of 0.0.
        /// </param>
        /// <param name="method">
        /// The method of interpolation. Valid interpolation methods for rotation are InterpolationMethod.ZeroOrder 
        /// and InterpolationMethod.Bilinear. The default is InterpolationMethod.Bilinear.
        /// </param>
        /// <param name="maintainSize">
        /// Specifies whether the rotated image should have the same size as the source image.
        /// The default is true.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, and Rgb32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///     
        /// 'Rotate the image in Viewer1 by 45 degrees.
        /// 'Store the result in i.
        /// Algorithms.Rotate (imageViewer1.Image, i, 45, 128, InterpolationMethod.Bilinear, true)    
        /// imageViewer2.Attach (i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Rotate the image in Viewer1 by 45 degrees.
        /// //Store the result in i.
        /// Algorithms.Rotate(imageViewer1.Image, i, 45, 128, InterpolationMethod.Bilinear, true);
        /// imageViewer2.Attach(i);
        /// </code>
        /// </example>

        public static void Rotate(VisionImage source, VisionImage destination, double angle, PixelValue fill, InterpolationMethod method, bool maintainSize)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            source.ThrowIfWrongType(fill);
            CVI_PixValue cviFill = new CVI_PixValue();
            cviFill.ConvertFromExternal(fill);
            Utilities.ThrowError(VisionDll.imaqRotate2(destination._image, source._image, (float)angle, cviFill, method, maintainSize ? 1 : 0));
        }
        //==========================================================================================
        /// <summary>
        /// Shifts an image based on a horizontal and vertical offset.
        /// </summary>
        /// <param name="source">
        /// The image to shift.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="offset">
        /// Specifies how many pixels to shift the image horizontally and vertically.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void Shift(VisionImage source, VisionImage destination, PointContour offset)
        {
            // We have to check source for null here since we need the image type.
            if (source == null) { throw new ArgumentNullException("source"); }
            Shift(source, destination, offset, new PixelValue(source.Type));
        }
        //==========================================================================================
        /// <summary>
        /// Shifts an image based on a horizontal and vertical offset.
        /// </summary>
        /// <param name="source">
        /// The image to shift.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="offset">
        /// Specifies the ROI containing the pixels to shift horizontally and vertically. This parameter must contain exactly one contour, and it must be a PointContour.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void Shift(VisionImage source, VisionImage destination, Roi offset)
        {
            // We have to check source for null here since we need the image type.
            if (source == null) { throw new ArgumentNullException("source"); }
            if (offset == null) { throw new ArgumentNullException("offset"); }
            offset.ThrowIfDisposed();
            Utilities.ThrowIfNotSinglePoint(offset);
            Shift(source, destination, (PointContour)offset[0].Shape, new PixelValue(source.Type));
        }
        //==========================================================================================
        /// <summary>
        /// Shifts an image based on a horizontal and vertical offset.
        /// </summary>
        /// <param name="source">
        /// The image to shift.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="offset">
        /// Specifies the ROI containing the pixels to shift horizontally and vertically. This parameter must contain exactly one contour, and it must be a PointContour.
        /// </param>
        /// <param name="fill">
        /// The value with which to fill the uncovered image pixels. The default is 0.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void Shift(VisionImage source, VisionImage destination, Roi offset, PixelValue fill)
        {
            if (offset == null) { throw new ArgumentNullException("offset"); }
            offset.ThrowIfDisposed();
            Utilities.ThrowIfNotSinglePoint(offset);
            Shift(source, destination, (PointContour)offset[0].Shape, fill);
        }
        //==========================================================================================
        /// <summary>
        /// Shifts an image based on a horizontal and vertical offset.
        /// </summary>
        /// <param name="source">
        /// The image to shift.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="offset">
        /// Specifies how many pixels to shift the image horizontally and vertically.
        /// </param>
        /// <param name="fill">
        /// The value with which to fill the uncovered image pixels. The default is 0.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Shift the image in Viewer1 10 pixels to the right and 20 pixels down.
        /// 'Store the results in i.
        /// Algorithms.Shift (imageViewer1.Image, i, New PointContour(10, 20), New PixelValue(128))
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// //Shift the image in Viewer1 10 pixels to the right and 20 pixels down.
        /// //Store the results in i.
        /// Algorithms.Shift(imageViewer1.Image, i, new PointContour(10, 20), new PixelValue(128));
        /// </code>
        /// </example>

        public static void Shift(VisionImage source, VisionImage destination, PointContour offset, PixelValue fill)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (offset == null) { throw new ArgumentNullException("offset"); }
            source.ThrowIfWrongType(fill);
            CVI_PixValue cviFill = new CVI_PixValue();
            cviFill.ConvertFromExternal(fill);
            Utilities.ThrowError(VisionDll.imaqShift(destination._image, source._image, (Int32)Math.Round(offset.X), (Int32)Math.Round(offset.Y), cviFill));
        }
        //==========================================================================================
        /// <summary>
        /// Transposes an image.
        /// </summary>
        /// <param name="source">
        /// The image to transpose.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// Use this method with all image types.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Transpose the image in Viewer1 and
        /// ' store the result in i.
        /// Algorithms.Transpose (imageViewer1.Image, i)
        ///  
        /// ' Transpose the image in Viewer1.
        /// ' Do the operation in-place.
        /// Algorithms.Transpose (imageViewer1.Image, imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // Transpose the image in Viewer1 and
        /// // store the result in i.
        /// Algorithms.Transpose(imageViewer1.Image, i);
        ///  
        /// // Transpose the image in Viewer1.
        /// // Do the operation in-place.
        /// Algorithms.Transpose(imageViewer1.Image, imageViewer1.Image);
        /// </code>
        /// </example>

        public static void Transpose(VisionImage source, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqTranspose(destination._image, source._image));
        }
        //==========================================================================================
        /// <summary>
        /// Unwraps an annulus from an image into a rectangular strip.
        /// </summary>
        /// <param name="source">
        /// The image containing the annulus of pixels to be unwrapped.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="annulus">
        /// The coordinate location of the annulus the method unwraps.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static void Unwrap(VisionImage source, VisionImage destination, Roi annulus)
        {
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            annulus.ThrowIfDisposed();
            Utilities.ThrowIfNotSingleAnnulus(annulus);
            Unwrap(source, destination, (AnnulusContour)annulus[0].Shape, RectangleOrientation.BaseInside, InterpolationMethod.ZeroOrder);
        }
        //==========================================================================================
        /// <summary>
        /// Unwraps an annulus from an image into a rectangular strip.
        /// </summary>
        /// <param name="source">
        /// The image containing the annulus of pixels to be unwrapped.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="annulus">
        /// The coordinate location of the annulus the method unwraps.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static void Unwrap(VisionImage source, VisionImage destination, AnnulusContour annulus)
        {
            Unwrap(source, destination, annulus, RectangleOrientation.BaseInside, InterpolationMethod.ZeroOrder);
        }
        //==========================================================================================
        /// <summary>
        /// Unwraps an annulus from an image into a rectangular strip.
        /// </summary>
        /// <param name="source">
        /// The image containing the annulus of pixels to be unwrapped.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="annulus">
        /// The coordinate location of the annulus the method unwraps.
        /// </param>
        /// <param name="orientation">
        /// Specifies the orientation of the resulting rectangular image relative to the annulus. The default 
        /// is <see cref="NationalInstruments.Vision.Analysis.RectangleOrientation.BaseInside" crefType="Unqualified"/>.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static void Unwrap(VisionImage source, VisionImage destination, Roi annulus, RectangleOrientation orientation)
        {
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            annulus.ThrowIfDisposed();
            Utilities.ThrowIfNotSingleAnnulus(annulus);
            Unwrap(source, destination, (AnnulusContour)annulus[0].Shape, orientation, InterpolationMethod.ZeroOrder);
        }
        //==========================================================================================
        /// <summary>
        /// Unwraps an annulus from an image into a rectangular strip.
        /// </summary>
        /// <param name="source">
        /// The image containing the annulus of pixels to be unwrapped.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="annulus">
        /// The coordinate location of the annulus the method unwraps.
        /// </param>
        /// <param name="orientation">
        /// Specifies the orientation of the resulting rectangular image relative to the annulus. The default 
        /// is <see cref="NationalInstruments.Vision.Analysis.RectangleOrientation.BaseInside" crefType="Unqualified"/>.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static void Unwrap(VisionImage source, VisionImage destination, AnnulusContour annulus, RectangleOrientation orientation)
        {
            Unwrap(source, destination, annulus, orientation, InterpolationMethod.ZeroOrder);
        }
        //==========================================================================================
        /// <summary>
        /// Unwraps an annulus from an image into a rectangular strip.
        /// </summary>
        /// <param name="source">
        /// The image containing the annulus of pixels to be unwrapped.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="annulus">
        /// The coordinate location of the annulus the method unwraps.
        /// </param>
        /// <param name="orientation">
        /// Specifies the orientation of the resulting rectangular image relative to the annulus. The default 
        /// is <see cref="NationalInstruments.Vision.Analysis.RectangleOrientation.BaseInside" crefType="Unqualified"/>.
        /// </param>
        /// <param name="method">
        /// Specifies the interpolation algorithm used in the unwrapping process. The default is ZeroOrder.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// 'Unwrap the image in viewer1.
        /// 'This requires that an annulus be selected on the viewer.
        /// Algorithms.Unwrap (imageViewer1.Image, i, imageViewer1.Roi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // Unwrap the image in viewer1.
        /// // This requires that an annulus be selected on the viewer.
        /// Algorithms.Unwrap(imageViewer1.Image, i, imageViewer1.Roi);
        /// </code>
        /// </example>

        public static void Unwrap(VisionImage source, VisionImage destination, Roi annulus, RectangleOrientation orientation, InterpolationMethod method)
        {
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            annulus.ThrowIfDisposed();
            Utilities.ThrowIfNotSingleAnnulus(annulus);
            Unwrap(source, destination, (AnnulusContour)annulus[0].Shape, orientation, method);
        }
        //==========================================================================================
        /// <summary>
        /// Unwraps an annulus from an image into a rectangular strip.
        /// </summary>
        /// <param name="source">
        /// The image containing the annulus of pixels to be unwrapped.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="annulus">
        /// The coordinate location of the annulus the method unwraps.
        /// </param>
        /// <param name="orientation">
        /// Specifies the orientation of the resulting rectangular image relative to the annulus. The default 
        /// is <see cref="NationalInstruments.Vision.Analysis.RectangleOrientation.BaseInside" crefType="Unqualified"/>.
        /// </param>
        /// <param name="method">
        /// Specifies the interpolation algorithm used in the unwrapping process. The default is ZeroOrder.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static void Unwrap(VisionImage source, VisionImage destination, AnnulusContour annulus, RectangleOrientation orientation, InterpolationMethod method)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (annulus == null) { throw new ArgumentNullException("annulus"); }
            CVI_Annulus cviAnnulus = new CVI_Annulus();
            cviAnnulus.ConvertFromExternal(annulus);
            Utilities.ThrowError(VisionDll.imaqUnwrapImage(destination._image, source._image, cviAnnulus, orientation, method));
        }
        //==========================================================================================
        /// <summary>
        /// Displays an image using an isometric view. Each pixel from the image source appears as a column of pixels in the 
        /// 3D view. The pixel value corresponds to the altitude.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8 and Complex images.
        /// </remarks>

        public static void View3D(VisionImage source, VisionImage destination)
        {
            View3D(source, destination, new View3DOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Displays an image using an isometric view. Each pixel from the image source appears as a column of pixels in the 
        /// 3D view. The pixel value corresponds to the altitude.
        /// </summary>
        /// <param name="source">
        /// The input image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="options">
        /// Specifies how to convert the image to a three-dimensional representation.
        /// </param>
        /// <remarks>
        /// This method modifies the source image. If you need the original source image, create a copy of the image using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.Copy" crefType="Unqualified"/> method before using this method.
        /// Use this method with U8 and Complex images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Convert the image in viewer1 to a three-dimensional representation in i.
        /// Algorithms.View3D (imageViewer1.Image, i)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // Convert the image in viewer1 to a three-dimensional representation in i.
        /// Algorithms.View3D(imageViewer1.Image, i);
        /// </code>
        /// </example>

        public static void View3D(VisionImage source, VisionImage destination, View3DOptions options)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_View3DOptions cviOptions = new CVI_View3DOptions();
            cviOptions.ConvertFromExternal(options);
            Utilities.ThrowError(VisionDll.imaqView3D(destination._image, source._image, ref cviOptions));
        }
        #endregion

        #region Pattern Matching functions
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image for which you are going to look when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchPattern" crefType="Unqualified"/> method. 
        /// This description data is appended to the input template image. During the matching step, the template 
        /// descriptor is extracted from the template image and used to search for the template in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the function learns pattern matching information.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnPattern(VisionImage template)
        {
            LearnPattern(template, new LearnPatternOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image for which you are going to look when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchPattern" crefType="Unqualified"/> method. 
        /// This description data is appended to the input template image. During the matching step, the template 
        /// descriptor is extracted from the template image and used to search for the template in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the function learns pattern matching information.
        /// </param>
        /// <param name="options">
        /// Specifies the parameters the method uses for learning.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnPattern(VisionImage template, LearnPatternOptions options)
        {
            LearnPattern(template, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image for which you are going to look when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchPattern" crefType="Unqualified"/> method. 
        /// This description data is appended to the input template image. During the matching step, the template 
        /// descriptor is extracted from the template image and used to search for the template in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the function learns pattern matching information.
        /// </param>
        /// <param name="options">
        /// Specifies the parameters the method uses for learning.
        /// </param>
        /// <param name="mask">
        /// An 8-bit image that specifies what regions and edges to ignore in the 
        /// template. The method learns only those pixels in the source image whose corresponding pixels in the 
        /// mask are non-zero. Pass null or Nothing for this parameter to learn the whole image. 
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Learn the template in viewer1.
        /// Algorithms.LearnPattern (imageViewer1.Image)
        /// 'Match the template in viewer1.
        /// 'Look for 2 matches that could be rotated.
        /// Dim Options As New MatchPatternOptions (MatchMode.RotationInvariant, 2)
        /// Dim Matches As Collection (Of PatternMatch)
        /// Matches = Algorithms.MatchPattern (imageViewer2.Image, imageViewer1.Image, Options)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Learn the template in viewer1.
        /// Algorithms.LearnPattern(imageViewer1.Image);
        /// // Match the template in viewer1.
        /// // Look for 2 matches that could be rotated.
        /// MatchPatternOptions options = new MatchPatternOptions(MatchMode.RotationInvariant, 2);
        /// Collection&lt;PatternMatch&gt; matches;
        /// matches = Algorithms.MatchPattern(imageViewer2.Image, imageViewer1.Image, options);
        /// </code>
        /// </example>

        public static unsafe void LearnPattern(VisionImage template, LearnPatternOptions options, VisionImage mask)
        {
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            CVI_LearnPatternAdvancedShiftOptions shiftOptions = new CVI_LearnPatternAdvancedShiftOptions();
            shiftOptions.ConvertFromExternal(options.AdvancedOptions.Shift);
            CVI_LearnPatternAdvancedRotationOptions rotationOptions = new CVI_LearnPatternAdvancedRotationOptions();
            rotationOptions.ConvertFromExternal(options.AdvancedOptions.Rotation);
            CVI_LearnPatternAdvancedOptions advancedOptions = new CVI_LearnPatternAdvancedOptions(&shiftOptions, &rotationOptions);
            Utilities.ThrowError(VisionDll.imaqLearnPattern3(template._image, (int)options.Mode, &advancedOptions, VisionImage.GetIntPtr(mask)));
        }

        public static unsafe LearnTemplateReport LearnPattern2(VisionImage template)
        {
            return LearnPattern2(template, null);
        }

        public static unsafe LearnTemplateReport LearnPattern2(VisionImage template, VisionImage mask)
        {
            return LearnPattern2(template, mask, MatchingAlgorithm.MatchAllAlgorithms);
        }

        public static unsafe LearnTemplateReport LearnPattern2(VisionImage template, VisionImage mask, MatchingAlgorithm matchingAlgorithm)
        {
            return LearnPattern2(template, mask, matchingAlgorithm, new RotationAngleRange(0.0f, 0.0f));
        }

        public static unsafe LearnTemplateReport LearnPattern2(VisionImage template, VisionImage mask, MatchingAlgorithm matchingAlgorithm, RotationAngleRange rotationAngleRange)
        {
            return LearnPattern2(template, mask, matchingAlgorithm, rotationAngleRange, new Collection<PMLearnAdvancedSetupDataOption>());
        }

        public static unsafe LearnTemplateReport LearnPattern2(VisionImage template, VisionImage mask, MatchingAlgorithm matchingAlgorithm, RotationAngleRange rotationAngleRange, Collection<PMLearnAdvancedSetupDataOption> LearnAdvancedSetupDataOption)
        {
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (rotationAngleRange == null) { throw new ArgumentNullException("rotationAngleRange"); }
            if (LearnAdvancedSetupDataOption == null) { throw new ArgumentNullException("LearnAdvancedSetupDataOption"); }
            
            CVI_PMRotationAngleRange cviRotationAngleRange = new CVI_PMRotationAngleRange();
            cviRotationAngleRange.ConvertFromExternal(rotationAngleRange);
            CVI_LearnTemplateReport cviLearnReport = new CVI_LearnTemplateReport();
            IntPtr cviLearnAdvancedSetupDataOption = Utilities.ConvertCollectionToIntPtr<PMLearnAdvancedSetupDataOption, CVI_PMLearnAdvancedSetupDataOption>(LearnAdvancedSetupDataOption);
            Utilities.ThrowError(VisionDll.imaqLearnPattern4(template._image, VisionImage.GetIntPtr(mask), matchingAlgorithm, ref cviRotationAngleRange, cviLearnAdvancedSetupDataOption, LearnAdvancedSetupDataOption.Count, out cviLearnReport));
            return cviLearnReport.ConvertToExternal();
        }

        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given template image. Use the the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method prior to
        /// using this method to ensure that the template image has been configured for the pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The image to be located during the pattern matching process. The template image is the image supplied 
        /// to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Never)]
        public static Collection<PatternMatch> MatchPattern(VisionImage image, VisionImage template)
        {
            return MatchPattern(image, template, new MatchPatternOptions(), RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given template image. Use the the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method prior to
        /// using this method to ensure that the template image has been configured for the pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The image to be located during the pattern matching process. The template image is the image supplied 
        /// to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Never)]
        public static Collection<PatternMatch> MatchPattern(VisionImage image, VisionImage template, MatchPatternOptions options)
        {
            return MatchPattern(image, template, options, RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given template image. Use the the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method prior to
        /// using this method to ensure that the template image has been configured for the pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The image to be located during the pattern matching process. The template image is the image supplied 
        /// to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image. 
        /// </param>
        /// <param name="searchArea">
        /// Specifies the rectangular region within an image in which to search for the template pattern. 
        /// Pass RectangleContour.None for this parameter if you want to search the entire image for the template image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        [EditorBrowsable(EditorBrowsableState.Never)]
        public static Collection<PatternMatch> MatchPattern(VisionImage image, VisionImage template, MatchPatternOptions options, RectangleContour searchArea)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            if (searchArea == null) { throw new ArgumentNullException("searchArea"); }
            CVI_Rectangle rect = new CVI_Rectangle();
            rect.ConvertFromExternal(searchArea);
            // Make sure we dispose the CVI_MatchPatternOptions object
            CVI_MatchPatternOptions cviOptions = new CVI_MatchPatternOptions();
            cviOptions.ConvertFromExternal(options);
            try
            {
                CVI_MatchPatternAdvancedOptions cviAdvancedOptions = new CVI_MatchPatternAdvancedOptions();
                cviAdvancedOptions.ConvertFromExternal(options.Advanced);
                Int32 numMatches;
                IntPtr report = VisionDll.imaqMatchPattern2(image._image, template._image, ref cviOptions, ref cviAdvancedOptions, rect, out numMatches);
                Utilities.ThrowError(report);
                Collection<PatternMatch> toReturn = Utilities.ConvertIntPtrToCollection<PatternMatch, CVI_PatternMatch>(report, numMatches, true);
                return toReturn;
            }
            finally
            {
                cviOptions.Dispose();
            }
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given template image. Use the the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method prior to
        /// using this method to ensure that the template image has been configured for the pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The image to be located during the pattern matching process. The template image is the image supplied 
        /// to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image. 
        /// </param>
        /// <param name="searchArea">
        /// Specifies the rectangular region within an image in which to search for the template pattern. 
        /// Pass null or Nothing for this parameter if you want to search the entire image for the template image.</param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Learn the template in viewer1.
        /// Algorithms.LearnPattern (imageViewer1.Image)
        /// 'Match the template in viewer1.
        /// 'Look for 2 matches that could be rotated.
        /// Dim Options As New MatchPatternOptions (MatchMode.RotationInvariant, 2)
        /// Dim Matches As Collection (Of PatternMatch)
        /// Matches = Algorithms.MatchPattern (imageViewer2.Image, imageViewer1.Image, Options)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Learn the template in viewer1.
        /// Algorithms.LearnPattern(imageViewer1.Image);
        /// // Match the template in viewer1.
        /// // Look for 2 matches that could be rotated.
        /// MatchPatternOptions options = new MatchPatternOptions(MatchMode.RotationInvariant, 2);
        /// Collection&lt;PatternMatch&gt; matches;
        /// matches = Algorithms.MatchPattern(imageViewer2.Image, imageViewer1.Image, options);
        /// </code>
        /// </example>

        [EditorBrowsable(EditorBrowsableState.Never)]
        public static Collection<PatternMatch> MatchPattern(VisionImage image, VisionImage template, MatchPatternOptions options, Roi searchArea)
        {
            Roi.ThrowIfNonNullAndDisposed(searchArea);
            return MatchPattern(image, template, options, Utilities.ConvertRoiToRectangle(searchArea));
        }

        /// <summary>
        /// Searches for areas in an image that match a given template image. Use the the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method prior to
        /// using this method to ensure that the template image has been configured for the pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The image to be located during the pattern matching process. The template image is the image supplied 
        /// to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<PatternMatch> MatchPattern2(VisionImage image, VisionImage template)
        {
            return MatchPattern2(image, template, new MatchPatternOptions(), new Roi());
        }
        /// <summary>
        /// Searches for areas in an image that match a given template image. Use the the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method prior to
        /// using this method to ensure that the template image has been configured for the pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The image to be located during the pattern matching process. The template image is the image supplied 
        /// to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<PatternMatch> MatchPattern2(VisionImage image, VisionImage template, MatchPatternOptions options)
        {
            return MatchPattern2(image, template, options, new Roi());
        }
        /// <summary>
        /// Searches for areas in an image that match a given template image. Use the the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method prior to
        /// using this method to ensure that the template image has been configured for the pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The image to be located during the pattern matching process. The template image is the image supplied 
        /// to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image. 
        /// </param>
        /// <param name="searchArea">
        /// Specifies the rectangular or rotated rectangular region within an image in which to search for the template pattern. 
        /// Pass null or Nothing for this parameter if you want to search the entire image for the template image.</param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<PatternMatch> MatchPattern2(VisionImage image, VisionImage template, MatchPatternOptions options, Roi searchArea)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            if (searchArea == null) { throw new ArgumentNullException("searchArea"); }
            searchArea.ThrowIfDisposed();
            // Make sure we dispose the CVI_MatchPatternOptions object
            CVI_MatchPatternOptions cviOptions = new CVI_MatchPatternOptions();
            cviOptions.ConvertFromExternal(options);
            Collection<PatternMatch> toReturn;
            try
            {
                CVI_MatchPatternAdvancedOptions cviAdvancedOptions = new CVI_MatchPatternAdvancedOptions();
                cviAdvancedOptions.ConvertFromExternal(options.Advanced);
                Int32 numMatches;
                IntPtr report = VisionDll.imaqMatchPattern3(image._image, template._image, ref cviOptions, ref cviAdvancedOptions, Roi.GetIntPtr(searchArea), out numMatches);
                Utilities.ThrowError(report);
                toReturn = Utilities.ConvertIntPtrToCollection<PatternMatch, CVI_PatternMatch>(report, numMatches, true);
            }
            finally
            {
                cviOptions.Dispose();
            }
            return toReturn;
        }

        public static Collection<PatternMatchReport> MatchPattern3(VisionImage image, VisionImage template)
        {
            return MatchPattern3(image, template, MatchingAlgorithm.MatchGrayValuePyramid);
        }

        public static Collection<PatternMatchReport> MatchPattern3(VisionImage image, VisionImage template, MatchingAlgorithm matchingAlgorithm)
        {
            return MatchPattern3(image, template, matchingAlgorithm, 0);
        }

        public static Collection<PatternMatchReport> MatchPattern3(VisionImage image, VisionImage template, MatchingAlgorithm matchingAlgorithm, Int32 numRequestedMatches)
        {
            return MatchPattern3(image, template, matchingAlgorithm, numRequestedMatches, 600.0f);
        }

        public static Collection<PatternMatchReport> MatchPattern3(VisionImage image, VisionImage template, MatchingAlgorithm matchingAlgorithm, Int32 numRequestedMatches, float minScore)
        {
            return MatchPattern3(image, template, matchingAlgorithm, numRequestedMatches, minScore, new Collection<RotationAngleRange>());
        }

        public static Collection<PatternMatchReport> MatchPattern3(VisionImage image, VisionImage template, MatchingAlgorithm matchingAlgorithm, Int32 numRequestedMatches, float minScore, Collection<RotationAngleRange> rotationAngleRange)
        {
            return MatchPattern3(image, template, matchingAlgorithm, numRequestedMatches, minScore, rotationAngleRange, new Roi());
        }

        public static Collection<PatternMatchReport> MatchPattern3(VisionImage image, VisionImage template, MatchingAlgorithm matchingAlgorithm, Int32 numRequestedMatches, float minScore, Collection<RotationAngleRange> rotationAngleRange, Roi searchArea)
        {
            return MatchPattern3(image, template, matchingAlgorithm, numRequestedMatches, minScore, rotationAngleRange, searchArea, new Collection<PMMatchAdvancedSetupDataOption>());
        }

        public static Collection<PatternMatchReport> MatchPattern3(VisionImage image, VisionImage template, MatchingAlgorithm matchingAlgorithm, Int32 numRequestedMatches, float minScore, Collection<RotationAngleRange> rotationAngleRange, Roi searchArea, Collection<PMMatchAdvancedSetupDataOption> MatchAdvancedSetupDataOption)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (searchArea == null) { throw new ArgumentNullException("searchArea"); }
            searchArea.ThrowIfDisposed();
            if (rotationAngleRange == null) { throw new ArgumentNullException("rotationAngleRange"); }
            if (MatchAdvancedSetupDataOption == null) { throw new ArgumentNullException("MatchAdvancedSetupDataOption"); }
            
            IntPtr cviRotationAngleRange = Utilities.ConvertCollectionToIntPtr<RotationAngleRange, CVI_PMRotationAngleRange>(rotationAngleRange);
            IntPtr cviMatchAdvancedSetupDataOption = Utilities.ConvertCollectionToIntPtr<PMMatchAdvancedSetupDataOption, CVI_PMMatchAdvancedSetupDataOption>(MatchAdvancedSetupDataOption);
            Collection<PatternMatchReport> toReturn;
            Int32 numMatchesFound;
            IntPtr report = VisionDll.imaqMatchPattern4(image._image, template._image, matchingAlgorithm, numRequestedMatches, minScore, cviRotationAngleRange, rotationAngleRange.Count, Roi.GetIntPtr(searchArea), cviMatchAdvancedSetupDataOption, MatchAdvancedSetupDataOption.Count, out numMatchesFound);
            Utilities.ThrowError(report);
            toReturn = Utilities.ConvertIntPtrToCollection<PatternMatchReport, CVI_PatternMatchReport>(report, numMatchesFound, true);
            return toReturn;
        }
        
        public static PatternMatchTemplateInfo GetPatternMatchTemplateInfo(VisionImage template, VisionImage mask)
        {
            return GetPatternMatchTemplateInfo(template, mask, MatchingAlgorithm.MatchGrayValuePyramid);
        }

        public static PatternMatchTemplateInfo GetPatternMatchTemplateInfo(VisionImage template, VisionImage mask, MatchingAlgorithm matchingAlgorithm)
        {
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            CVI_PatternMatchTemplateInfo cviPatternMatchTemplateInfo = new CVI_PatternMatchTemplateInfo();
            Utilities.ThrowError(VisionDll.imaqGetTemplateInformation(template._image, VisionImage.GetIntPtr(mask), matchingAlgorithm, out cviPatternMatchTemplateInfo.pyramidInfo, out cviPatternMatchTemplateInfo.matchOffSetInfo));
            return cviPatternMatchTemplateInfo.ConvertToExternal();
        }
        //==========================================================================================
        /// <summary>
        /// Refines matches returned from the <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchPattern" crefType="Unqualified"/> 
        /// method using subpixel information learned using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="image">
        /// The image in which you originally located the matches you want to refine.
        /// </param>
        /// <param name="template">
        /// The template for which you want to search during the refinement phase.
        /// </param>
        /// <param name="matches">
        /// Information about each match found in the image. This information is returned by the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>

        public static Collection<PatternMatch> RefineMatches(VisionImage image, VisionImage template, Collection<PatternMatch> matches)
        {
            return RefineMatches(image, template, matches, new MatchPatternOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Refines matches returned from the <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchPattern" crefType="Unqualified"/> 
        /// method using subpixel information learned using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnPattern" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="image">
        /// The image in which you originally located the matches you want to refine.
        /// </param>
        /// <param name="template">
        /// The template for which you want to search during the refinement phase.
        /// </param>
        /// <param name="matches">
        /// Information about each match found in the image. This information is returned by the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <param name="options">
        /// The options that were passed to the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchPattern" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim template As VisionImage
        /// 'Assumes template is an existing learned pattern.
        /// 'First match the image on viewer1.  Look for 3 matches, possibly rotated.
        /// Dim matchOptions As New MatchPatternOptions(MatchMode.RotationInvariant, 3)
        /// Dim matches As Collection(Of PatternMatch) = Algorithms.MatchPattern (imageViewer1.Image, template, matchOptions)
        /// 'Now refine the matches.
        /// matches = Algorithms.RefineMatches (imageViewer1.Image, template, matches, matchOptions)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage template;
        /// // Assumes template is an existing learned pattern.
        /// // First match the image on viewer1.  Look for 3 matches, possibly rotated.
        /// MatchPatternOptions matchOptions = new MatchPatternOptions(MatchMode.RotationInvariant, 3);
        /// Collection&lt;PatternMatch&gt; matches = Algorithms.MatchPattern(imageViewer1.Image, template, matchOptions);
        /// // Now refine the matches.
        /// matches = Algorithms.RefineMatches(imageViewer1.Image, template, matches, matchOptions);
        /// </code>
        /// </example>

        public static Collection<PatternMatch> RefineMatches(VisionImage image, VisionImage template, Collection<PatternMatch> matches, MatchPatternOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (matches == null) { throw new ArgumentNullException("matches"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            // Make sure the CVI_MatchPatternOptions gets disposed properly.
            CVI_MatchPatternOptions cviOptions = new CVI_MatchPatternOptions();
            cviOptions.ConvertFromExternal(options);
            try
            {
                CVI_MatchPatternAdvancedOptions cviAdvancedOptions = new CVI_MatchPatternAdvancedOptions();
                cviAdvancedOptions.ConvertFromExternal(options.Advanced);
                CVI_PatternMatch[] cviMatches = Utilities.ConvertCollectionToArray<PatternMatch, CVI_PatternMatch>(matches);
                Int32 numMatches;
                IntPtr report = VisionDll.imaqRefineMatches(image._image, template._image, cviMatches, cviMatches.Length, ref cviOptions, ref cviAdvancedOptions, out numMatches);
                Utilities.ThrowError(report);
                Collection<PatternMatch> toReturn = Utilities.ConvertIntPtrToCollection<PatternMatch, CVI_PatternMatch>(report, numMatches, true);
                return toReturn;
            }
            finally
            {
                cviOptions.Dispose();
            }
        }

        //==========================================================================================
        /// <summary>
        /// Creates a description of the color template image to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColorPattern" crefType="Unqualified"/> method. This description data is appended to the input color template image. 
        /// During the color pattern match, the color template descriptor is extracted from the color 
        /// template image and used to search for the template in the color match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to the image. 
        /// </param>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>

        public static void LearnColorPattern(VisionImage template)
        {
            LearnColorPattern(template, new LearnColorPatternOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the color template image to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColorPattern" crefType="Unqualified"/> method. This description data is appended to the input color template image. 
        /// During the color pattern match, the color template descriptor is extracted from the color 
        /// template image and used to search for the template in the color match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to the image. 
        /// </param>
        /// <param name="options">
        /// Describes the information the algorithm learns about the color pattern. 
        /// </param>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Learn the template in viewer1, ignoring black and white colors.
        /// Dim ColorOptions As New LearnColorPatternOptions
        /// ColorOptions.IgnoreMode = ColorIgnoreMode.BlackAndWhite
        /// Algorithms.LearnColorPattern (imageViewer1.Image, ColorOptions)
        /// 'Match the template in viewer2.
        /// Dim Matches As Collection(Of PatternMatch) = Algorithms.MatchColorPattern (imageViewer2.Image, 
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Learn the template in viewer1, ignoring black and white colors.
        /// LearnColorPatternOptions colorOptions = new LearnColorPatternOptions();
        /// colorOptions.IgnoreMode = ColorIgnoreMode.BlackAndWhite;
        /// Algorithms.LearnColorPattern(imageViewer1.Image, colorOptions);
        /// // Match the template in viewer2.
        /// Collection&lt;PatternMatch&gt; matches = Algorithms.MatchColorPattern(imageViewer2.Image, imageViewer1.Image);
        /// </code>
        /// </example>

        public static void LearnColorPattern(VisionImage template, LearnColorPatternOptions options)
        {
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_LearnColorPatternOptions cviOptions = new CVI_LearnColorPatternOptions();
            cviOptions.ConvertFromExternal(options);
            try
            {
                Utilities.ThrowError(VisionDll.imaqLearnColorPattern(template._image, ref cviOptions));
            }
            finally
            {
                cviOptions.Dispose();
            }
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given color template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColorPattern" crefType="Unqualified"/> method prior 
        /// to using this method to ensure that the color template image has been configured for the color pattern 
        /// match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the function finds matches to the color template image. 
        /// </param>
        /// <param name="template">
        /// The color image to be located during the color pattern matching process. The template image is the color 
        /// image supplied to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColorPattern" crefType="Unqualified"/> 
        /// method.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> objects that
        /// contains information about each about each match found.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>

        public static Collection<PatternMatch> MatchColorPattern(VisionImage image, VisionImage template)
        {
            return MatchColorPattern(image, template, new MatchColorPatternOptions(), RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given color template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColorPattern" crefType="Unqualified"/> method prior 
        /// to using this method to ensure that the color template image has been configured for the color pattern 
        /// match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the function finds matches to the color template image. 
        /// </param>
        /// <param name="template">
        /// The color image to be located during the color pattern matching process. The template image is the color 
        /// image supplied to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColorPattern" crefType="Unqualified"/> 
        /// method.
        /// </param>
        /// <param name="options">
        /// Describes how to search for the color template image. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> objects that
        /// contains information about each about each match found.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>

        public static Collection<PatternMatch> MatchColorPattern(VisionImage image, VisionImage template, MatchColorPatternOptions options)
        {
            return MatchColorPattern(image, template, options, RectangleContour.None);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given color template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColorPattern" crefType="Unqualified"/> method prior 
        /// to using this method to ensure that the color template image has been configured for the color pattern 
        /// match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the function finds matches to the color template image. 
        /// </param>
        /// <param name="template">
        /// The color image to be located during the color pattern matching process. The template image is the color 
        /// image supplied to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColorPattern" crefType="Unqualified"/> 
        /// method.
        /// </param>
        /// <param name="options">
        /// Describes how to search for the color template image. 
        /// </param>
        /// <param name="searchArea">
        /// Specifies the rectangular region within an image in which to search for the color template pattern. 
        /// Pass null or Nothing for this parameter if you want the entire image to be examined.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> objects that
        /// contains information about each about each match found.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>

        public static Collection<PatternMatch> MatchColorPattern(VisionImage image, VisionImage template, MatchColorPatternOptions options, RectangleContour searchArea)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            if (searchArea == null) { throw new ArgumentNullException("searchArea"); }
            CVI_Rectangle rect = new CVI_Rectangle();
            rect.ConvertFromExternal(searchArea);
            // Make sure we dispose the CVI_MatchColorPatternOptions object
            CVI_MatchColorPatternOptions cviOptions = new CVI_MatchColorPatternOptions();
            cviOptions.ConvertFromExternal(options);
            try
            {
                Int32 numMatches;
                IntPtr report = VisionDll.imaqMatchColorPattern(image._image, template._image, ref cviOptions, rect, out numMatches);
                Utilities.ThrowError(report);
                Collection<PatternMatch> toReturn = Utilities.ConvertIntPtrToCollection<PatternMatch, CVI_PatternMatch>(report, numMatches, true);
                return toReturn;
            }
            finally
            {
                cviOptions.Dispose();
            }
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given color template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColorPattern" crefType="Unqualified"/> method prior 
        /// to using this method to ensure that the color template image has been configured for the color pattern 
        /// match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the function finds matches to the color template image. 
        /// </param>
        /// <param name="template">
        /// The color image to be located during the color pattern matching process. The template image is the color 
        /// image supplied to the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColorPattern" crefType="Unqualified"/> 
        /// method.
        /// </param>
        /// <param name="options">
        /// Describes how to search for the color template image. 
        /// </param>
        /// <param name="searchArea">
        /// Specifies the ROI within an image in which to search for the color template pattern. 
        /// Pass null or Nothing for this parameter if you want the entire image to be examined.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.PatternMatch" crefType="Unqualified"/> objects that
        /// contains information about each about each match found.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Learn the template in viewer1, ignoring black and white colors.
        /// Dim ColorOptions As New LearnColorPatternOptions
        /// ColorOptions.IgnoreMode = ColorIgnoreMode.BlackAndWhite
        /// Algorithms.LearnColorPattern (imageViewer1.Image, ColorOptions)
        /// 'Match the template in viewer2.
        /// Dim Matches As Collection(Of PatternMatch) = Algorithms.MatchColorPattern (imageViewer2.Image, imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Learn the template in viewer1, ignoring black and white colors.
        /// LearnColorPatternOptions colorOptions = new LearnColorPatternOptions();
        /// colorOptions.IgnoreMode = ColorIgnoreMode.BlackAndWhite;
        /// Algorithms.LearnColorPattern(imageViewer1.Image, colorOptions);
        /// // Match the template in viewer2.
        /// Collection&lt;PatternMatch&gt; matches = Algorithms.MatchColorPattern(imageViewer2.Image, imageViewer1.Image);
        ///  
        /// </code>
        /// </example>

        public static Collection<PatternMatch> MatchColorPattern(VisionImage image, VisionImage template, MatchColorPatternOptions options, Roi searchArea)
        {
            Roi.ThrowIfNonNullAndDisposed(searchArea);
            return MatchColorPattern(image, template, options, Utilities.ConvertRoiToRectangle(searchArea));
        }
        //==========================================================================================
        /// <summary>
        /// Detects circles in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the circles are to be located.
        /// </param>
        /// <param name="circleDescriptor">
        /// Specifies the circles to look for in the search image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CircleMatch" crefType="Unqualified"/> objects about each circle found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<CircleMatch> DetectCircles(VisionImage image, CircleDescriptor circleDescriptor)
        {
            return DetectCircles(image, circleDescriptor, null, new CurveOptions(), new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects circles in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the circles are to be located.
        /// </param>
        /// <param name="circleDescriptor">
        /// Specifies the circles to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CircleMatch" crefType="Unqualified"/> objects about each circle found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<CircleMatch> DetectCircles(VisionImage image, CircleDescriptor circleDescriptor, Roi roi)
        {
            return DetectCircles(image, circleDescriptor, roi, new CurveOptions(), new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects circles in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the circles are to be located.
        /// </param>
        /// <param name="circleDescriptor">
        /// Specifies the circles to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <param name="curveOptions">
        /// Options that specify how the algorithm finds curves in the image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CircleMatch" crefType="Unqualified"/> objects about each circle found. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<CircleMatch> DetectCircles(VisionImage image, CircleDescriptor circleDescriptor, Roi roi, CurveOptions curveOptions)
        {
            return DetectCircles(image, circleDescriptor, roi, curveOptions, new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects circles in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the circles are to be located.
        /// </param>
        /// <param name="circleDescriptor">
        /// Specifies the circles to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <param name="curveOptions">
        /// Options that specify how the algorithm finds curves in the image.
        /// </param>
        /// <param name="shapeDetectionOptions">
        /// Options the method uses for detecting shapes.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CircleMatch" crefType="Unqualified"/> objects about each circle found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim i As New VisionImage
        /// Dim circles As Collection(Of CircleMatch)
        ///  
        /// ' Find circles from radius size 10 to 100 in the image
        /// circles = Algorithms.DetectCircles (i, New CircleDescriptor(10, 100))
        ///  
        /// ' Overlay the found circles on the image
        /// For Each circle As CircleMatch In circles
        ///     imageViewer1.Image.Ovelays.Default.AddOval (circle.Circle)
        /// Next
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage i = new VisionImage();
        /// Collection&lt;CircleMatch&gt; circles;
        ///  
        /// // Find circles from radius size 10 to 100 in the image
        /// circles = Algorithms.DetectCircles (i, new CircleDescriptor(10, 100))
        ///  
        /// // Overlay the found circles on the image
        /// foreach (CircleMatch circle in circles) {
        ///     imageViewer1.Image.Overlays.Default.AddOval (circle.Circle);
        /// }
        /// </code>
        /// </example>

        public static Collection<CircleMatch> DetectCircles(VisionImage image, CircleDescriptor circleDescriptor, Roi roi, CurveOptions curveOptions, ShapeDetectionOptions shapeDetectionOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (circleDescriptor == null) { throw new ArgumentNullException("circleDescriptor"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (curveOptions == null) { throw new ArgumentNullException("curveOptions"); }
            if (shapeDetectionOptions == null) { throw new ArgumentNullException("shapeDetectionOptions"); }
            CVI_CircleDescriptor cviCircleDescriptor = new CVI_CircleDescriptor();
            cviCircleDescriptor.ConvertFromExternal(circleDescriptor);
            CVI_CurveOptions cviCurveOptions = new CVI_CurveOptions();
            cviCurveOptions.ConvertFromExternal(curveOptions);
            CVI_ShapeDetectionOptions cviShapeDetectionOptions = new CVI_ShapeDetectionOptions();
            cviShapeDetectionOptions.ConvertFromExternal(shapeDetectionOptions);
            try
            {
                int numMatches;

                //IntPtr report = (In)null;
                IntPtr report = (IntPtr)null;
                //IntPtr report = VisionDll.imaqDetectCircles(image._image, ref cviCircleDescriptor, ref cviCurveOptions, ref cviShapeDetectionOptions, Roi.GetIntPtr(roi), out numMatches);
                // It turns out if we find no matches, this report will be NULL even if we had no error.
                // So, only throw if there was an error (Utilities.ThrowError() checks imaqGetLastError()).
                Utilities.ThrowError();
                return Utilities.ConvertIntPtrToCollection<CircleMatch, CVI_CircleMatch>(report, 0, true);
            }
            catch(Exception e)
            {
                return (Collection<CircleMatch>)null;
            }
            finally
            {
                cviShapeDetectionOptions.Dispose();
            }
        }
        //==========================================================================================
        /// <summary>
        /// Detects ellipses in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the ellipses are located.
        /// </param>
        /// <param name="ellipseDescriptor">
        /// Specifies the ellipses to look for in the search image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.EllipseMatch" crefType="Unqualified"/> objects about each ellipse found. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<EllipseMatch> DetectEllipses(VisionImage image, EllipseDescriptor ellipseDescriptor)
        {
            return DetectEllipses(image, ellipseDescriptor, null, new CurveOptions(), new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects ellipses in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the ellipses are located.
        /// </param>
        /// <param name="ellipseDescriptor">
        /// Specifies the ellipses to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.EllipseMatch" crefType="Unqualified"/> objects about each ellipse found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<EllipseMatch> DetectEllipses(VisionImage image, EllipseDescriptor ellipseDescriptor, Roi roi)
        {
            return DetectEllipses(image, ellipseDescriptor, roi, new CurveOptions(), new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects ellipses in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the ellipses are located.
        /// </param>
        /// <param name="ellipseDescriptor">
        /// Specifies the ellipses to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <param name="curveOptions">Options that specify how the algorithm finds curves in the image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.EllipseMatch" crefType="Unqualified"/> objects about each ellipse found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<EllipseMatch> DetectEllipses(VisionImage image, EllipseDescriptor ellipseDescriptor, Roi roi, CurveOptions curveOptions)
        {
            return DetectEllipses(image, ellipseDescriptor, roi, curveOptions, new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects ellipses in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the ellipses are located.
        /// </param>
        /// <param name="ellipseDescriptor">
        /// Specifies the ellipses to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <param name="curveOptions">Options that specify how the algorithm finds curves in the image.
        /// </param>
        /// <param name="shapeDetectionOptions">Options the method uses for detecting shapes.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.EllipseMatch" crefType="Unqualified"/> objects about each ellipse found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim i As New VisionImage
        /// Dim ellipses As Collection(Of EllipseMatch)
        ///  
        /// ' Find ellipses from minor radius size 10 to 30, major radius size
        /// ' 50 to 100 in the image
        /// ellipses = Algorithms.DetectEllipses (i, New EllipseDescriptor(New Range(50, 100), New Range(10, 30)))
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage i = new VisionImage();
        /// Collection&lt;EllipseMatch&gt; ellipses;
        ///  
        /// // Find ellipses from minor radius size 10 to 30, major radius size
        /// // 50 to 100 in the image
        /// ellipses = Algorithms.DetectEllipses (i, new EllipseDescriptor(new Range(50, 100), new Range(10, 30)));
        /// </code>
        /// </example>

        public static Collection<EllipseMatch> DetectEllipses(VisionImage image, EllipseDescriptor ellipseDescriptor, Roi roi, CurveOptions curveOptions, ShapeDetectionOptions shapeDetectionOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (ellipseDescriptor == null) { throw new ArgumentNullException("ellipseDescriptor"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (curveOptions == null) { throw new ArgumentNullException("curveOptions"); }
            if (shapeDetectionOptions == null) { throw new ArgumentNullException("shapeDetectionOptions"); }
            CVI_EllipseDescriptor cviEllipseDescriptor = new CVI_EllipseDescriptor();
            cviEllipseDescriptor.ConvertFromExternal(ellipseDescriptor);
            CVI_CurveOptions cviCurveOptions = new CVI_CurveOptions();
            cviCurveOptions.ConvertFromExternal(curveOptions);
            CVI_ShapeDetectionOptions cviShapeDetectionOptions = new CVI_ShapeDetectionOptions();
            cviShapeDetectionOptions.ConvertFromExternal(shapeDetectionOptions);
            try
            {
                int numMatches;
                IntPtr report = VisionDll.imaqDetectEllipses(image._image, ref cviEllipseDescriptor, ref cviCurveOptions, ref cviShapeDetectionOptions, Roi.GetIntPtr(roi), out numMatches);
                // It turns out if we find no matches, this report will be NULL even if we had no error.
                // So, only throw if there was an error (Utilities.ThrowError() checks imaqGetLastError()).
                Utilities.ThrowError();
                return Utilities.ConvertIntPtrToCollection<EllipseMatch, CVI_EllipseMatch>(report, numMatches, true);
            }
            finally
            {
                cviShapeDetectionOptions.Dispose();
            }
        }
        //==========================================================================================
        /// <summary>
        /// Detects lines in a search image.
        /// </summary>
        /// <param name="image">The image in which the lines are located.
        /// </param>
        /// <param name="lineDescriptor">A descriptor specifying the lines to look for in the search image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.LineMatch" crefType="Unqualified"/> objects about each line found. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<LineMatch> DetectLines(VisionImage image, LineDescriptor lineDescriptor)
        {
            return DetectLines(image, lineDescriptor, null, new CurveOptions(), new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects lines in a search image.
        /// </summary>
        /// <param name="image">The image in which the lines are located.
        /// </param>
        /// <param name="lineDescriptor">A descriptor specifying the lines to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.LineMatch" crefType="Unqualified"/> objects about each line found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<LineMatch> DetectLines(VisionImage image, LineDescriptor lineDescriptor, Roi roi)
        {
            return DetectLines(image, lineDescriptor, roi, new CurveOptions(), new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects lines in a search image.
        /// </summary>
        /// <param name="image">The image in which the lines are located.
        /// </param>
        /// <param name="lineDescriptor">A descriptor specifying the lines to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <param name="curveOptions">Options that specify how the algorithm finds curves in the image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.LineMatch" crefType="Unqualified"/> objects about each line found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<LineMatch> DetectLines(VisionImage image, LineDescriptor lineDescriptor, Roi roi, CurveOptions curveOptions)
        {
            return DetectLines(image, lineDescriptor, roi, curveOptions, new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects lines in a search image.
        /// </summary>
        /// <param name="image">The image in which the lines are located.
        /// </param>
        /// <param name="lineDescriptor">A descriptor specifying the lines to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <param name="curveOptions">Options that specify how the algorithm finds curves in the image.
        /// </param>
        /// <param name="shapeDetectionOptions">Options the method uses for detecting shapes.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.LineMatch" crefType="Unqualified"/> objects about each line found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim i As New VisionImage
        /// Dim lines As Collection(Of LineMatch)
        ///  
        /// ' Find lines from length 20 to 50 in the image
        /// lines = Algorithms.DetectLines (i, New LineDescriptor(20, 50))
        ///  
        /// ' Overlay the found lines on the image
        /// For Each line As LineMatch In lines
        ///     imageViewer1.Image.Overlays.Default.AddLine (line.Line)
        /// Next
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage i = new VisionImage();
        /// Collection&lt;LineMatch&gt; lines;
        ///  
        /// // Find lines from length 20 to 50 in the image
        /// lines = Algorithms.DetectLines (i, new LineDescriptor(20, 50))
        ///  
        /// // Overlay the found lines on the image
        /// foreach (LineMatch line in lines) {
        ///     imageViewer1.Image.Overlays.Default.AddLine (line.Line);
        /// }
        /// </code>
        /// </example>

        public static Collection<LineMatch> DetectLines(VisionImage image, LineDescriptor lineDescriptor, Roi roi, CurveOptions curveOptions, ShapeDetectionOptions shapeDetectionOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (lineDescriptor == null) { throw new ArgumentNullException("lineDescriptor"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (curveOptions == null) { throw new ArgumentNullException("curveOptions"); }
            if (shapeDetectionOptions == null) { throw new ArgumentNullException("shapeDetectionOptions"); }
            CVI_LineDescriptor cviLineDescriptor = new CVI_LineDescriptor();
            cviLineDescriptor.ConvertFromExternal(lineDescriptor);
            CVI_CurveOptions cviCurveOptions = new CVI_CurveOptions();
            cviCurveOptions.ConvertFromExternal(curveOptions);
            CVI_ShapeDetectionOptions cviShapeDetectionOptions = new CVI_ShapeDetectionOptions();
            cviShapeDetectionOptions.ConvertFromExternal(shapeDetectionOptions);
            try
            {
                int numMatches;
                IntPtr report = VisionDll.imaqDetectLines(image._image, ref cviLineDescriptor, ref cviCurveOptions, ref cviShapeDetectionOptions, Roi.GetIntPtr(roi), out numMatches);
                // It turns out if we find no matches, this report will be NULL even if we had no error.
                // So, only throw if there was an error (Utilities.ThrowError() checks imaqGetLastError()).
                Utilities.ThrowError();
                return Utilities.ConvertIntPtrToCollection<LineMatch, CVI_LineMatch>(report, numMatches, true);
            }
            finally
            {
                cviShapeDetectionOptions.Dispose();
            }
        }
        //==========================================================================================
        /// <summary>
        /// Detects rectangles in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the rectangles are located.
        /// </param>
        /// <param name="rectangleDescriptor">
        /// Specifies the rectangles to look for in the search image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.RectangleMatch" crefType="Unqualified"/> objects about each rectangle found. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<RectangleMatch> DetectRectangles(VisionImage image, RectangleDescriptor rectangleDescriptor)
        {
            return DetectRectangles(image, rectangleDescriptor, null, new CurveOptions(), new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects rectangles in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the rectangles are located.
        /// </param>
        /// <param name="rectangleDescriptor">
        /// Specifies the rectangles to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.RectangleMatch" crefType="Unqualified"/> objects about each rectangle found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<RectangleMatch> DetectRectangles(VisionImage image, RectangleDescriptor rectangleDescriptor, Roi roi)
        {
            return DetectRectangles(image, rectangleDescriptor, roi, new CurveOptions(), new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects rectangles in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the rectangles are located.
        /// </param>
        /// <param name="rectangleDescriptor">
        /// Specifies the rectangles to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <param name="curveOptions">Options that specify how the algorithm finds curves in the image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.RectangleMatch" crefType="Unqualified"/> objects about each rectangle found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<RectangleMatch> DetectRectangles(VisionImage image, RectangleDescriptor rectangleDescriptor, Roi roi, CurveOptions curveOptions)
        {
            return DetectRectangles(image, rectangleDescriptor, roi, curveOptions, new ShapeDetectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Detects rectangles in a search image.
        /// </summary>
        /// <param name="image">
        /// The image in which the rectangles are located.
        /// </param>
        /// <param name="rectangleDescriptor">
        /// Specifies the rectangles to look for in the search image.
        /// </param>
        /// <param name="roi">Specifies the portion of the image in which the method searches. Pass null or Nothing for this parameter to search the entire image.
        /// </param>
        /// <param name="curveOptions">Options that specify how the algorithm finds curves in the image.
        /// </param>
        /// <param name="shapeDetectionOptions">Options the method uses for detecting shapes.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.RectangleMatch" crefType="Unqualified"/> objects about each rectangle found. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim i As New VisionImage
        /// Dim rectangles As Collection(Of RectangleMatch)
        ///  
        /// ' Find rectangles from width and height size 30 to 60 in the image
        /// rectangles = Algorithms.DetectRectangles (i, New RectangleDescriptor (New Range (30, 60), New Range (30, 60)))
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage i = new VisionImage();
        /// Collection&lt;RectangleMatch&gt; rectangles;
        ///  
        /// // Find rectangles from width and height size 30 to 60 in the image
        /// rectangles = Algorithms.DetectRectangles (i, new RectangleDescriptor (new Range (30, 60), new Range (30, 60)));
        /// </code>
        /// </example>

        public static Collection<RectangleMatch> DetectRectangles(VisionImage image, RectangleDescriptor rectangleDescriptor, Roi roi, CurveOptions curveOptions, ShapeDetectionOptions shapeDetectionOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (rectangleDescriptor == null) { throw new ArgumentNullException("rectangleDescriptor"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (curveOptions == null) { throw new ArgumentNullException("curveOptions"); }
            if (shapeDetectionOptions == null) { throw new ArgumentNullException("shapeDetectionOptions"); }
            CVI_RectangleDescriptor cviRectangleDescriptor = new CVI_RectangleDescriptor();
            cviRectangleDescriptor.ConvertFromExternal(rectangleDescriptor);
            CVI_CurveOptions cviCurveOptions = new CVI_CurveOptions();
            cviCurveOptions.ConvertFromExternal(curveOptions);
            CVI_ShapeDetectionOptions cviShapeDetectionOptions = new CVI_ShapeDetectionOptions();
            cviShapeDetectionOptions.ConvertFromExternal(shapeDetectionOptions);
            try
            {
                int numMatches;
                IntPtr report = VisionDll.imaqDetectRectangles(image._image, ref cviRectangleDescriptor, ref cviCurveOptions, ref cviShapeDetectionOptions, Roi.GetIntPtr(roi), out numMatches);
                // It turns out if we find no matches, this report will be NULL even if we had no error.
                // So, only throw if there was an error (Utilities.ThrowError() checks imaqGetLastError()).
                Utilities.ThrowError();
                return Utilities.ConvertIntPtrToCollection<RectangleMatch, CVI_RectangleMatch>(report, numMatches, true);
            }
            finally
            {
                cviShapeDetectionOptions.Dispose();
            }
        }
        #endregion

        #region Calibration functions
        //==========================================================================================
        /// <summary>
        /// Transforms pixel coordinates to real world coordinates, according to the calibration information contained in the image.
        /// </summary>
        /// <param name="calibratedImage">The calibrated image.
        /// </param>
        /// <param name="pixelPoints">The collection of points to transform.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.CoordinatesReport" crefType="Unqualified"/> containing the points in real-world coordinates. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Attach calibration information to this image using <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.SetSimpleCalibration" crefType="Unqualified"/>, or <see cref="NationalInstruments.Vision.Analysis.Algorithms.CopyCalibrationInformation" crefType="Unqualified"/>. 
        /// </remarks>

        public static CoordinatesReport ConvertPixelToRealWorldCoordinates(VisionImage calibratedImage, Roi pixelPoints)
        {
            if (pixelPoints == null) { throw new ArgumentNullException("pixelPoints"); }
            pixelPoints.ThrowIfDisposed();
            return ConvertPixelToRealWorldCoordinates(calibratedImage, Utilities.ConvertRoiToPoints(pixelPoints));
        }
        //==========================================================================================
        /// <summary>
        /// Transforms pixel coordinates to real world coordinates, according to the calibration information contained in the image.
        /// </summary>
        /// <param name="calibratedImage">The calibrated image.
        /// </param>
        /// <param name="pixelPoint">The point to transform.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.CoordinatesReport" crefType="Unqualified"/> containing the points in real-world coordinates. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Attach calibration information to this image using <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.SetSimpleCalibration" crefType="Unqualified"/>, or <see cref="NationalInstruments.Vision.Analysis.Algorithms.CopyCalibrationInformation" crefType="Unqualified"/>. 
        /// </remarks>

        public static CoordinatesReport ConvertPixelToRealWorldCoordinates(VisionImage calibratedImage, PointContour pixelPoint)
        {
            if (pixelPoint == null) { throw new ArgumentNullException("pixelPoint"); }
            return ConvertPixelToRealWorldCoordinates(calibratedImage, new Collection<PointContour>(new PointContour[] { pixelPoint }));
        }
        //==========================================================================================
        /// <summary>
        /// Transforms pixel coordinates to real world coordinates, according to the calibration information contained in the image.
        /// </summary>
        /// <param name="calibratedImage">The calibrated image.
        /// </param>
        /// <param name="pixelPoints">The collection of points to transform.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.CoordinatesReport" crefType="Unqualified"/> containing the points in real-world coordinates. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Attach calibration information to this image using <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.SetSimpleCalibration" crefType="Unqualified"/>, or <see cref="NationalInstruments.Vision.Analysis.Algorithms.CopyCalibrationInformation" crefType="Unqualified"/>. 
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// ' This example assumes you have a Polyline selected on the viewer.
        /// Algorithms.SetSimpleCalibration(ImageViewer1.Image, New CoordinateSystem(), New GridDescriptor(10, 10, CalibrationUnit.Meter))
        /// Dim Polyline As PolylineContour
        /// Polyline = CType(ImageViewer1.Roi(0).Shape, PolylineContour)
        /// Dim report As CoordinatesReport = Algorithms.ConvertPixelToRealWorldCoordinates(ImageViewer1.Image, Polyline.Points)
        /// Dim i As Integer
        /// ' Overlay the results on the image
        /// For i = 0 To report.Points.Count - 1
        ///     If report.ValidPoints(i) Then
        ///         Dim OriginalPoint As PointContour = Polyline.Points(i)
        ///         Dim RealWorldPoint As PointContour = report.Points(i)
        ///         ImageViewer1.Image.Overlays.[Default].AddOval(New OvalContour(OriginalPoint.X - 3, OriginalPoint.Y - 3, 7, 7), Rgb32Value.RedColor, DrawingMode.PaintValue)
        ///         ImageViewer1.Image.Overlays.[Default].AddText(RealWorldPoint.ToString(), OriginalPoint, Rgb32Value.WhiteColor)
        ///     End If
        /// Next
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // This example assumes you have a Polyline selected on the viewer.
        /// Algorithms.SetSimpleCalibration(imageViewer1.Image, new CoordinateSystem(), new GridDescriptor(10, 10, CalibrationUnit.Meter));
        /// PolylineContour polyline;
        /// polyline = imageViewer1.Roi[0].Shape as PolylineContour;
        /// CoordinatesReport report = Algorithms.ConvertPixelToRealWorldCoordinates(imageViewer1.Image, polyline.Points);
        /// // Overlay the results on the image
        /// for (int i = 0; i &lt; report.Points.Count; ++i) {
        ///     if (report.ValidPoints[i]) {
        ///         PointContour originalPoint = polyline.Points[i];
        ///         PointContour realWorldPoint = report.Points[i];
        ///         imageViewer1.Image.Overlays.Default.AddOval(new OvalContour(originalPoint.X - 3, originalPoint.Y - 3, 7, 7), Rgb32Value.RedColor, DrawingMode.PaintValue);
        ///         imageViewer1.Image.Overlays.Default.AddText(realWorldPoint.ToString(), originalPoint, Rgb32Value.WhiteColor);
        ///     }
        /// }
        /// </code>
        /// </example>

        public static CoordinatesReport ConvertPixelToRealWorldCoordinates(VisionImage calibratedImage, Collection<PointContour> pixelPoints)
        {
            if (calibratedImage == null) { throw new ArgumentNullException("calibratedImage"); }
            calibratedImage.ThrowIfDisposed();
            if (pixelPoints == null) { throw new ArgumentNullException("pixelPoints"); }
            CVI_PointFloat[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointFloat>(pixelPoints);
            IntPtr result = VisionDll.imaqTransformPixelToRealWorld(calibratedImage._image, cviPoints, cviPoints.Length);
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToStructure<CoordinatesReport, CVI_TransformReport>(result, true);
        }

        //==========================================================================================
        /// <summary>
        /// Transforms real world coordinates to pixel coordinates, according to the calibration information.
        /// </summary>
        /// <param name="calibratedImage">
        /// The calibrated image.
        /// </param>
        /// <param name="realWorldPoints">
        /// The collection of real-world points to transform.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.CoordinatesReport" crefType="Unqualified"/> containing the points in pixel coordinates. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Attach calibration information to this image using <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.SetSimpleCalibration" crefType="Unqualified"/>, or <see cref="NationalInstruments.Vision.Analysis.Algorithms.CopyCalibrationInformation" crefType="Unqualified"/>. 
        /// </remarks>

        public static CoordinatesReport ConvertRealWorldToPixelCoordinates(VisionImage calibratedImage, Roi realWorldPoints)
        {
            if (realWorldPoints == null) { throw new ArgumentNullException("realWorldPoints"); }
            realWorldPoints.ThrowIfDisposed();
            return ConvertRealWorldToPixelCoordinates(calibratedImage, Utilities.ConvertRoiToPoints(realWorldPoints));
        }
        //==========================================================================================
        /// <summary>
        /// Transforms real world coordinates to pixel coordinates, according to the calibration information.
        /// </summary>
        /// <param name="calibratedImage">
        /// The calibrated image.
        /// </param>
        /// <param name="realWorldPoint">
        /// The real-world point to transform.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.CoordinatesReport" crefType="Unqualified"/> containing the points in pixel coordinates. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Attach calibration information to this image using <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.SetSimpleCalibration" crefType="Unqualified"/>, or <see cref="NationalInstruments.Vision.Analysis.Algorithms.CopyCalibrationInformation" crefType="Unqualified"/>. 
        /// </remarks>

        public static CoordinatesReport ConvertRealWorldToPixelCoordinates(VisionImage calibratedImage, PointContour realWorldPoint)
        {
            if (realWorldPoint == null) { throw new ArgumentNullException("realWorldPoint"); }
            return ConvertRealWorldToPixelCoordinates(calibratedImage, new Collection<PointContour>(new PointContour[] { realWorldPoint }));
        }
        //==========================================================================================
        /// <summary>
        /// Transforms real world coordinates to pixel coordinates, according to the calibration information.
        /// </summary>
        /// <param name="calibratedImage">
        /// The calibrated image.
        /// </param>
        /// <param name="realWorldPoints">
        /// The collection of real-world points to transform.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.CoordinatesReport" crefType="Unqualified"/> containing the points in pixel coordinates. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Attach calibration information to this image using <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.SetSimpleCalibration" crefType="Unqualified"/>, or <see cref="NationalInstruments.Vision.Analysis.Algorithms.CopyCalibrationInformation" crefType="Unqualified"/>. 
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// ' Assumes the image in the viewer is calibrated.
        /// Dim point As PointContour
        /// point = Algorithms.ConvertRealWorldToPixelCoordinates(imageViewer1.Image, New Point(100, 200))
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Assumes the image in the viewer is calibrated.
        /// PointContour point;
        /// point = Algorithms.ConvertRealWorldToPixelCoordinates(imageViewer1.Image, new Point(100, 200));
        /// </code>
        /// </example>

        public static CoordinatesReport ConvertRealWorldToPixelCoordinates(VisionImage calibratedImage, Collection<PointContour> realWorldPoints)
        {
            if (calibratedImage == null) { throw new ArgumentNullException("calibratedImage"); }
            calibratedImage.ThrowIfDisposed();
            if (realWorldPoints == null) { throw new ArgumentNullException("realWorldPoints"); }
            CVI_PointFloat[] cviPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointFloat>(realWorldPoints);
            IntPtr result = VisionDll.imaqTransformRealWorldToPixel(calibratedImage._image, cviPoints, cviPoints.Length);
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToStructure<CoordinatesReport, CVI_TransformReport>(result, true);
        }

        //==========================================================================================
        /// <summary>Corrects a calibrated image by applying a calibration to create a spatially correct image.
        /// </summary>
        /// <param name="source">The calibrated image to correct.
        /// </param>
        /// <param name="destination">The corrected image.
        /// </param>
        /// <param name="fill">The pixel replace value for pixel points that cannot be corrected.
        /// </param>
        /// <remarks>Use this method with image types U8, I16, Sgl, Rgb32, and Hsl32.
        /// </remarks>

        public static void CorrectCalibratedImage(VisionImage source, VisionImage destination, PixelValue fill)
        {
            CorrectCalibratedImage(source, destination, fill, InterpolationMethod.ZeroOrder, null);
        }
        //==========================================================================================
        /// <summary>Corrects a calibrated image by applying a calibration to create a spatially correct image.
        /// </summary>
        /// <param name="source">The calibrated image to correct.
        /// </param>
        /// <param name="destination">The corrected image.
        /// </param>
        /// <param name="fill">The pixel replace value for pixel points that cannot be corrected.
        /// </param>
        /// <param name="interpolationMethod">Specifies the method of interpolation. Valid interpolation methods for correction are <see cref="NationalInstruments.Vision.Analysis.InterpolationMethod.ZeroOrder" crefType="Unqualified"/> and <see cref="NationalInstruments.Vision.Analysis.InterpolationMethod.Bilinear" crefType="Unqualified"/>. The default is ZeroOrder.
        /// </param>
        /// <remarks>Use this method with image types U8, I16, Sgl, Rgb32, and Hsl32.
        /// </remarks>

        public static void CorrectCalibratedImage(VisionImage source, VisionImage destination, PixelValue fill, InterpolationMethod interpolationMethod)
        {
            CorrectCalibratedImage(source, destination, fill, interpolationMethod, null);
        }
        //==========================================================================================
        /// <summary>Corrects a calibrated image by applying a calibration to create a spatially correct image.
        /// </summary>
        /// <param name="source">The calibrated image to correct.
        /// </param>
        /// <param name="destination">The corrected image.
        /// </param>
        /// <param name="fill">The pixel replace value for pixel points that cannot be corrected.
        /// </param>
        /// <param name="interpolationMethod">Specifies the method of interpolation. Valid interpolation methods for correction are <see cref="NationalInstruments.Vision.Analysis.InterpolationMethod.ZeroOrder" crefType="Unqualified"/> and <see cref="NationalInstruments.Vision.Analysis.InterpolationMethod.Bilinear" crefType="Unqualified"/>. The default is ZeroOrder.
        /// </param>
        /// <param name="roi">Specifies the portion of the image the method corrects. If you do not supply this parameter, the method corrects the entire image.
        /// </param>
        /// <remarks>Use this method with image types U8, I16, Sgl, Rgb32, and Hsl32.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// ' Calibrate the image in ImageViewer1
        /// Algorithms.SetSimpleCalibration (ImageViewer1.Image, New CoordinateSystem(New PointContour(), 45), New GridDescriptor(10, 10, CalibrationUnit.Meter))
        /// ' Correct the calibrated image in ImageViewer1 and
        /// ' store the result in i.
        /// Algorithms.CorrectCalibratedImage (imageViewer1.Image, i, New PixelValue(0), InterpolationMethod.Bilinear)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// // Calibrate the image in imageViewer1
        /// Algorithms.SetSimpleCalibration(imageViewer1.Image, new CoordinateSystem(new PointContour(), 45), new GridDescriptor(10, 10, CalibrationUnit.Meter));
        /// // Correct the calibrated image in imageViewer1 and
        /// // store the result in i.
        /// Algorithms.CorrectCalibratedImage(imageViewer1.Image, i, new PixelValue(0), InterpolationMethod.Bilinear);
        /// </code>
        /// </example>

        public static void CorrectCalibratedImage(VisionImage source, VisionImage destination, PixelValue fill, InterpolationMethod interpolationMethod, Roi roi)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            Utilities.ThrowError(VisionDll.imaqCorrectCalibratedImage(VisionImage.GetIntPtr(destination), source._image, fill.CVI_PixValue, (Int32)interpolationMethod, cviRoi));
        }

        //==========================================================================================
        /// <summary>
        /// Learns a calibration from an image of a grid of circles.
        /// </summary>
        /// <param name="image">
        /// The template used for calibrating your system.
        /// </param>
        /// <returns>
        /// The quality score of the learning process, which is a value between 0-1000. A quality of 1000 means that 
        /// the method learned the feature points perfectly with the chosen algorithm. It does not necessarily reflect 
        /// the absolute accuracy of the estimated calibration mapping, but instead reflects how well the calibration 
        /// mapping adapts to the learned grid.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 and I16 images.
        /// </remarks>

        public static double LearnCalibrationGrid(VisionImage image)
        {
            return LearnCalibrationGrid(image, new CalibrationGridOptions(), new LearnCalibrationOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Learns a calibration from an image of a grid of circles.
        /// </summary>
        /// <param name="image">
        /// The template used for calibrating your system.
        /// </param>
        /// <param name="calibrationGridOptions">
        /// Specifies information about the calibration grid image.
        /// </param>
        /// <returns>
        /// The quality score of the learning process, which is a value between 0-1000. A quality of 1000 means that 
        /// the method learned the feature points perfectly with the chosen algorithm. It does not necessarily reflect 
        /// the absolute accuracy of the estimated calibration mapping, but instead reflects how well the calibration 
        /// mapping adapts to the learned grid.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 and I16 images.
        /// </remarks>

        public static double LearnCalibrationGrid(VisionImage image, CalibrationGridOptions calibrationGridOptions)
        {
            return LearnCalibrationGrid(image, calibrationGridOptions, new LearnCalibrationOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Learns a calibration from an image of a grid of circles.
        /// </summary>
        /// <param name="image">
        /// The template used for calibrating your system.
        /// </param>
        /// <param name="calibrationGridOptions">
        /// Specifies information about the calibration grid image.
        /// </param>
        /// <param name="learnCalibrationOptions">
        /// Describes how the method learns the calibration information. 
        /// </param>
        /// <returns>
        /// The quality score of the learning process, which is a value between 0-1000. A quality of 1000 means that 
        /// the method learned the feature points perfectly with the chosen algorithm. It does not necessarily reflect 
        /// the absolute accuracy of the estimated calibration mapping, but instead reflects how well the calibration 
        /// mapping adapts to the learned grid.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 and I16 images.
        /// </remarks>

        public static double LearnCalibrationGrid(VisionImage image, CalibrationGridOptions calibrationGridOptions, LearnCalibrationOptions learnCalibrationOptions)
        {
            return LearnCalibrationGrid(image, calibrationGridOptions, learnCalibrationOptions, null);
        }
        //==========================================================================================
        /// <summary>
        /// Learns a calibration from an image of a grid of circles.
        /// </summary>
        /// <param name="image">
        /// The template used for calibrating your system.
        /// </param>
        /// <param name="calibrationGridOptions">
        /// Specifies information about the calibration grid image.
        /// </param>
        /// <param name="learnCalibrationOptions">
        /// Describes how the method learns the calibration information. 
        /// </param>
        /// <param name="roi">
        /// Determines the region of the image that the function uses in the learning process. The function ignores 
        /// all the circles in the grid that are outside the defined region when estimating the calibration 
        /// transformation. Pass null or Nothing for this parameter to learn the entire image.
        /// </param>
        /// <returns>
        /// The quality score of the learning process, which is a value between 0-1000. A quality of 1000 means that 
        /// the method learned the feature points perfectly with the chosen algorithm. It does not necessarily reflect 
        /// the absolute accuracy of the estimated calibration mapping, but instead reflects how well the calibration 
        /// mapping adapts to the learned grid.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 and I16 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Learn the calibration grid in Viewer1, only using the grid circles
        /// 'present in the Roi in Viewer1.
        /// Algorithms.LearnCalibrationGrid (imageViewer1.Image, New CalibrationGridOptions(), New LearnCalibrationOptions(), imageViewer1.Roi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Learn the calibration grid in Viewer1, only using the grid circles
        /// //present in the Roi in Viewer1.
        /// Algorithms.LearnCalibrationGrid(imageViewer1.Image, new CalibrationGridOptions(), new LearnCalibrationOptions(), imageViewer1.Roi);
        /// </code>
        /// </example>

        public static double LearnCalibrationGrid(VisionImage image, CalibrationGridOptions calibrationGridOptions, LearnCalibrationOptions learnCalibrationOptions, Roi roi)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (calibrationGridOptions == null) { throw new ArgumentNullException("calibrationGridOptions"); }
            if (learnCalibrationOptions == null) { throw new ArgumentNullException("learnCalibrationOptions"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            CVI_LearnCalibrationOptions cviLearnCalibrationOptions = new CVI_LearnCalibrationOptions();
            cviLearnCalibrationOptions.ConvertFromExternal(learnCalibrationOptions);
            CVI_GridDescriptor cviGridDescriptor = new CVI_GridDescriptor();
            cviGridDescriptor.ConvertFromExternal(calibrationGridOptions.GridDescriptor);
            CVI_CoordinateSystem cviCoordinateSystem = new CVI_CoordinateSystem();
            cviCoordinateSystem.ConvertFromExternal(learnCalibrationOptions.AxisInfo);
            CVI_RangeFloat cviRange = new CVI_RangeFloat();
            cviRange.ConvertFromExternal(calibrationGridOptions.ThresholdRange);
            float quality;
            Utilities.ThrowError(VisionDll.imaqLearnCalibrationGrid(image._image, cviRoi, ref cviLearnCalibrationOptions, ref cviGridDescriptor, ref cviCoordinateSystem, ref cviRange, out quality));
            return quality;
        }

        //==========================================================================================
        /// <summary>
        /// Learns a calibration from a set of pixel coordinates and corresponding real-world coordinates.
        /// </summary>
        /// <param name="image">
        /// The image to which the function attaches calibration information. 
        /// </param>
        /// <param name="pixelCoordinates">
        /// The coordinates of the pixel reference points.
        /// </param>
        /// <param name="realWorldCoordinates">
        /// The real-world coordinates corresponding to the <format type="italics">pixelCoordinates.</format>
        /// </param>
        /// <returns>
        /// The quality score of the learning process, which is a value between 0?000. A quality of 1000 
        /// means that the method learned the feature points perfectly with the chosen algorithm. It 
        /// does not necessarily reflect the absolute accuracy of the estimated calibration mapping, but 
        /// instead reflects how well the calibration mapping adapts to the learned points.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static double LearnCalibrationPoints(VisionImage image, Collection<PointContour> pixelCoordinates, Collection<PointContour> realWorldCoordinates)
        {
            return LearnCalibrationPoints(image, pixelCoordinates, realWorldCoordinates, new LearnCalibrationOptions(), new GridDescriptor(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Learns a calibration from a set of pixel coordinates and corresponding real-world coordinates.
        /// </summary>
        /// <param name="image">
        /// The image to which the function attaches calibration information. 
        /// </param>
        /// <param name="pixelCoordinates">
        /// The coordinates of the pixel reference points.
        /// </param>
        /// <param name="realWorldCoordinates">
        /// The real-world coordinates corresponding to the <format type="italics">pixelCoordinates.</format>
        /// </param>
        /// <param name="learnCalibrationOptions">
        /// Describes how the method learns the calibration information. 
        /// </param>
        /// <returns>
        /// The quality score of the learning process, which is a value between 0?000. A quality of 1000 
        /// means that the method learned the feature points perfectly with the chosen algorithm. It 
        /// does not necessarily reflect the absolute accuracy of the estimated calibration mapping, but 
        /// instead reflects how well the calibration mapping adapts to the learned points.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static double LearnCalibrationPoints(VisionImage image, Collection<PointContour> pixelCoordinates, Collection<PointContour> realWorldCoordinates, LearnCalibrationOptions learnCalibrationOptions)
        {
            return LearnCalibrationPoints(image, pixelCoordinates, realWorldCoordinates, learnCalibrationOptions, new GridDescriptor(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Learns a calibration from a set of pixel coordinates and corresponding real-world coordinates.
        /// </summary>
        /// <param name="image">
        /// The image to which the function attaches calibration information. 
        /// </param>
        /// <param name="pixelCoordinates">
        /// The coordinates of the pixel reference points.
        /// </param>
        /// <param name="realWorldCoordinates">
        /// The real-world coordinates corresponding to the <format type="italics">pixelCoordinates.</format>
        /// </param>
        /// <param name="learnCalibrationOptions">
        /// Describes how the method learns the calibration information. 
        /// </param>
        /// <param name="grid">
        /// Contains scaling constants for the real-world coordinates that the method uses to learn the calibration. 
        /// </param>
        /// <returns>
        /// The quality score of the learning process, which is a value between 0-1000. A quality of 1000 
        /// means that the method learned the feature points perfectly with the chosen algorithm. It 
        /// does not necessarily reflect the absolute accuracy of the estimated calibration mapping, but 
        /// instead reflects how well the calibration mapping adapts to the learned points.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static double LearnCalibrationPoints(VisionImage image, Collection<PointContour> pixelCoordinates, Collection<PointContour> realWorldCoordinates, LearnCalibrationOptions learnCalibrationOptions, GridDescriptor grid)
        {
            return LearnCalibrationPoints(image, pixelCoordinates, realWorldCoordinates, learnCalibrationOptions, grid, null);
        }
        //==========================================================================================
        /// <summary>
        /// Learns a calibration from a set of pixel coordinates and corresponding real-world coordinates.
        /// </summary>
        /// <param name="image">
        /// The image to which the function attaches calibration information. 
        /// </param>
        /// <param name="pixelCoordinates">
        /// The coordinates of the pixel reference points.
        /// </param>
        /// <param name="realWorldCoordinates">
        /// The real-world coordinates corresponding to the <format type="italics">pixelCoordinates.</format>
        /// </param>
        /// <param name="learnCalibrationOptions">
        /// Describes how the method learns the calibration information. 
        /// </param>
        /// <param name="grid">
        /// Contains scaling constants for the real-world coordinates that the method uses to learn the calibration. 
        /// </param>
        /// <param name="roi">
        /// Determines which pixel coordinates the method uses in the learning process. The function ignores 
        /// all pixel coordinates that are outside the defined ROI when estimating the calibration 
        /// transformation. Pass null or Nothing for this parameter to learn all of the pixel coordinates. 
        /// </param>
        /// <returns>
        /// The quality score of the learning process, which is a value between 0-1000. A quality of 1000 
        /// means that the method learned the feature points perfectly with the chosen algorithm. It 
        /// does not necessarily reflect the absolute accuracy of the estimated calibration mapping, but 
        /// instead reflects how well the calibration mapping adapts to the learned points.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Learn some calibrated points.
        /// Dim PixelPoints As New Collection(Of PointContour)
        /// Dim RealWorldPoints As New Collection(Of PointContour)
        /// PixelPoints.Add (New PointContour(10, 10))
        /// RealWorldPoints.Add (New PointContour(20.5, 25))
        /// PixelPoints.Add (New PointContour(10, 15))
        /// RealWorldPoints.Add (New PointContour(20.5, 35))
        /// PixelPoints.Add (New PointContour(15, 10))
        /// RealWorldPoints.Add (New PointContour(22, 25))
        /// PixelPoints.Add (New PointContour(15, 15))
        /// RealWorldPoints.Add (New PointContour(22, 35))
        ///  
        /// Algorithms.LearnCalibrationPoints (imageViewer1.Image, PixelPoints, RealWorldPoints)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// //Learn some calibrated points.
        /// Collection&lt;PointContour&gt; pixelPoints = new Collection&lt;PointContour&gt;();
        /// Collection&lt;PointContour&gt; realWorldPoints = new Collection&lt;PointContour&gt;();
        /// pixelPoints.Add(new PointContour(10, 10));
        /// realWorldPoints.Add(new PointContour(20.5, 25));
        /// pixelPoints.Add(new PointContour(10, 15));
        /// realWorldPoints.Add(new PointContour(20.5, 35));
        /// pixelPoints.Add(new PointContour(15, 10));
        /// realWorldPoints.Add(new PointContour(22, 25));
        /// pixelPoints.Add(new PointContour(15, 15));
        /// realWorldPoints.Add(new PointContour(22, 35));
        ///  
        /// Algorithms.LearnCalibrationPoints(imageViewer1.Image, pixelPoints, realWorldPoints);
        /// </code>
        /// </example>

        public static double LearnCalibrationPoints(VisionImage image, Collection<PointContour> pixelCoordinates, Collection<PointContour> realWorldCoordinates, LearnCalibrationOptions learnCalibrationOptions, GridDescriptor grid, Roi roi)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (pixelCoordinates == null) { throw new ArgumentNullException("pixelCoordinates"); }
            if (realWorldCoordinates == null) { throw new ArgumentNullException("realWorldCoordinates"); }
            if (learnCalibrationOptions == null) { throw new ArgumentNullException("learnCalibrationOptions"); }
            if (grid == null) { throw new ArgumentNullException("grid"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            CVI_LearnCalibrationOptions cviOptions = new CVI_LearnCalibrationOptions();
            cviOptions.ConvertFromExternal(learnCalibrationOptions);
            // Make sure the CVI_CalibrationPoints gets disposed properly.
            CVI_CalibrationPoints cviCalibrationPoints = new CVI_CalibrationPoints();
            cviCalibrationPoints.ConvertFromExternal(pixelCoordinates, realWorldCoordinates);
            try {
                CVI_GridDescriptor cviGridDescriptor = new CVI_GridDescriptor();
                cviGridDescriptor.ConvertFromExternal(grid);
                CVI_CoordinateSystem cviCoordinateSystem = new CVI_CoordinateSystem();
                cviCoordinateSystem.ConvertFromExternal(learnCalibrationOptions.AxisInfo);
                float quality;
                Utilities.ThrowError(VisionDll.imaqLearnCalibrationPoints(image._image, ref cviCalibrationPoints, cviRoi, ref cviOptions, ref cviGridDescriptor, ref cviCoordinateSystem, out quality));
                return quality;
            } finally {
                cviCalibrationPoints.Dispose();
            }
        }

        //==========================================================================================
        /// <summary>
        /// Sets a simple calibration for an image or resets the real-world coordinate system in a calibrated image. 
        /// When used to reset a coordinate system, if the correction table is still required, the table must be 
        /// relearned. When the coordinate system angle is set to zero, you do not need to make a correction.
        /// </summary>
        /// <param name="image">
        /// The image to be calibrated.
        /// </param>
        /// <param name="system">
        /// Defines the coordinate system for the calibrated real-world coordinates. 
        /// </param>
        /// <param name="grid">
        /// Defines scaling constants for the image. If the image has been calibrated previously, using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/> method 
        /// or the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>  
        /// method, this parameter is ignored and the previously defined scaling constants are used. 
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void SetSimpleCalibration(VisionImage image, CoordinateSystem system, GridDescriptor grid)
        {
            SetSimpleCalibration(image, system, grid, ScalingMethod.ScaleToPreserveArea, false);
        }
        //==========================================================================================
        /// <summary>
        /// Sets a simple calibration for an image or resets the real-world coordinate system in a calibrated image. 
        /// When used to reset a coordinate system, if the correction table is still required, the table must be 
        /// relearned. When the coordinate system angle is set to zero, you do not need to make a correction.
        /// </summary>
        /// <param name="image">
        /// The image to be calibrated.
        /// </param>
        /// <param name="system">
        /// Defines the coordinate system for the calibrated real-world coordinates.
        /// </param>
        /// <param name="grid">
        /// Defines scaling constants for the image. If the image has been calibrated previously, using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/> method 
        /// or the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>  
        /// method, this parameter is ignored and the previously defined scaling constants are used. 
        /// </param>
        /// <param name="method">
        /// Defines the scaling method correction functions used to correct the image.  If the image has been calibrated previously, using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/> method 
        /// or the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>  
        /// method, this parameter is ignored and the previously defined scaling is used. Otherwise, the default value for this parameter is ScaleToPreserveArea.
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>

        public static void SetSimpleCalibration(VisionImage image, CoordinateSystem system, GridDescriptor grid, ScalingMethod method)
        {
            SetSimpleCalibration(image, system, grid, method, false);
        }
        //==========================================================================================
        /// <summary>
        /// Sets a simple calibration for an image or resets the real-world coordinate system in a calibrated image. 
        /// When used to reset a coordinate system, if the correction table is still required, the table must be 
        /// relearned. When the coordinate system angle is set to zero, you do not need to make a correction.
        /// </summary>
        /// <param name="image">
        /// The image to be calibrated.
        /// </param>
        /// <param name="system">
        /// Defines the coordinate system for the calibrated real-world coordinates.
        /// </param>
        /// <param name="grid">
        /// Defines scaling constants for the image. If the image has been calibrated previously, using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/> method 
        /// or the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>  
        /// method, this parameter is ignored and the previously defined scaling constants are used. 
        /// </param>
        /// <param name="method">
        /// Defines the scaling method correction functions used to correct the image.  If the image has been calibrated previously, using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/> method 
        /// or the <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>  
        /// method, this parameter is ignored and the previously defined scaling is used. Otherwise, the default value for this parameter is ScaleToPreserveArea.
        /// </param>
        /// <param name="learnTable">
        /// Set this parameter to true to process and store the correction table. The correction table accelerates 
        /// the process of correcting an image and is useful if you plan to correct several images using this 
        /// calibration setup. The default is false.
        /// </param>
        /// <remarks>
        /// Use this method with U8, U16, I16, Single, Rgb32, and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim CoordSystem As New CoordinateSystem(New PointContour(5, 10), 45, AxisOrientation.Direct)
        /// Dim GridDesc As New GridDescriptor(1, 2, CalibrationUnit.Centimeter)
        ///  
        /// 'Set the simple calibration on the image in viewer1.
        /// Algorithms.SetSimpleCalibration (imageViewer1.Image, CoordSystem, GridDesc)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// CoordinateSystem coordSystem = new CoordinateSystem(new PointContour(5, 10), 45, 
        ///  
        /// AxisOrientation.Direct);
        /// GridDescriptor gridDesc = new GridDescriptor(1, 2, CalibrationUnit.Centimeter);
        ///  
        /// // Set the simple calibration on the image in viewer1.
        /// Algorithms.SetSimpleCalibration(imageViewer1.Image, coordSystem, gridDesc);
        ///  
        /// </code>
        /// </example>

        public static void SetSimpleCalibration(VisionImage image, CoordinateSystem system, GridDescriptor grid, ScalingMethod method, bool learnTable)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (system == null) { throw new ArgumentNullException("system"); }
            if (grid == null) { throw new ArgumentNullException("grid"); }
            CVI_GridDescriptor cviGridDescriptor = new CVI_GridDescriptor();
            cviGridDescriptor.ConvertFromExternal(grid);
            CVI_CoordinateSystem cviCoordinateSystem = new CVI_CoordinateSystem();
            cviCoordinateSystem.ConvertFromExternal(system);
            Utilities.ThrowError(VisionDll.imaqSetSimpleCalibration(image._image, (Int32)method, learnTable ? 1 : 0, ref cviGridDescriptor, ref cviCoordinateSystem));
        }
        //==========================================================================================
        /// <summary>Copies calibration information from a calibrated image to an image to be calibrated. Both images must be the same size.
        /// </summary>
        /// <param name="source">The calibrated input image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <remarks>
        /// Use <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/>, or <see cref="NationalInstruments.Vision.Analysis.Algorithms.SetSimpleCalibration" crefType="Unqualified"/> to set the calibration information in the source image.
        /// </remarks>

        public static void CopyCalibrationInformation(VisionImage source, VisionImage destination)
        {
            CopyCalibrationInformation(source, destination, new PointContour());
        }
        //==========================================================================================
        /// <summary>Copies calibration information from a calibrated image to an image to be calibrated. Both images must be the same size.
        /// </summary>
        /// <param name="source">The calibrated input image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="offset">The position of the source image relative to the destination image.
        /// </param>
        /// <remarks>
        /// Use <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationGrid" crefType="Unqualified"/>, <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnCalibrationPoints" crefType="Unqualified"/>, or <see cref="NationalInstruments.Vision.Analysis.Algorithms.SetSimpleCalibration" crefType="Unqualified"/> to set the calibration information in the source image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// ' Copy the calibration information from the image in ImageViewer1 into i
        /// Algorithms.CopyCalibrationInformation (imageViewer1.Image, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// // Copy the calibration information from the image in imageViewer1 into i
        /// Algorithms.CopyCalibrationInformation(imageViewer1.Image, i);</code>
        /// </example>

        public static void CopyCalibrationInformation(VisionImage source, VisionImage destination, PointContour offset)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (offset == null) { throw new ArgumentNullException("offset"); }
            CVI_Point cviOffset = new CVI_Point();
            cviOffset.ConvertFromExternal(offset);
            Utilities.ThrowError(VisionDll.imaqCopyCalibrationInfo2(destination._image, source._image, cviOffset));
        }
        //==========================================================================================
        /// <summary> CalibrationTargetToPoints detects the circular dots in binary image and returns the pixel and real world points for calibration.
        /// </summary>
        /// <param name="image">The calibrated input image.
        /// </param>
        /// <param name="roi">The roi.
        /// </param>
        /// <param name="gridDescriptor">The grid descriptor.</param>
        /// <returns>
        /// The set of reference points to use in learning a calibration transformation.
        /// </returns>

        public static CalibrationReferencePoints CalibrationTargetToPoints(VisionImage image, Roi roi)
        {
            return CalibrationTargetToPoints(image, roi, new GridDescriptor());
        }
        //==========================================================================================
        /// <summary> CalibrationTargetToPoints detects the circular dots in binary image and returns the pixel and real world points for calibration.
        /// </summary>
        /// <param name="image">The image with circular dots.
        /// </param>
        /// <param name="roi">The ROI to be corrected.
        /// </param>
        /// <param name="gridDescriptor">The Grid Descriptor contains information about a grid image that is used to learn the calibration.
        /// </param>
        /// <returns>
        /// The set of reference points to use in learning a calibration transformation.
        /// </returns>

        public static CalibrationReferencePoints CalibrationTargetToPoints(VisionImage image, Roi roi, GridDescriptor gridDescriptor)
        {
            return CalibrationTargetToPoints(image, roi, gridDescriptor, new MaxGridSize());
        }
        //==========================================================================================
        /// <summary> 
        /// CalibrationTargetToPoints detects the circular dots in binary image and returns the pixel and real world points for calibration.
        /// </summary>
        /// <param name="image">The image with circular dots.
        /// </param>
        /// <param name="roi">The ROI to be corrected.
        /// </param>
        /// <param name="gridDescriptor">The Grid Descriptor contains information about a grid image that is used to learn the calibration.
        /// </param>
        /// <param name="maxGridSize">Specifies the limit of grid size to be used for pixel and real world points. 
        /// </param>
        /// <returns>
        /// The set of reference points to use in learning a calibration transformation.
        /// </returns>

        public static CalibrationReferencePoints CalibrationTargetToPoints(VisionImage image, Roi roi, GridDescriptor gridDescriptor, MaxGridSize maxGridSize)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (gridDescriptor == null) { throw new ArgumentNullException("gridDescriptor"); }
            if (maxGridSize == null) { throw new ArgumentNullException("maxGridSize"); }
            CVI_GridDescriptor _gridDescriptor = new CVI_GridDescriptor();
            _gridDescriptor.ConvertFromExternal(gridDescriptor);
            CVI_MaxGridSize _maxGridSize = new CVI_MaxGridSize();
            _maxGridSize.ConvertFromExternal(maxGridSize);
            IntPtr report = VisionDll.imaqCalibrationTargetToPoints(image._image, roi._roi, ref _gridDescriptor, ref _maxGridSize);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<CalibrationReferencePoints, CVI_CalibrationReferencePoints>(report, true);
        }
        //==========================================================================================
        /// <summary> 
        /// SetSimpleCalibration2 sets a simple calibration using pixel distances in x and y direction.
        /// </summary>
        /// <param name="image">The uncalibrated image.
        /// </param>
        /// <param name="gridDescriptor">The grid descriptor specifies pixel distances in x and y direction for real world units.
        /// </param>

        public static void SetSimpleCalibration2(VisionImage image, GridDescriptor gridDescriptor)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (gridDescriptor == null) { throw new ArgumentNullException("gridDescriptor"); }
            CVI_GridDescriptor _gridDescriptor = new CVI_GridDescriptor();
            _gridDescriptor.ConvertFromExternal(gridDescriptor);
            Utilities.ThrowError(VisionDll.imaqSetSimpleCalibration2(image._image, ref _gridDescriptor));
        }
        //==========================================================================================
        /// <summary> CalibrationSetAxisInfo assigns the coodinate system information to calibration template image.
        /// </summary>
        /// <param name="image">A reference to the template used for calibrating your system.
        /// </param>

        public static void CalibrationSetAxisInfo(VisionImage image)
        {
            CalibrationSetAxisInfo(image, new CoordinateSystem(new PointContour(0, 0), 0, AxisOrientation.Indirect));
        }
        //==========================================================================================
        /// <summary> CalibrationSetAxisInfo assigns the coodinate system information to calibration template image.
        /// </summary>
        /// <param name="image">A reference to the template used for calibrating your system.
        /// </param>
        /// <param name="axisInfo">Defines a Reference Coordinate System for the real-world coordinates.
        /// </param>

        public static void CalibrationSetAxisInfo(VisionImage image, CoordinateSystem axisInfo)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (axisInfo == null) { throw new ArgumentNullException("axisInfo"); }
            CVI_CoordinateSystem axisInformation = new CVI_CoordinateSystem();
            axisInformation.ConvertFromExternal(axisInfo);
            Utilities.ThrowError(VisionDll.imaqCalibrationSetAxisInfo(image._image, ref axisInformation));
        }

        //==========================================================================================
        /// <summary> CalibrationSetAxisInfoByReferencePoints assigns the coodinate system information to calibration template image using 3 reference points
        /// </summary>
        /// <param name="image">A reference to the template used for calibrating your system.
        /// </param>
        /// <param name="pixelPoints">Pixel world points (must be 3)
        /// </param>
        /// <param name="realWorldPoints">Real world points (must be 3)
        /// </param>        
public static void CalibrationSetAxisInfoByReferencePoints(VisionImage image, Collection<PointContour> pixelPoints, Collection<PointContour> realWorldPoints)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (pixelPoints == null) { throw new ArgumentNullException("pixelPoints"); }
            CVI_PointDouble[] cviPixelPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointDouble>(pixelPoints);
            if (realWorldPoints == null) { throw new ArgumentNullException("realWorldPoints"); }
            CVI_PointDouble[] cviRealWorldPoints = Utilities.ConvertCollectionToArray<PointContour, CVI_PointDouble>(realWorldPoints);
            Utilities.ThrowError(VisionDll.imaqCalibrationSetAxisInfoByReferencePoints(image._image, cviPixelPoints, cviPixelPoints.Length, cviRealWorldPoints, cviRealWorldPoints.Length));
        }

        //==========================================================================================
        /// <summary> Gets the thumbnails, stored during learning calibration.
        /// </summary>
        /// <param name="templateImage">Reference to the input image that determines the output calibration information.
        /// </param>
        /// <param name="thumbnailImage">Reference to the thumbnail image to be stored.
        /// </param>

        public static void CalibrationGetThumbnailImage(VisionImage templateImage, VisionImage thumbnailImage)
        {
            CalibrationGetThumbnailImage(templateImage, thumbnailImage, CalibrationThumbnailType.CameraModelType);
        }
        //==========================================================================================
        /// <summary> Gets the thumbnails, stored during learning calibration.
        /// </summary>
        /// <param name="templateImage">Reference to the input image that determines the output calibration information.
        /// </param>
        /// <param name="thumbnailImage">Reference to the thumbnail image to be stored.
        /// </param>
        /// <param name="type">Specifies the type of thumbnail to be returned. Default is Camera Model type.
        /// </param>

        public static void CalibrationGetThumbnailImage(VisionImage templateImage, VisionImage thumbnailImage, CalibrationThumbnailType type)
        {
            CalibrationGetThumbnailImage(templateImage, thumbnailImage, type, 0);
        }
        //==========================================================================================
        /// <summary> Gets the thumbnails, stored during learning calibration.
        /// </summary>
        /// <param name="templateImage">Reference to the input image that determines the output calibration information.
        /// </param>
        /// <param name="thumbnailImage">Reference to the thumbnail image to be stored.
        /// </param>
        /// <param name="type">Specifies the type of thumbnail to be returned. Default is Camera Model type.
        /// </param>
        /// <param name="index">Specifies the index of thumbnail to be returned from distortion and camera model template images. Default value is 0.
        /// </param>

        public static void CalibrationGetThumbnailImage(VisionImage templateImage, VisionImage thumbnailImage, CalibrationThumbnailType type, UInt32 index)
        {
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (thumbnailImage == null) { throw new ArgumentNullException("thumbnailImage"); }
            thumbnailImage.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqCalibrationGetThumbnailImage(templateImage._image, thumbnailImage._image, type, index));
        }
        //==========================================================================================
        /// <summary> Gets Calibration information associated with an image.
        /// </summary>
        /// <param name="image">The calibrated input image.
        /// </param>
        /// <returns>
        /// The calibration information associated with an image.
        /// </returns>

        public static GetCalibrationInfoReport CalibrationGetCalibrationInfo(VisionImage image)
        {
            return CalibrationGetCalibrationInfo(image, true);
        }
        //==========================================================================================
        /// <summary> Gets Calibration information associated with an image.
        /// </summary>
        /// <param name="image">The calibrated input image.
        /// </param>
        /// <param name="isGetErrorMap">Set it to true if the Error map is also required in
        /// the result.
        /// </param>
        /// <returns>
        /// The calibration information associated with an image.
        /// </returns>

        public static GetCalibrationInfoReport CalibrationGetCalibrationInfo(VisionImage image, bool isGetErrorMap)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            UInt32 _isGetErrorMap = (UInt32)(isGetErrorMap ? 1 : 0); 
            IntPtr report = VisionDll.imaqCalibrationGetCalibrationInfo(image._image, _isGetErrorMap);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<GetCalibrationInfoReport, CVI_GetCalibrationInfoReport>(report, true);
        }
        //==========================================================================================
        /// <summary> Gets the internal and external paramters of learned camera model.
        /// </summary>
        /// <param name="image">The reference to the input image that determines the output calibration information.
        /// </param>
        /// <returns>
        /// The internal and external paramters of learned camera model.
        /// </returns>

        public static CameraParametersReport CalibrationGetCameraParameters(VisionImage image)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            IntPtr report = VisionDll.imaqCalibrationGetCameraParameters(image._image);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<CameraParametersReport, CVI_CameraParametersReport>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// CalibrationCompactInformation removes information, which was used learn the calibration, and thumbnails to make the size of calibration smaller. Re-learning and retrieving thumbnails are not possible after compact operation.
        /// </summary>
        /// <param name="image">The calibrated input image.
        /// </param>

        public static void CalibrationCompactInformation(VisionImage image)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqCalibrationCompactInformation(image._image));
        }
        //==========================================================================================
        /// <summary> 
        /// Learns perspective calibration to correct the distortion, created by non-perpendicular planes to camera.
        /// </summary>
        /// <param name="templateImage">The template used for calibrating your system.
        /// </param>
        /// <param name="image">The thumbnail image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>

        public static void LearnPerspectiveCalibration(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints)
        {
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (referencePoints == null) { throw new ArgumentNullException("referencePoints"); }
            CVI_CalibrationReferencePoints referencePts = new CVI_CalibrationReferencePoints();
            referencePts.ConvertFromExternal(referencePoints);
            Utilities.ThrowError(VisionDll.imaqLearnPerspectiveCalibration(templateImage._image, image._image, ref referencePts));
        }
        //==========================================================================================
        /// <summary> 
        /// Learns the microplane calibration information for correcting images in a nonplanar working plane.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>

        public static void LearnMicroPlaneCalibration(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints)
        {
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (referencePoints == null) { throw new ArgumentNullException("referencePoints"); }
            CVI_CalibrationReferencePoints referencePts = new CVI_CalibrationReferencePoints();
            referencePts.ConvertFromExternal(referencePoints);
            Utilities.ThrowError(VisionDll.imaqLearnMicroPlaneCalibration(templateImage._image, image._image, ref referencePts));
        }
        //==========================================================================================
        /// <summary> 
        /// Learns detailed camera characteristics, including the focal length, optical center, and 
        /// distortion model. Because a camera model includes a distortion model, you do not need to
        /// compute a separate distortion model.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>

        public static InternalParameters LearnCameraModel(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints)
        {
            return LearnCameraModel(templateImage, image, referencePoints, new CalibrationModelSetup());
        }//==========================================================================================
        /// <summary> 
        /// Learns detailed camera characteristics, including the focal length, optical center, and 
        /// distortion model. Because a camera model includes a distortion model, you do not need to
        /// compute a separate distortion model.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>
        /// <param name="calibrationModelSetup"> Specifies the distortion model settings to use. 
        /// </param>

        public static InternalParameters LearnCameraModel(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints, CalibrationModelSetup calibrationModelSetup)
        {
            return LearnCameraModel(templateImage, image, referencePoints, calibrationModelSetup, true);
        }
        //==========================================================================================
        /// <summary> 
        /// Learns detailed camera characteristics, including the focal length, optical center, and 
        /// distortion model. Because a camera model includes a distortion model, you do not need to
        /// compute a separate distortion model.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>
        /// <param name="calibrationModelSetup"> Specifies the distortion model settings to use. 
        /// </param>
        /// <param name="addPointsAndLearn"> When TRUE, adds points and learns the distortion model. 
        /// When FALSE, only points are added. The default is TRUE. If you learn calibration using one
        /// image, set this parameter to TRUE to add points and learn the calibration template. If you
        /// learn calibration from multiple grid images, set this parameter to FALSE for the first n-1
        /// images, then set it to TRUE for the last image.
        /// </param>

        public static InternalParameters LearnCameraModel(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints, CalibrationModelSetup calibrationModelSetup, bool addPointsAndLearn)
        {
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (referencePoints == null) { throw new ArgumentNullException("referencePoints"); }
            CVI_CalibrationReferencePoints referencePts = new CVI_CalibrationReferencePoints();
            referencePts.ConvertFromExternal(referencePoints);
            if (calibrationModelSetup == null) { throw new ArgumentNullException("calibrationModelSetup"); }
            CVI_CalibrationModelSetup modelSetup = new CVI_CalibrationModelSetup();
            modelSetup.ConvertFromExternal(calibrationModelSetup);
            Int32 isAddPointsAndLearn = (Int32)(addPointsAndLearn ? 1 : 0);
            IntPtr report = VisionDll.imaqLearnCameraModel(templateImage._image, image._image, ref referencePts, ref modelSetup, isAddPointsAndLearn);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<InternalParameters, CVI_InternalParameters>(report, true);
        }
        //==========================================================================================
        /// <summary> 
        /// Learns detailed camera characteristics, including the focal length, optical center, and 
        /// distortion model. Because a camera model includes a distortion model, you do not need to
        /// compute a separate distortion model.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>

        public static InternalParameters LearnCameraModel2(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints)
        {
            return LearnCameraModel(templateImage, image, referencePoints, new CalibrationModelSetup());
        }//==========================================================================================
        /// <summary> 
        /// Learns detailed camera characteristics, including the focal length, optical center, and 
        /// distortion model. Because a camera model includes a distortion model, you do not need to
        /// compute a separate distortion model.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>
        /// <param name="calibrationModelSetup"> Specifies the distortion model settings to use. 
        /// </param>

        public static InternalParameters LearnCameraModel2(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints, CalibrationModelSetup calibrationModelSetup)
        {
            return LearnCameraModel(templateImage, image, referencePoints, calibrationModelSetup, true);
        }
        /// <summary> 
        /// Learns detailed camera characteristics, including the focal length, optical center, and 
        /// distortion model. Because a camera model includes a distortion model, you do not need to
        /// compute a separate distortion model.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints">The set of reference points to use in learning a calibration transformation.
        /// </param>
        /// <param name="calibrationModelSetup">Specifies the distortion model settings to use. 
        /// </param>
        /// <param name="addPointsAndLearn">When TRUE, adds points and learns the distortion model. 
        /// When FALSE, only points are added. The default is TRUE. If you learn calibration using one
        /// image, set this parameter to TRUE to add points and learn the calibration template. If you
        /// learn calibration from multiple grid images, set this parameter to FALSE for the first n-1
        /// images, then set it to TRUE for the last image.
        /// </param>

        public static InternalParameters LearnCameraModel2(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints, CalibrationModelSetup calibrationModelSetup, bool addPointsAndLearn)
        {
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (referencePoints == null) { throw new ArgumentNullException("referencePoints"); }
            CVI_CalibrationReferencePoints referencePts = new CVI_CalibrationReferencePoints();
            referencePts.ConvertFromExternal(referencePoints);
            if (calibrationModelSetup == null) { throw new ArgumentNullException("calibrationModelSetup"); }
            CVI_CalibrationModelSetup modelSetup = new CVI_CalibrationModelSetup();
            modelSetup.ConvertFromExternal(calibrationModelSetup);
            Int32 isAddPointsAndLearn = (Int32)(addPointsAndLearn ? 1 : 0);
            IntPtr report = VisionDll.imaqLearnCameraModel2(templateImage._image, image._image, ref referencePts, ref modelSetup, isAddPointsAndLearn);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<InternalParameters, CVI_InternalParameters>(report, true);
        }
        //==========================================================================================
        /// <summary> 
        /// Learns a distortion model of the camera and lens setup. If your camera is not perpendicular to 
        /// the object under inspection, you can combine distortion modeling with perspective calibration.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>

        public static void LearnDistortionModel(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints)
        {
            LearnDistortionModel(templateImage, image, referencePoints, new CalibrationModelSetup());
        }
        //==========================================================================================
        /// <summary> 
        /// Learns a distortion model of the camera and lens setup. If your camera is not perpendicular to 
        /// the object under inspection, you can combine distortion modeling with perspective calibration.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>
        /// <param name="calibrationModelSetup"> Specifies the distortion model settings to use. 
        /// </param>

        public static void LearnDistortionModel(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints, CalibrationModelSetup calibrationModelSetup)
        {
            LearnDistortionModel(templateImage, image, referencePoints, calibrationModelSetup, true);
        }
        //==========================================================================================
        /// <summary> 
        /// Learns a distortion model of the camera and lens setup. If your camera is not perpendicular to 
        /// the object under inspection, you can combine distortion modeling with perspective calibration.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>
        /// <param name="calibrationModelSetup"> Specifies the distortion model settings to use. 
        /// </param>
        /// <param name="addPointsAndLearn"> When TRUE, adds points and learns the distortion model. 
        /// When FALSE, only points are added. The default is TRUE. If you learn calibration using one
        /// image, set this parameter to TRUE to add points and learn the calibration template. If you
        /// learn calibration from multiple grid images, set this parameter to FALSE for the first n-1
        /// images, then set it to TRUE for the last image.
        /// </param>

        public static void LearnDistortionModel(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints, CalibrationModelSetup calibrationModelSetup, bool addPointsAndLearn)
        {
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (referencePoints == null) { throw new ArgumentNullException("referencePoints"); }
            CVI_CalibrationReferencePoints referencePts = new CVI_CalibrationReferencePoints();
            referencePts.ConvertFromExternal(referencePoints);
            if (calibrationModelSetup == null) { throw new ArgumentNullException("calibrationModelSetup"); }
            CVI_CalibrationModelSetup modelSetup = new CVI_CalibrationModelSetup();
            modelSetup.ConvertFromExternal(calibrationModelSetup);
            Int32 isAddPointsAndLearn = (Int32)(addPointsAndLearn ? 1 : 0);
            Utilities.ThrowError(VisionDll.imaqLearnDistortionModel(templateImage._image, image._image, ref referencePts, ref modelSetup, isAddPointsAndLearn));
        }
        //==========================================================================================
        /// <summary> 
        /// Learns a distortion model of the camera and lens setup. If your camera is not perpendicular to 
        /// the object under inspection, you can combine distortion modeling with perspective calibration.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>

        public static void LearnDistortionModel2(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints)
        {
            LearnDistortionModel2(templateImage, image, referencePoints, new CalibrationModelSetup());
        }//==========================================================================================
        /// <summary> 
        /// Learns a distortion model of the camera and lens setup. If your camera is not perpendicular to 
        /// the object under inspection, you can combine distortion modeling with perspective calibration.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>
        /// <param name="calibrationModelSetup"> Specifies the distortion model settings to use. 
        /// </param>

        public static void LearnDistortionModel2(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints, CalibrationModelSetup calibrationModelSetup)
        {
            LearnDistortionModel2(templateImage, image, referencePoints, calibrationModelSetup, true);
        }
        //==========================================================================================
        /// <summary> 
        /// Learns a distortion model of the camera and lens setup. If your camera is not perpendicular to 
        /// the object under inspection, you can combine distortion modeling with perspective calibration.
        /// </summary>
        /// <param name="templateImage">The image containing calibration image.
        /// </param>
        /// <param name="image">The source image.
        /// </param>
        /// <param name="referencePoints"> The set of reference points to use in learning a calibration transformation.
        /// </param>
        /// <param name="calibrationModelSetup"> Specifies the distortion model settings to use. 
        /// </param>
        /// <param name="addPointsAndLearn"> When TRUE, adds points and learns the distortion model. 
        /// When FALSE, only points are added. The default is TRUE. If you learn calibration using one
        /// image, set this parameter to TRUE to add points and learn the calibration template. If you
        /// learn calibration from multiple grid images, set this parameter to FALSE for the first n-1
        /// images, then set it to TRUE for the last image.
        /// </param>

        public static void LearnDistortionModel2(VisionImage templateImage, VisionImage image, CalibrationReferencePoints referencePoints, CalibrationModelSetup calibrationModelSetup, bool addPointsAndLearn)
        {
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (referencePoints == null) { throw new ArgumentNullException("referencePoints"); }
            CVI_CalibrationReferencePoints referencePts = new CVI_CalibrationReferencePoints();
            referencePts.ConvertFromExternal(referencePoints);
            if (calibrationModelSetup == null) { throw new ArgumentNullException("calibrationModelSetup"); }
            CVI_CalibrationModelSetup modelSetup = new CVI_CalibrationModelSetup();
            modelSetup.ConvertFromExternal(calibrationModelSetup);
            Int32 isAddPointsAndLearn = (Int32)(addPointsAndLearn ? 1 : 0);
            Utilities.ThrowError(VisionDll.imaqLearnDistortionModel2(templateImage._image, image._image, ref referencePts, ref modelSetup, isAddPointsAndLearn));
        }
        //==========================================================================================
        /// <summary> 
        /// Sets parameters used to correct the calibration in an image.
        /// </summary>
        /// <param name="image">The source image containing calibration information.
        /// </param>
        /// <param name="calibrationLearnSetupInfo"> Provides information about how the image correction is performed.
        /// </param>
        /// <param name="correctionRoi"> Specifies the ROI to use when correcting the calibration in an image. 
        /// </param>

        public static void CalibrationCorrectionLearnSetup(VisionImage image, CalibrationLearnSetupInfo calibrationLearnSetupInfo, Roi correctionRoi)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (correctionRoi == null) { throw new ArgumentNullException("correctionRoi"); }
            correctionRoi.ThrowIfDisposed();
            if (calibrationLearnSetupInfo == null) { throw new ArgumentNullException("calibrationLearnSetupInfo"); }
            CVI_CalibrationLearnSetupInfo setupInfo = new CVI_CalibrationLearnSetupInfo();
            setupInfo.ConvertFromExternal(calibrationLearnSetupInfo);
            Utilities.ThrowError(VisionDll.imaqCalibrationCorrectionLearnSetup(image._image, ref setupInfo, correctionRoi._roi));
        }

        #endregion

        #region Color Matching functions
        //==========================================================================================
        /// <summary>
        /// Extracts the color features of an image, which can be used for color matching or other 
        /// applications related with color information, such as color identification and color image 
        /// segmentation. Use these features for color matching with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColor" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="image">
        /// The image containing the color information to learn. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ColorInformation" crefType="Unqualified"/> object 
        /// containing the color information, which can be passed to the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColor" crefType="Unqualified"/> method.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>

        public static ColorInformation LearnColor(VisionImage image)
        {
            return LearnColor(image, null, ColorSensitivity.Low, 80);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts the color features of an image, which can be used for color matching or other 
        /// applications related with color information, such as color identification and color image 
        /// segmentation. Use these features for color matching with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColor" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="image">
        /// The image containing the color information to learn. 
        /// </param>
        /// <param name="roi">
        /// Describes regions in the image that contain the color to be learned. If regions contains multiple 
        /// region objects, the color information in all these regions is accumulated before learning. Pass null
        /// or Nothing for this parameter to learn color information about the whole image.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ColorInformation" crefType="Unqualified"/> object 
        /// containing the color information, which can be passed to the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColor" crefType="Unqualified"/> method.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>

        public static ColorInformation LearnColor(VisionImage image, Roi roi)
        {
            return LearnColor(image, roi, ColorSensitivity.Low, 80);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts the color features of an image, which can be used for color matching or other 
        /// applications related with color information, such as color identification and color image 
        /// segmentation. Use these features for color matching with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColor" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="image">
        /// The image containing the color information to learn. 
        /// </param>
        /// <param name="roi">
        /// Describes regions in the image that contain the color to be learned. If regions contains multiple 
        /// region objects, the color information in all these regions is accumulated before learning. Pass null
        /// or Nothing for this parameter to learn color information about the whole image.
        /// </param>
        /// <param name="sensitivity">
        /// Specifies the sensitivity of the color information in the image. The sensitivity can be low, medium, or high. Set this value
        /// to high when you need to distinguish colors with close hue values. The default is Low.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ColorInformation" crefType="Unqualified"/> object 
        /// containing the color information, which can be passed to the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColor" crefType="Unqualified"/> method.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>

        public static ColorInformation LearnColor(VisionImage image, Roi roi, ColorSensitivity sensitivity)
        {
            return LearnColor(image, roi, sensitivity, 80);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts the color features of an image, which can be used for color matching or other 
        /// applications related with color information, such as color identification and color image 
        /// segmentation. Use these features for color matching with the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColor" crefType="Unqualified"/> method.
        /// </summary>
        /// <param name="image">
        /// The image containing the color information to learn. 
        /// </param>
        /// <param name="roi">
        /// Describes regions in the image that contain the color to be learned. If regions contains multiple 
        /// region objects, the color information in all these regions is accumulated before learning. Pass null
        /// or Nothing for this parameter to learn color information about the whole image.
        /// </param>
        /// <param name="sensitivity">
        /// Specifies the sensitivity of the color information in the image. The sensitivity can be low, medium, or high. Set this value
        /// to high when you need to distinguish colors with close hue values. The default is Low.
        /// </param>
        /// <param name="saturation">
        /// Sets a threshold value which the method uses to separate colors with similar hues. The 
        /// function classifies colors below the given saturation value separately from colors above the 
        /// given saturation value. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ColorInformation" crefType="Unqualified"/> object 
        /// containing the color information, which can be passed to the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchColor" crefType="Unqualified"/> method.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Learn the image in viewer1 and match it in viewer2.
        /// Dim ColorInfo as ColorInformation = Algorithms.LearnColor (imageViewer1.Image)
        /// Dim ColorScore as Integer = Algorithms.MatchColor (imageViewer2.Image, ColorInfo)(0)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Learn the image in viewer1 and match it in viewer2.
        /// ColorInformation colorInfo = Algorithms.LearnColor(imageViewer1.Image);
        /// int colorScore = Algorithms.MatchColor(imageViewer2.Image, colorInfo)[0];
        /// </code>
        /// </example>

        public static ColorInformation LearnColor(VisionImage image, Roi roi, ColorSensitivity sensitivity, Int32 saturation)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr info = VisionDll.imaqLearnColor(image._image, Roi.GetIntPtr(roi), sensitivity, saturation);
            Utilities.ThrowError(info);
            return Utilities.ConvertIntPtrToStructure<ColorInformation, CVI_ColorInformation>(info, true);
        }
        //==========================================================================================
        /// <summary>
        /// Determines how closely colors in an image match colors in the given color information. 
        /// </summary>
        /// <param name="image">
        /// The image containing colors you want to compare with the given color information. 
        /// </param>
        /// <param name="colorInformation">
        /// The color information. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColor" crefType="Unqualified"/> method
        /// to get the color information.
        /// </param>
        /// <returns>
        /// A collection of integers containing the match score for the image. The match score is represented on a 
        /// scale of 0 to 1000 where 0 indicates no match and 1000 indicates a perfect match.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>

        public static Collection<int> MatchColor(VisionImage image, ColorInformation colorInformation)
        {
            return MatchColor(image, colorInformation, null);
        }
        //==========================================================================================
        /// <summary>
        /// Determines how closely colors in an image match colors in the given color information. 
        /// </summary>
        /// <param name="image">
        /// The image containing colors you want to compare with the given color information. 
        /// </param>
        /// <param name="colorInformation">
        /// The color information. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnColor" crefType="Unqualified"/> method
        /// to get the color information.
        /// </param>
        /// <param name="roi">
        /// The region of the image in which to compare the colors. All region contours are considered to be external. 
        /// If <format type="italics">roi</format> contains multiple regions, the color information in each region is 
        /// compared individually to the color information specified by the <format type="italics">colorInformation</format> 
        /// parameter and the match results are reported for each region. Pass null or Nothing for this parameter to 
        /// compare colors in the entire image. 
        /// </param>
        /// <returns>
        /// A collection of integers containing the match score for the image. The match score is represented on a 
        /// scale of 0 to 1000 where 0 indicates no match and 1000 indicates a perfect match.
        /// </returns>
        /// <remarks>
        /// Use this method with Rgb32 and Hsl32 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Learn the image in viewer1 and match it in viewer2.
        /// Dim ColorInfo as ColorInformation = Algorithms.LearnColor (imageViewer1.Image)
        /// Dim ColorScore as Integer = Algorithms.MatchColor (imageViewer2.Image, ColorInfo)(0)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Learn the image in viewer1 and match it in viewer2.
        /// ColorInformation colorInfo = Algorithms.LearnColor(imageViewer1.Image);
        /// int colorScore = Algorithms.MatchColor(imageViewer2.Image, colorInfo)[0];
        /// </code>
        /// </example>

        public static Collection<int> MatchColor(VisionImage image, ColorInformation colorInformation, Roi roi)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (colorInformation == null) { throw new ArgumentNullException("colorInformation"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            CVI_ColorInformation cviColorInfo = new CVI_ColorInformation();
            cviColorInfo.ConvertFromExternal(colorInformation);
            try
            {
                Int32 numScores;
                IntPtr scores = VisionDll.imaqMatchColor(image._image, ref cviColorInfo, Roi.GetIntPtr(roi), out numScores);
                Utilities.ThrowError(scores);
                return Utilities.ConvertIntPtrToCollection<Int32>(scores, numScores, true);
            }
            finally
            {
                cviColorInfo.Dispose();
            }
        }
        #endregion

        #region Meter functions
        //==========================================================================================
        /// <summary>
        /// Returns the arc information of a meter. The 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadMeter" crefType="Unqualified"/> method uses this 
        /// information to read a meter.
        /// </summary>
        /// <param name="color">
        /// Determines whether to find a light-colored needle on a dark background or a dark-colored needle on a light 
        /// background.
        /// </param>
        /// <param name="initialNeedle">
        /// A line contour representing the minimum possible position of the meter needle.
        /// </param>
        /// <param name="fullNeedle">
        /// A line contour representing the maximum position of the meter needle.
        /// </param>
        /// <returns>
        /// A MeterArc object describing the arc across which a meter sweeps.
        /// </returns>

        public static MeterArc GetMeterArc(MeterNeedleColor color, LineContour initialNeedle, LineContour fullNeedle)
        {
            if (initialNeedle == null) { throw new ArgumentNullException("initialNeedle"); }
            if (fullNeedle == null) { throw new ArgumentNullException("fullNeedle"); }
            using (Roi roi = new Roi(new Shape[] { initialNeedle, fullNeedle }))
            {
                return GetMeterArc(color, roi);
            }
        }
        //==========================================================================================
        /// <summary>
        /// Returns the arc information of a meter. The 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadMeter" crefType="Unqualified"/> method uses this 
        /// information to read a meter.
        /// </summary>
        /// <param name="color">
        /// Determines whether to find a light-colored needle on a dark background or a dark-colored needle on a light 
        /// background.
        /// </param>
        /// <param name="lines">
        /// A region consisting of two line contours, each drawn from the tip of the needle to its base. The first line 
        /// contour represents the minimum position of the needle, and the second line contour represents the maximum 
        /// position of the needle. 
        /// </param>
        /// <returns>
        /// A MeterArc object describing the arc across which a meter sweeps.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Get the meter arc from the Roi in Viewer1.
        /// Dim Arc As MeterArc = Algorithms.GetMeterArc (MeterNeedleColor.DarkOnLight, imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Get the meter arc from the Roi in Viewer1.
        /// MeterArc arc = Algorithms.GetMeterArc(MeterNeedleColor.DarkOnLight, imageViewer1.Image);
        /// </code>
        /// </example>

        public static MeterArc GetMeterArc(MeterNeedleColor color, Roi lines)
        {
            if (lines == null) { throw new ArgumentNullException("lines"); }
            lines.ThrowIfDisposed();
            IntPtr arc = VisionDll.imaqGetMeterArc(color == MeterNeedleColor.LightOnDark ? 1 : 0, CVI_MeterArcMode.Roi, lines._roi, new CVI_PointFloat(), new CVI_PointFloat(), new CVI_PointFloat());
            Utilities.ThrowError(arc);
            MeterArc toReturn = new MeterArc();
            toReturn.MeterArcPtr = arc;
            return toReturn;
        }
        //==========================================================================================
        /// <summary>
        /// Returns the arc information of a meter. The 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadMeter" crefType="Unqualified"/> method uses this 
        /// information to read a meter.
        /// </summary>
        /// <param name="color">
        /// Determines whether to find a light-colored needle on a dark background or a dark-colored needle on a light 
        /// background.
        /// </param>
        /// <param name="basePoint">
        /// The location of the base of the needle.
        /// </param>
        /// <param name="startPoint">
        /// The location of the tip of the needle when the needle is at the minimum sweep position.
        /// </param>
        /// <param name="endPoint">
        /// The location of the tip of the needle when the needle is at the maximum sweep position.
        /// </param>
        /// <returns>
        /// A MeterArc object describing the arc across which a meter sweeps.
        /// </returns>

        public static MeterArc GetMeterArc(MeterNeedleColor color, PointContour basePoint, PointContour startPoint, PointContour endPoint)
        {
            if (basePoint == null) { throw new ArgumentNullException("basePoint"); }
            if (startPoint == null) { throw new ArgumentNullException("startPoint"); }
            if (endPoint == null) { throw new ArgumentNullException("endPoint"); }
            CVI_PointFloat cviBase = new CVI_PointFloat();
            cviBase.ConvertFromExternal(basePoint);
            CVI_PointFloat cviStart = new CVI_PointFloat();
            cviStart.ConvertFromExternal(startPoint);
            CVI_PointFloat cviEnd = new CVI_PointFloat();
            cviEnd.ConvertFromExternal(endPoint);
            IntPtr arc = VisionDll.imaqGetMeterArc(color == MeterNeedleColor.LightOnDark ? 1 : 0, CVI_MeterArcMode.Points, IntPtr.Zero, cviBase, cviStart, cviEnd);
            Utilities.ThrowError(arc);
            MeterArc toReturn = new MeterArc();
            toReturn.MeterArcPtr = arc;
            return toReturn;
        }
        //==========================================================================================
        /// <summary>
        /// Reads a meter. The method reads VU-meters with a dark needle on a light background or with a 
        /// light needle on a dark background. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.GetMeterArc" crefType="Unqualified"/> method to 
        /// determine the arc information.
        /// </summary>
        /// <param name="image">
        /// The image containing the meter to read.
        /// </param>
        /// <param name="arc">
        /// Information about the arc of the meter. This information is returned by the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.GetMeterArc" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.MeterReport" crefType="Unqualified"/> object containing the location of the endpoint 
        /// of the needle and the sweep position of the needle.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim Arc As MeterArc
        ///  
        /// 'Assumes the Arc has been populated with GetMeterArc.
        /// Dim Report As MeterReport = Algorithms.ReadMeter (imageViewer1.Image, Arc)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// MeterArc arc;
        ///  
        /// // Assumes the arc has been populated with GetMeterArc.
        /// MeterReport report = Algorithms.ReadMeter(imageViewer1.Image, arc);
        /// </code>
        /// </example>

        public static MeterReport ReadMeter(VisionImage image, MeterArc arc)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (arc == null) { throw new ArgumentNullException("arc"); }
            arc.ThrowIfDisposed();
            double percentage;
            CVI_PointFloat endOfNeedle = new CVI_PointFloat();
            Utilities.ThrowError(VisionDll.imaqReadMeter(image._image, arc.MeterArcPtr, out percentage, out endOfNeedle));
            MeterReport report = new MeterReport();
            report.Percentage = percentage;
            report.EndOfNeedle = endOfNeedle.ConvertToExternal();
            return report;
        }
        #endregion

        #region Barcode I/O functions
        //==========================================================================================
        /// <summary>
        /// Reads a barcode from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the barcode to read.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.BarcodeReport" crefType="Unqualified"/> object containing information about the barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static BarcodeReport ReadBarcode(VisionImage image)
        {
            return ReadBarcode(image, BarcodeTypes.Ean13, null, false);
        }
        //==========================================================================================
        /// <summary>
        /// Reads a barcode from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the barcode to read.
        /// </param>
        /// <param name="type">
        /// The type of the barcode to read.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.BarcodeReport" crefType="Unqualified"/> object containing information about the barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static BarcodeReport ReadBarcode(VisionImage image, BarcodeTypes type)
        {
            return ReadBarcode(image, type, null, false);
        }
        //==========================================================================================
        /// <summary>
        /// Reads a barcode from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the barcode to read.
        /// </param>
        /// <param name="type">
        /// The type of the barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.BarcodeReport" crefType="Unqualified"/> object containing information about the barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static BarcodeReport ReadBarcode(VisionImage image, BarcodeTypes type, Roi roi)
        {
            return ReadBarcode(image, type, roi, false);
        }
        //==========================================================================================
        /// <summary>
        /// Reads a barcode from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the barcode to read.
        /// </param>
        /// <param name="type">
        /// The type of the barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image.
        /// </param>
        /// <param name="validate">
        /// Specifies whether to validate the barcode's data. If the barcode type is Codabar, Code 39, or Interleaved 2 of 5, the error 
        /// correction information is used to validate the results. For all other barcode types, either no validation is performed or 
        /// the validation is performed automatically because the type requires it. The default is False.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.BarcodeReport" crefType="Unqualified"/> object containing information about the barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Read the Ean8 barcode on the viewer.
        /// Dim Report As BarcodeReport = Algorithms.ReadBarcode (imageViewer1.Image, BarcodeTypes.Ean8, imageViewer1.Roi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Read the Ean8 barcode on the viewer.
        /// BarcodeReport report = Algorithms.ReadBarcode(imageViewer1.Image, BarcodeTypes.Ean8, imageViewer1.Roi);
        ///  
        /// </code>
        /// </example>

        public static BarcodeReport ReadBarcode(VisionImage image, BarcodeTypes type, Roi roi, bool validate)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            IntPtr report = VisionDll.imaqReadBarcode(image._image, type, cviRoi, validate ? 1 : 0);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<BarcodeReport, CVI_BarcodeInfo>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Reads PDF417 barcodes from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the PDF417 barcode to read.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.Pdf417Report" crefType="Unqualified"/> object containing information about the 
        /// PDF417 barcodes, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<Pdf417Report> ReadPdf417Barcode(VisionImage image)
        {
            return ReadPdf417Barcode(image, null, Pdf417SearchMode.Single);
        }
        //==========================================================================================
        /// <summary>
        /// Reads PDF417 barcodes from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the PDF417 barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcodes in the image. The first contour of roi must be a rectangle, 
        /// rotated rectangle, oval, annulus or closed contour. Pass null or Nothing for this parameter to use the entire image. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.Pdf417Report" crefType="Unqualified"/> object containing information about the 
        /// PDF417 barcodes, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<Pdf417Report> ReadPdf417Barcode(VisionImage image, Roi roi)
        {
            return ReadPdf417Barcode(image, roi, Pdf417SearchMode.Single);
        }
        //==========================================================================================
        /// <summary>
        /// Reads PDF417 barcodes from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the PDF417 barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcodes in the image. The first contour of roi must be a rectangle, 
        /// rotated rectangle, oval, annulus or closed contour. Pass null or Nothing for this parameter to use the entire image. 
        /// </param>
        /// <param name="searchMode">
        /// Specifies the mode the method uses to search for barcodes.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.Pdf417Report" crefType="Unqualified"/> object containing information about the 
        /// PDF417 barcodes, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Reads the PDF417 barcode on the viewer.
        /// Dim Reports As Collection(Of Pdf417Report) = Algorithms.ReadPdf417Barcode (imageViewer1.Image, 
        /// imageViewer1.Roi, Pdf417SearchMode.Single)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Reads the PDF417 barcode on the viewer.
        /// Collection&lt;Pdf417Report&gt; reports = Algorithms.ReadPdf417Barcode(imageViewer1.Image, 
        /// imageViewer1.Roi, Pdf417SearchMode.Single);
        ///  
        /// </code>
        /// </example>

        public static Collection<Pdf417Report> ReadPdf417Barcode(VisionImage image, Roi roi, Pdf417SearchMode searchMode)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            uint numBarcodes;
            IntPtr reports = VisionDll.imaqReadPDF417Barcode(image._image, cviRoi, searchMode, out numBarcodes);
            Utilities.ThrowError(reports);
            return Utilities.ConvertIntPtrToCollection<Pdf417Report, CVI_Barcode2DInfo>(reports, numBarcodes, true);
        }
        //==========================================================================================
        /// <summary>
        /// Reads a QR code from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the QR code to read.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.QRReport" crefType="Unqualified"/> object containing information about 
        /// the QR code, including a string containing the decoded code data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static QRReport ReadQRCode(VisionImage image)
        {
            return ReadQRCode(image, null, new QRDescriptionOptions(), new QRSizeOptions(), new QRSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a QR code from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the QR code to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the code in the image. The first contour of 
        /// <format type="italics">roi</format> must be a rectangle, rotated rectangle, or closed contour.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.QRReport" crefType="Unqualified"/> object containing information about 
        /// the QR code, including a string containing the decoded code data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static QRReport ReadQRCode(VisionImage image, Roi roi)
        {
            return ReadQRCode(image, roi, new QRDescriptionOptions(), new QRSizeOptions(), new QRSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a QR code from an image. Many of the options provided by this method allow for automatic detection of 
        /// properties of the QR code or what methods the function should use to locate and decode the code. Specifying 
        /// specific properties and methods for these options will greatly increase the performance of the method.
        /// </summary>
        /// <param name="image">
        /// The image containing the QR code to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the code in the image. The first contour of 
        /// <format type="italics">roi</format> must be a rectangle, rotated rectangle, or closed contour.
        /// </param>
        /// <param name="descriptionOptions">
        /// Describes the QR code the method should look for.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.QRReport" crefType="Unqualified"/> object containing information about 
        /// the QR code, including a string containing the decoded code data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static QRReport ReadQRCode(VisionImage image, Roi roi, QRDescriptionOptions descriptionOptions)
        {
            return ReadQRCode(image, roi, descriptionOptions, new QRSizeOptions(), new QRSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a QR code from an image. Many of the options provided by this method allow for automatic detection of 
        /// properties of the QR code or what methods the function should use to locate and decode the code. Specifying 
        /// specific properties and methods for these options will greatly increase the performance of the method.
        /// </summary>
        /// <param name="image">
        /// The image containing the QR code to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the code in the image. The first contour of 
        /// <format type="italics">roi</format> must be a rectangle, rotated rectangle, or closed contour.
        /// </param>
        /// <param name="descriptionOptions">
        /// Describes the QR code the method should look for.
        /// </param>
        /// <param name="sizeOptions">
        /// Specifies the sizing information for the QR code the method should look for.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.QRReport" crefType="Unqualified"/> object containing information about 
        /// the QR code, including a string containing the decoded code data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static QRReport ReadQRCode(VisionImage image, Roi roi, QRDescriptionOptions descriptionOptions, QRSizeOptions sizeOptions)
        {
            return ReadQRCode(image, roi, descriptionOptions, sizeOptions, new QRSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a QR code from an image. Many of the options provided by this method allow for automatic detection of 
        /// properties of the QR code or what methods the function should use to locate and decode the code. Specifying 
        /// specific properties and methods for these options will greatly increase the performance of the method.
        /// </summary>
        /// <param name="image">
        /// The image containing the QR code to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the code in the image. The first contour of 
        /// <format type="italics">roi</format> must be a rectangle, rotated rectangle, or closed contour.
        /// </param>
        /// <param name="descriptionOptions">
        /// Describes the QR code the method should look for.
        /// </param>
        /// <param name="sizeOptions">
        /// Specifies the sizing information for the QR code the method should look for.
        /// </param>
        /// <param name="searchOptions">
        /// Specifies the parameters the method uses to search for the QR code.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.QRReport" crefType="Unqualified"/> object containing information about 
        /// the QR code, including a string containing the decoded code data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Reads the QR code on the viewer.
        /// 'Assumes the QR code is 141x141, black on white, and not mirrored.
        /// Dim DescriptionOptions As New QRDescriptionOptions(QRDimension.Size141x141, QRPolarity.BlackOnWhite, QRMirrorMode.Normal)
        /// Dim Report As QRReport = Algorithms.ReadQRCode (imageViewer1.Image, imageViewer1.Roi, DescriptionOptions)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Reads the QR code on the viewer.
        /// // Assumes the QR code is 141x141, black on white, and not mirrored.
        /// QRDescriptionOptions descriptionOptions = new QRDescriptionOptions(QRDimension.Size141x141, QRPolarity.BlackOnWhite, QRMirrorMode.Normal);
        /// QRReport report = Algorithms.ReadQRCode(imageViewer1.Image, imageViewer1.Roi, descriptionOptions);
        /// </code>
        /// </example>

        public static QRReport ReadQRCode(VisionImage image, Roi roi, QRDescriptionOptions descriptionOptions, QRSizeOptions sizeOptions, QRSearchOptions searchOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (descriptionOptions == null) { throw new ArgumentNullException("descriptionOptions"); }
            if (sizeOptions == null) { throw new ArgumentNullException("sizeOptions"); }
            if (searchOptions == null) { throw new ArgumentNullException("searchOptions"); }
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            CVI_QRCodeDescriptionOptions cviDescriptionOptions = new CVI_QRCodeDescriptionOptions();
            cviDescriptionOptions.ConvertFromExternal(descriptionOptions);
            CVI_QRCodeSizeOptions cviSizeOptions = new CVI_QRCodeSizeOptions();
            cviSizeOptions.ConvertFromExternal(sizeOptions);
            CVI_QRCodeSearchOptions cviSearchOptions = new CVI_QRCodeSearchOptions();
            cviSearchOptions.ConvertFromExternal(searchOptions);
            // Here we pass 0 for the QRGradingMode
            IntPtr report = VisionDll.imaqReadQRCode(image._image, cviRoi, 0, ref cviDescriptionOptions, ref cviSizeOptions, ref cviSearchOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<QRReport, CVI_QRCodeReport>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Reads a Data Matrix barcode from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to read.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.DataMatrixReport" crefType="Unqualified"/> object containing information 
        /// about the Data Matrix barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static DataMatrixReport ReadDataMatrixBarcode(VisionImage image)
        {
            return ReadDataMatrixBarcode(image, null, DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a Data Matrix barcode from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.DataMatrixReport" crefType="Unqualified"/> object containing information 
        /// about the Data Matrix barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static DataMatrixReport ReadDataMatrixBarcode(VisionImage image, Roi roi)
        {
            return ReadDataMatrixBarcode(image, roi, DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a Data Matrix barcode from an image.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image. 
        /// </param>
        /// <param name="gradingMode">
        /// Specifies if the method should make calculations needed to prepare to grade the Data Matrix barcode.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.DataMatrixReport" crefType="Unqualified"/> object containing information 
        /// about the Data Matrix barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static DataMatrixReport ReadDataMatrixBarcode(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode)
        {
            return ReadDataMatrixBarcode(image, roi, gradingMode, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a Data Matrix barcode from an image. Many of the options provided by this method allow for automatic detection 
        /// of properties of the Data Matrix barcode or what methods the function should use to locate and decode the barcode. 
        /// Specificying specific properties and methods for these options will greatly increase the performance of the method.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image. 
        /// </param>
        /// <param name="gradingMode">
        /// Specifies if the method should make calculations needed to prepare to grade the Data Matrix barcode.
        /// </param>
        /// <param name="descriptionOptions">
        /// Describes the Data Matrix barcode the method should look for.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.DataMatrixReport" crefType="Unqualified"/> object containing information 
        /// about the Data Matrix barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static DataMatrixReport ReadDataMatrixBarcode(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions)
        {
            return ReadDataMatrixBarcode(image, roi, gradingMode, descriptionOptions, new DataMatrixSizeOptions(), new DataMatrixSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a Data Matrix barcode from an image. Many of the options provided by this method allow for automatic detection 
        /// of properties of the Data Matrix barcode or what methods the function should use to locate and decode the barcode. 
        /// Specificying specific properties and methods for these options will greatly increase the performance of the method.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image. 
        /// </param>
        /// <param name="gradingMode">
        /// Specifies if the method should make calculations needed to prepare to grade the Data Matrix barcode.
        /// </param>
        /// <param name="descriptionOptions">
        /// Describes the Data Matrix barcode the method should look for.
        /// </param>
        /// <param name="sizeOptions">
        /// Specifies the sizing information for the Data Matrix barcode the method should look for.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.DataMatrixReport" crefType="Unqualified"/> object containing information 
        /// about the Data Matrix barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static DataMatrixReport ReadDataMatrixBarcode(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, DataMatrixSizeOptions sizeOptions)
        {
            return ReadDataMatrixBarcode(image, roi, gradingMode, descriptionOptions, sizeOptions, new DataMatrixSearchOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads a Data Matrix barcode from an image. Many of the options provided by this method allow for automatic detection 
        /// of properties of the Data Matrix barcode or what methods the function should use to locate and decode the barcode. 
        /// Specificying specific properties and methods for these options will greatly increase the performance of the method.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image. 
        /// </param>
        /// <param name="gradingMode">
        /// Specifies if the method should make calculations needed to prepare to grade the Data Matrix barcode.
        /// </param>
        /// <param name="descriptionOptions">
        /// Describes the Data Matrix barcode the method should look for.
        /// </param>
        /// <param name="sizeOptions">
        /// Specifies the sizing information for the Data Matrix barcode the method should look for.
        /// </param>
        /// <param name="searchOptions">
        /// Specifies the parameters the method uses to search for the Data Matrix barcode.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.DataMatrixReport" crefType="Unqualified"/> object containing information 
        /// about the Data Matrix barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Read the Data Matrix barcode on the viewer.
        /// 'This assumes the barcode is 19 rows by 17 columns.
        /// Dim Report As DataMatrixReport = Algorithms.ReadDataMatrixBarcode (imageViewer1.Image, 
        /// imageViewer1.Roi, DataMatrixGradingMode.None, New DataMatrixDescriptionOptions(19, 17))
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Read the Data Matrix barcode on the viewer.
        /// // This assumes the barcode is 19 rows by 17 columns.
        /// DataMatrixReport report = Algorithms.ReadDataMatrixBarcode(imageViewer1.Image, imageViewer1.Roi, 
        /// DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(19, 17));
        /// </code>
        /// </example>

        public static DataMatrixReport ReadDataMatrixBarcode(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, DataMatrixSizeOptions sizeOptions, DataMatrixSearchOptions searchOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (descriptionOptions == null) { throw new ArgumentNullException("descriptionOptions"); }
            if (sizeOptions == null) { throw new ArgumentNullException("sizeOptions"); }
            if (searchOptions == null) { throw new ArgumentNullException("searchOptions"); }
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            CVI_DataMatrixDescriptionOptions cviDescriptionOptions = new CVI_DataMatrixDescriptionOptions();
            cviDescriptionOptions.ConvertFromExternal(descriptionOptions);
            CVI_DataMatrixSizeOptions cviSizeOptions = new CVI_DataMatrixSizeOptions();
            cviSizeOptions.ConvertFromExternal(sizeOptions);
            CVI_DataMatrixSearchOptions cviSearchOptions = new CVI_DataMatrixSearchOptions();
            cviSearchOptions.ConvertFromExternal(searchOptions);
            IntPtr report = VisionDll.imaqReadDataMatrixBarcode2(image._image, cviRoi, gradingMode, ref cviDescriptionOptions, ref cviSizeOptions, ref cviSearchOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<DataMatrixReport, CVI_DataMatrixReport>(report, true);
        }

        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode3(VisionImage image) 
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode3(image, null, DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions(), options);
        }

        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode3(VisionImage image, Roi roi)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode3(image, roi, DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions(),options);
        }

        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode3(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode3(image, roi, gradingMode, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions(), options);
        }

        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode3(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode3(image, roi, gradingMode, descriptionOptions, new DataMatrixSizeOptions(), new DataMatrixSearchOptions(), options);
        }
        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode3(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, DataMatrixSizeOptions sizeOptions)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode3(image, roi, gradingMode, descriptionOptions, sizeOptions, new DataMatrixSearchOptions(),options);
        }
        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode3(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, DataMatrixSizeOptions sizeOptions, DataMatrixSearchOptions searchOptions)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode3(image, roi, gradingMode, descriptionOptions, sizeOptions, searchOptions, options);
        }
        //==========================================================================================
        /// <summary>
        /// Reads a Data Matrix barcode from an image. Many of the options provided by this method allow for automatic detection 
        /// of properties of the Data Matrix barcode or what methods the function should use to locate and decode the barcode. 
        /// Specificying specific properties and methods for these options will greatly increase the performance of the method.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image. 
        /// </param>
        /// <param name="gradingMode">
        /// Specifies if the method should make calculations needed to prepare to grade the Data Matrix barcode.
        /// </param>
        /// <param name="descriptionOptions">
        /// Describes the Data Matrix barcode the method should look for.
        /// </param>
        /// <param name="sizeOptions">
        /// Specifies the sizing information for the Data Matrix barcode the method should look for.
        /// </param>
        /// <param name="searchOptions">
        /// Specifies the parameters the method uses to search for the Data Matrix barcode.
        /// </param>
        /// <param name="advancedOptions">
        /// Specifies advanced options.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.DataMatrixReport" crefType="Unqualified"/> object containing information 
        /// about the Data Matrix barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Read the Data Matrix barcode on the viewer.
        /// 'This assumes the barcode is 19 rows by 17 columns.
        /// Dim Report As DataMatrixReport = Algorithms.ReadDataMatrixBarcode (imageViewer1.Image, 
        /// imageViewer1.Roi, DataMatrixGradingMode.None, New DataMatrixDescriptionOptions(19, 17))
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Read the Data Matrix barcode on the viewer.
        /// // This assumes the barcode is 19 rows by 17 columns.
        /// DataMatrixReport report = Algorithms.ReadDataMatrixBarcode3(imageViewer1.Image, imageViewer1.Roi, 
        /// DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(19, 17));
        /// </code>
        /// </example>

        public static DataMatrixReport ReadDataMatrixBarcode3(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, DataMatrixSizeOptions sizeOptions, DataMatrixSearchOptions searchOptions, Collection<DataMatrixAdvancedOptions> advancedOptions)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (descriptionOptions == null) { throw new ArgumentNullException("descriptionOptions"); }
            if (sizeOptions == null) { throw new ArgumentNullException("sizeOptions"); }
            if (searchOptions == null) { throw new ArgumentNullException("searchOptions"); }
            if (advancedOptions == null) { throw new ArgumentNullException("advancedOptions"); }
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            CVI_DataMatrixDescriptionOptions cviDescriptionOptions = new CVI_DataMatrixDescriptionOptions();
            cviDescriptionOptions.ConvertFromExternal(descriptionOptions);
            CVI_DataMatrixSizeOptions cviSizeOptions = new CVI_DataMatrixSizeOptions();
            cviSizeOptions.ConvertFromExternal(sizeOptions);
            CVI_DataMatrixSearchOptions cviSearchOptions = new CVI_DataMatrixSearchOptions();
            cviSearchOptions.ConvertFromExternal(searchOptions);

            //converting the collection to the IntPtr
            IntPtr cviDataMatrixAdvancedOptions = Utilities.ConvertCollectionToIntPtr<DataMatrixAdvancedOptions, CVI_DataMatrixAdvancedOptions>(advancedOptions);
            IntPtr report = VisionDll.imaqReadDataMatrixBarcode3(image._image, cviRoi, gradingMode, ref cviDescriptionOptions, ref cviSizeOptions, ref cviSearchOptions, cviDataMatrixAdvancedOptions, advancedOptions.Count);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<DataMatrixReport, CVI_DataMatrixReport>(report, true);
        }

        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode4(VisionImage image, ref float meanLight)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode4(image, null, DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions(), options, ref meanLight);
        }

        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode4(VisionImage image, Roi roi, ref float meanLight)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode4(image, roi, DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions(), options, ref meanLight);
        }

        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode4(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, ref float meanLight)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode4(image, roi, gradingMode, new DataMatrixDescriptionOptions(), new DataMatrixSizeOptions(), new DataMatrixSearchOptions(), options, ref meanLight);
        }

        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode4(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, ref float meanLight)
        {
            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode4(image, roi, gradingMode, descriptionOptions, new DataMatrixSizeOptions(), new DataMatrixSearchOptions(), options, ref meanLight);
        }
        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode4(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, DataMatrixSizeOptions sizeOptions, ref float meanLight)
        {

            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode4(image, roi, gradingMode, descriptionOptions, sizeOptions, new DataMatrixSearchOptions(), options, ref meanLight);
        }
        //==========================================================================================
        public static DataMatrixReport ReadDataMatrixBarcode4(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, DataMatrixSizeOptions sizeOptions, DataMatrixSearchOptions searchOptions, ref float meanLight)
        {

            Collection<DataMatrixAdvancedOptions> options = new Collection<DataMatrixAdvancedOptions>();
            options.Add(new DataMatrixAdvancedOptions(DataMatrixAdvancedProcessing.EnableFNC1, 1.0));
            return ReadDataMatrixBarcode4(image, roi, gradingMode, descriptionOptions, sizeOptions, searchOptions, options, ref meanLight);
        }
        //==========================================================================================
        /// <summary>
        /// Reads a Data Matrix barcode from an image. Many of the options provided by this method allow for automatic detection 
        /// of properties of the Data Matrix barcode or what methods the function should use to locate and decode the barcode. 
        /// Specificying specific properties and methods for these options will greatly increase the performance of the method.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to read.
        /// </param>
        /// <param name="roi">
        /// A region of interest specifying the location of the barcode in the image. 
        /// </param>
        /// <param name="gradingMode">
        /// Specifies if the method should make calculations needed to prepare to grade the Data Matrix barcode.
        /// </param>
        /// <param name="descriptionOptions">
        /// Describes the Data Matrix barcode the method should look for.
        /// </param>
        /// <param name="sizeOptions">
        /// Specifies the sizing information for the Data Matrix barcode the method should look for.
        /// </param>
        /// <param name="searchOptions">
        /// Specifies the parameters the method uses to search for the Data Matrix barcode.
        /// </param>
        /// <param name="advancedOptions">
        /// Specifies advanced options.
        /// </param>
        /// <param name="meanLight">
        /// Specifies value for mean light.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.DataMatrixReport" crefType="Unqualified"/> object containing information 
        /// about the Data Matrix barcode, including a string containing the decoded barcode data.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Read the Data Matrix barcode on the viewer.
        /// 'This assumes the barcode is 19 rows by 17 columns.
        /// Dim Report As DataMatrixReport = Algorithms.ReadDataMatrixBarcode (imageViewer1.Image, 
        /// imageViewer1.Roi, DataMatrixGradingMode.None, New DataMatrixDescriptionOptions(19, 17))
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Read the Data Matrix barcode on the viewer.
        /// // This assumes the barcode is 19 rows by 17 columns.
        /// DataMatrixReport report = Algorithms.ReadDataMatrixBarcode4(imageViewer1.Image, imageViewer1.Roi, 
        /// DataMatrixGradingMode.None, new DataMatrixDescriptionOptions(19, 17));
        /// </code>
        /// </example>

        public static DataMatrixReport ReadDataMatrixBarcode4(VisionImage image, Roi roi, DataMatrixGradingMode gradingMode, DataMatrixDescriptionOptions descriptionOptions, DataMatrixSizeOptions sizeOptions, DataMatrixSearchOptions searchOptions, Collection<DataMatrixAdvancedOptions> advancedOptions, ref float meanLight)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (descriptionOptions == null) { throw new ArgumentNullException("descriptionOptions"); }
            if (sizeOptions == null) { throw new ArgumentNullException("sizeOptions"); }
            if (searchOptions == null) { throw new ArgumentNullException("searchOptions"); }
            if (advancedOptions == null) { throw new ArgumentNullException("advancedOptions"); }
            IntPtr cviRoi = Roi.GetIntPtr(roi);
            meanLight = 0.0f ;
            CVI_DataMatrixDescriptionOptions cviDescriptionOptions = new CVI_DataMatrixDescriptionOptions();
            cviDescriptionOptions.ConvertFromExternal(descriptionOptions);
            CVI_DataMatrixSizeOptions cviSizeOptions = new CVI_DataMatrixSizeOptions();
            cviSizeOptions.ConvertFromExternal(sizeOptions);
            CVI_DataMatrixSearchOptions cviSearchOptions = new CVI_DataMatrixSearchOptions();
            cviSearchOptions.ConvertFromExternal(searchOptions);

            //converting the collection to the IntPtr
            IntPtr cviDataMatrixAdvancedOptions = Utilities.ConvertCollectionToIntPtr<DataMatrixAdvancedOptions, CVI_DataMatrixAdvancedOptions>(advancedOptions);
            IntPtr report = VisionDll.imaqReadDataMatrixBarcode4(image._image, cviRoi, gradingMode, ref cviDescriptionOptions, ref cviSizeOptions, ref cviSearchOptions, cviDataMatrixAdvancedOptions,advancedOptions.Count, ref meanLight);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<DataMatrixReport, CVI_DataMatrixReport>(report, true);
        }
  
        //==========================================================================================
        /// <summary>
        /// Grades a Data Matrix barcode using the AIM Print Quality metrics (included in the ISO 16022 specification).
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to grade. You must first prepare this image for grading using the
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadDataMatrixBarcode" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.AimGradeReport" crefType="Unqualified"/> object containing the AIM 
        /// standard grades for the Data Matrix barcode and the raw scores used to derive the grades. If a Data Matrix 
        /// barcode can not be located by the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadDataMatrixBarcode" crefType="Unqualified"/> method, the 
        /// method will assign the barcode an AimGrade of 
        /// <see cref="NationalInstruments.Vision.Analysis.AimGrade.F" crefType="Unqualified"/> for all grades and 0 for all raw 
        /// scores.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Assumes the image in Viewer1 has already been read with
        /// 'Algorithms.ReadDataMatrixBarcode
        /// Dim GradeReport As AimGradeReport
        /// GradeReport = Algorithms.GradeDataMatrixBarcodeAim (imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Assumes the image in Viewer1 has already been read with
        /// // Algorithms.ReadDataMatrixBarcode
        /// AimGradeReport gradeReport;
        /// gradeReport = Algorithms.GradeDataMatrixBarcodeAim(imageViewer1.Image);
        /// </code>
        /// </example>

        public static AimGradeReport GradeDataMatrixBarcodeAim(VisionImage image)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            CVI_AIMGradeReport cviReport = new CVI_AIMGradeReport();
            Utilities.ThrowError(VisionDll.imaqGradeDataMatrixBarcodeAIM(image._image, out cviReport));
            return cviReport.ConvertToExternal();
        }

        //==========================================================================================
        public static AimDpmGradeReport GradeDataMatrixBarcodeAimDpm(VisionImage image)
        {
            return GradeDataMatrixBarcodeAimDpm(image, new CalibReflectanceStruct());
        }

        //==========================================================================================
        /// <summary>
        /// Grades a Data Matrix barcode using the AIM Print Quality metrics included in the AIM DPM specification.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to grade. You must first prepare this image for grading using the
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadDataMatrixBarcode" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.AimDpmGradeReport" crefType="Unqualified"/> object containing the AIM 
        /// standard grades for the Data Matrix barcode and the raw scores used to derive the grades. If a Data Matrix 
        /// barcode can not be located by the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadDataMatrixBarcode" crefType="Unqualified"/> method, the 
        /// method will assign the barcode an AimGrade of 
        /// <see cref="NationalInstruments.Vision.Analysis.AimGrade.F" crefType="Unqualified"/> for all grades and 0 for all raw 
        /// scores.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Assumes the image in Viewer1 has already been read with
        /// 'Algorithms.ReadDataMatrixBarcode
        /// Dim GradeReport As AimDpmGradeReport
        /// GradeReport = Algorithms.GradeDataMatrixBarcodeAIMDPM (imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Assumes the image in Viewer1 has already been read with
        /// // Algorithms.ReadDataMatrixBarcode
        /// AimDpmGradeReport gradeReport;
        /// gradeReport = Algorithms.GradeDataMatrixBarcodeAIMDPM(imageViewer1.Image, CalibReflectanceStruct);
        /// </code>
        /// </example>

        public static AimDpmGradeReport GradeDataMatrixBarcodeAimDpm(VisionImage image, CalibReflectanceStruct calibReflectance)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            CVI_AIMDPMGradeReport cviReport = new CVI_AIMDPMGradeReport();
            CVI_CalibReflectanceStruct cvicalibReflectance = new CVI_CalibReflectanceStruct();
            cvicalibReflectance.ConvertFromExternal(calibReflectance);
            Utilities.ThrowError(VisionDll.imaqGradeDataMatrixBarcodeAIMDPM(image._image, ref cvicalibReflectance, out cviReport));
            return cviReport.ConvertToExternal();
        }

        /// <summary>
        /// Grades a Data Matrix barcode using the Print Quality metrics included in the ISO 15415 specification.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to grade. You must first prepare this image for grading using the
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadDataMatrixBarcode" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.GradeReportISO15415" crefType="Unqualified"/> object containing the AIM 
        /// standard grades for the Data Matrix barcode and the raw scores used to derive the grades. If a Data Matrix 
        /// barcode can not be located by the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadDataMatrixBarcode" crefType="Unqualified"/> method, the 
        /// method will assign the barcode an AimGrade of 
        /// <see cref="NationalInstruments.Vision.Analysis.AimGrade.F" crefType="Unqualified"/> for all grades and 0 for all raw 
        /// scores.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Assumes the image in Viewer1 has already been read with
        /// 'Algorithms.ReadDataMatrixBarcode
        /// Dim GradeReport As GradeReportISO15415
        /// GradeReport = Algorithms.GradeDataMatrixBarcodeISO15415 (imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Assumes the image in Viewer1 has already been read with
        /// // Algorithms.ReadDataMatrixBarcode
        /// GradeReportISO15415 gradeReport;
        /// gradeReport = Algorithms.GradeDataMatrixBarcodeISO15415(imageViewer1.Image);
        /// </code>
        /// </example>

        public static GradeReportISO15415 GradeDataMatrixBarcodeISO15415(VisionImage image)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            CVI_GradeReportISO15415 cviReport = new CVI_GradeReportISO15415();
            Utilities.ThrowError(VisionDll.imaqGradeDataMatrixBarcodeISO15415(image._image, out cviReport));
            return cviReport.ConvertToExternal();
        }

        /// <summary>
        /// Grades a Data Matrix barcode using the Print Quality metrics included in the ISO 16022 specification.
        /// </summary>
        /// <param name="image">
        /// The image containing the Data Matrix barcode to grade. You must first prepare this image for grading using the
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadDataMatrixBarcode" crefType="Unqualified"/> method.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.GradeReportISO16022" crefType="Unqualified"/> object containing the AIM 
        /// standard grades for the Data Matrix barcode and the raw scores used to derive the grades. If a Data Matrix 
        /// barcode can not be located by the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadDataMatrixBarcode" crefType="Unqualified"/> method, the 
        /// method will assign the barcode an AimGrade of 
        /// <see cref="NationalInstruments.Vision.Analysis.AimGrade.F" crefType="Unqualified"/> for all grades and 0 for all raw 
        /// scores.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Assumes the image in Viewer1 has already been read with
        /// 'Algorithms.ReadDataMatrixBarcode
        /// Dim GradeReport As GradeReportISO16022
        /// GradeReport = Algorithms.GradeDataMatrixBarcodeISO16022 (imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Assumes the image in Viewer1 has already been read with
        /// // Algorithms.ReadDataMatrixBarcode
        /// GradeReportISO16022 gradeReport;
        /// gradeReport = Algorithms.GradeDataMatrixBarcodeISO16022(imageViewer1.Image);
        /// </code>
        /// </example>

        public static GradeReportISO16022 GradeDataMatrixBarcodeISO16022(VisionImage image)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            CVI_GradeReportISO16022 cviReport = new CVI_GradeReportISO16022();
            Utilities.ThrowError(VisionDll.imaqGradeDataMatrixBarcodeISO16022(image._image, out cviReport));
            return cviReport.ConvertToExternal();
        }
        #endregion

        #region Shape Matching functions
        //==========================================================================================
        /// <summary>
        /// Finds objects in an image whose shape matches the shape of the object specified by a template.
        /// </summary>
        /// <param name="source">
        /// The image in which the function searches for shapes. 
        /// </param>
        /// <param name="template">
        /// The 8-bit image containing the shape to find. 
        /// </param>
        /// <param name="destination">
        /// The resulting image that contains the objects in the source image that match the object in the 
        /// template image.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ShapeReport" crefType="Unqualified"/> object containing information
        /// about the matched objects.
        /// </returns>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">template</format> must be U8 binary 
        /// images that contain only pixel values of 0 or 1.
        /// <para>
        /// The matching process is invariant to rotation. You can set the match process to be invariant to the 
        /// scale of the objects. This method labels each object in the image with a unique identification number 
        /// before performing the match operation.
        /// </para>
        /// </remarks>

        public static Collection<ShapeReport> ShapeMatch(VisionImage source, VisionImage template, VisionImage destination)
        {
            return ShapeMatch(source, template, destination, true, Connectivity.Connectivity8, .03);
        }
        //==========================================================================================
        /// <summary>
        /// Finds objects in an image whose shape matches the shape of the object specified by a template.
        /// </summary>
        /// <param name="source">
        /// The image in which the function searches for shapes. 
        /// </param>
        /// <param name="template">
        /// The 8-bit image containing the shape to find. 
        /// </param>
        /// <param name="destination">
        /// The resulting image that contains the objects in the source image that match the object in the 
        /// template image.
        /// </param>
        /// <param name="scaleInvariant">
        /// Set this parameter to True to search for shapes regardless of size. Set this parameter to False to search 
        /// for shapes that are ?10 percent the same size as the template shape. The default is True.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ShapeReport" crefType="Unqualified"/> object containing information
        /// about the matched objects.
        /// </returns>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">template</format> must be U8 binary 
        /// images that contain only pixel values of 0 or 1.
        /// <para>
        /// The matching process is invariant to rotation. You can set the match process to be invariant to the 
        /// scale of the objects. This method labels each object in the image with a unique identification number 
        /// before performing the match operation.
        /// </para>
        /// </remarks>

        public static Collection<ShapeReport> ShapeMatch(VisionImage source, VisionImage template, VisionImage destination, bool scaleInvariant)
        {
            return ShapeMatch(source, template, destination, scaleInvariant, Connectivity.Connectivity8, .03);
        }
        //==========================================================================================
        /// <summary>
        /// Finds objects in an image whose shape matches the shape of the object specified by a template.
        /// </summary>
        /// <param name="source">
        /// The image in which the function searches for shapes. 
        /// </param>
        /// <param name="template">
        /// The 8-bit image containing the shape to find. 
        /// </param>
        /// <param name="destination">
        /// The resulting image that contains the objects in the source image that match the object in the 
        /// template image.
        /// </param>
        /// <param name="scaleInvariant">
        /// Set this parameter to True to search for shapes regardless of size. Set this parameter to False to search 
        /// for shapes that are ?10 percent the same size as the template shape. The default is True.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are 
        /// touching. Set this parameter to Connectivity4 to use connectivity-4 to determine whether particles 
        /// are touching. The default is Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ShapeReport" crefType="Unqualified"/> object containing information
        /// about the matched objects.
        /// </returns>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">template</format> must be U8 binary 
        /// images that contain only pixel values of 0 or 1.
        /// <para>
        /// The matching process is invariant to rotation. You can set the match process to be invariant to the 
        /// scale of the objects. This method labels each object in the image with a unique identification number 
        /// before performing the match operation.
        /// </para>
        /// </remarks>

        public static Collection<ShapeReport> ShapeMatch(VisionImage source, VisionImage template, VisionImage destination, bool scaleInvariant, Connectivity connectivity)
        {
            return ShapeMatch(source, template, destination, scaleInvariant, connectivity, .03);
        }
        //==========================================================================================
        /// <summary>
        /// Finds objects in an image whose shape matches the shape of the object specified by a template.
        /// </summary>
        /// <param name="source">
        /// The image in which the function searches for shapes. 
        /// </param>
        /// <param name="template">
        /// The 8-bit image containing the shape to find. 
        /// </param>
        /// <param name="destination">
        /// The resulting image that contains the objects in the source image that match the object in the 
        /// template image.
        /// </param>
        /// <param name="scaleInvariant">
        /// Set this parameter to True to search for shapes regardless of size. Set this parameter to False to search 
        /// for shapes that are ?10 percent the same size as the template shape. The default is True.
        /// </param>
        /// <param name="connectivity">
        /// Set this parameter to Connectivity8 to use connectivity-8 to determine whether particles are 
        /// touching. Set this parameter to Connectivity4 to use connectivity-4 to determine whether particles 
        /// are touching. The default is Connectivity8. For more information about connectivity, refer to the 
        /// <format type="italics">NI Vision Concepts Help</format>.
        /// </param>
        /// <param name="tolerance">
        /// Specifies the allowable difference between the template shape and similar shapes in the image. The 
        /// difference is expressed as a value from 0 to 1. The default is .03.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.ShapeReport" crefType="Unqualified"/> object containing information
        /// about the matched objects.
        /// </returns>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">template</format> must be U8 binary 
        /// images that contain only pixel values of 0 or 1.
        /// <para>
        /// The matching process is invariant to rotation. You can set the match process to be invariant to the 
        /// scale of the objects. This method labels each object in the image with a unique identification number 
        /// before performing the match operation.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Match the template shape stored in i in the image in Viewer1.
        /// 'Store the result in the image in Viewer2
        /// Dim i As New VisionImage
        ///  
        /// 'Convert the image in Viewer1 into a binary image
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255))
        ///  
        /// 'Convert i into a binary image
        /// Algorithms.Threshold (i, i, New Range(128, 255))
        ///  
        /// 'Perform the ShapeMatch
        /// Dim Reports As Collection(Of ShapeReport) = Algorithms.ShapeMatch (imageViewer1.Image, i, imageViewer2.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// //Match the template shape stored in i in the image in Viewer1.
        /// //Store the result in the image in Viewer2
        /// VisionImage i = new VisionImage();
        ///  
        /// //Convert the image in Viewer1 into a binary image
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255));
        ///  
        /// //Convert i into a binary image
        /// Algorithms.Threshold(i, i, new Range(128, 255));
        ///  
        /// //Perform the ShapeMatch
        /// Collection&lt;ShapeReport&gt; reports = Algorithms.ShapeMatch(imageViewer1.Image, i, imageViewer2.Image);
        /// </code>
        /// </example>

        public static Collection<ShapeReport> ShapeMatch(VisionImage source, VisionImage template, VisionImage destination, bool scaleInvariant, Connectivity connectivity, double tolerance)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            int numMatches;
            IntPtr reports = VisionDll.imaqMatchShape(destination._image, source._image, template._image, scaleInvariant ? 1 : 0, connectivity, tolerance, out numMatches);
            Utilities.ThrowError(reports);
            return Utilities.ConvertIntPtrToCollection<ShapeReport, CVI_ShapeReport>(reports, numMatches, true);
        }
        #endregion

        #region Image Analysis functions
        //==========================================================================================
        /// <summary>
        /// Computes the centroid of an image.
        /// </summary>
        /// <param name="image">
        /// The image whose centroid the method calculates.
        /// </param>
        /// <returns>
        /// The centroid of the image. On failure, an exception is thrown.</returns>
        /// <remarks>
        /// Use this method with image types U8, I16, and Sgl.
        /// </remarks>

        public static PointContour Centroid(VisionImage image)
        {
            return Centroid(image, null);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the centroid of an image.
        /// </summary>
        /// <param name="image">
        /// The image whose centroid the method calculates.
        /// </param>
        /// <param name="mask">
        /// Specifies the region in the image to use for computing the centroid. The method uses only those source image pixels whose corresponding mask pixels are non-zero to compute the centroid. Do not set this parameter if you want the method to use the whole image in the centroid calculation.
        /// </param>
        /// <returns>
        /// The centroid of the image. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with image types U8, I16, and Sgl. <format type="italics">mask</format> must be a U8 image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim centroid As PointContour
        /// Dim maskImage As New VisionImage
        /// 'Do something here to populate the maskImage
        /// 'Calculate the centroid of the Image in Viewer1 using
        /// 'maskImage as a mask.
        /// centroid = Algorithms.Centroid (imageViewer1.Image, maskImage)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// PointContour centroid;
        /// VisionImage maskImage = new VisionImage();
        /// //Do something here to populate the maskImage
        /// //Calculate the centroid of the Image in Viewer1 using
        /// //maskImage as a mask.
        /// centroid = Algorithms.Centroid(imageViewer1.Image, maskImage);
        /// 	</code>
        /// </example>

        public static PointContour Centroid(VisionImage image, VisionImage mask)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            CVI_PointFloat centroid = new CVI_PointFloat();
            Utilities.ThrowError(VisionDll.imaqCentroid(image._image, out centroid, VisionImage.GetIntPtr(mask)));
            return centroid.ConvertToExternal();
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel value distribution, of an image.
        /// </summary>
        /// <param name="image">
        /// The image the method uses to compute the histogram.
        /// </param>
        /// <returns>
        /// A HistogramReport describing the pixel value classification. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16, I16, and Single images.
        /// </remarks>

        public static HistogramReport Histogram(VisionImage image)
        {
            return Histogram(image, 256, new Range(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel value distribution, of an image.
        /// </summary>
        /// <param name="image">
        /// The image the method uses to compute the histogram.
        /// </param>
        /// <param name="classes">
        /// The number of classes into which the method separates the pixels. The default is 256.
        /// </param>
        /// <returns>
        /// A HistogramReport describing the pixel value classification. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16, I16, and Single images.
        /// </remarks>

        public static HistogramReport Histogram(VisionImage image, Int32 classes)
        {
            return Histogram(image, classes, new Range(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel value distribution, of an image.
        /// </summary>
        /// <param name="image">
        /// The image the method uses to compute the histogram.
        /// </param>
        /// <param name="classes">
        /// The number of classes into which the method separates the pixels. The default is 256.
        /// </param>
        /// <param name="intervalRange">
        /// The minimum and maximum values for the range of pixel values to consider for the histogram. 
        /// Pixel values outside of the range are not considered. The default value is (0, 0), which considers all pixels in the image.
        /// </param>
        /// <returns>
        /// A HistogramReport describing the pixel value classification. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16, I16, and Single images.
        /// </remarks>

        public static HistogramReport Histogram(VisionImage image, Int32 classes, Range intervalRange)
        {
            return Histogram(image, classes, intervalRange, null);
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the histogram, or pixel value distribution, of an image.
        /// </summary>
        /// <param name="image">
        /// The image the method uses to compute the histogram.
        /// </param>
        /// <param name="classes">
        /// The number of classes into which the method separates the pixels. The default is 256.
        /// </param>
        /// <param name="intervalRange">
        /// The minimum and maximum values for the range of pixel values to consider for the histogram. 
        /// Pixel values outside of the range are not considered. The default value is (0, 0), which considers all pixels in the image.
        /// </param>
        /// <param name="mask">
        /// the region in which the method computes the histogram. The method processes only those pixels in the image whose corresponding 
        /// pixels in the mask are non-zero. Pass null or Nothing for this parameter if you want to calculate the histogram for the entire 
        /// image.
        /// </param>
        /// <returns>
        /// A HistogramReport describing the pixel value classification. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16, I16, and Single images. <format type="italics">mask</format> must be a U8 image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim Report As HistogramReport
        /// Dim Mask As New VisionImage
        ///  
        /// 'Find the histogram of a portion of the image in Viewer1
        /// 'defined by the regions on Viewer1.
        /// Algorithms.RoiToMask (Mask, imageViewer1.Roi)
        ///  
        /// Report = Algorithms.Histogram (imageViewer1.Image, 256, New Range(), Mask)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// HistogramReport report;
        /// VisionImage mask = new VisionImage();
        ///  
        /// //Find the histogram of a portion of the image in Viewer1
        /// //defined by the regions on Viewer1.
        /// Algorithms.RoiToMask(mask, imageViewer1.Roi);
        ///  
        /// report = Algorithms.Histogram(imageViewer1.Image, 256, new Range(), mask);
        /// </code>
        /// </example>

        public static HistogramReport Histogram(VisionImage image, Int32 classes, Range intervalRange, VisionImage mask)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (intervalRange == null) { throw new ArgumentNullException("intervalRange"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            IntPtr result = VisionDll.imaqHistogram(image._image, classes, (float)intervalRange.Minimum, (float)intervalRange.Maximum, VisionImage.GetIntPtr(mask));
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToStructure<HistogramReport, CVI_HistogramReport>(result, true);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the profile of a line of pixels. This method returns other information such as pixel statistics and 
        /// the coordinates of the pixels along the line.
        /// </summary>
        /// <param name="image">
        /// The image on which the method computes the line profile.
        /// </param>
        /// <param name="line">
        /// The ROI containing the line along which the method computes the profile.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.LineProfileReport" crefType="Unqualified"/> object containing information
        /// about the line.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static LineProfileReport LineProfile(VisionImage image, Roi line)
        {
            if (line == null) { throw new ArgumentNullException("line"); }
            line.ThrowIfDisposed();
            Utilities.ThrowIfNotSingleLine(line);
            return LineProfile(image, (LineContour)line[0].Shape);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the profile of a line of pixels. This method returns other information such as pixel statistics and 
        /// the coordinates of the pixels along the line.
        /// </summary>
        /// <param name="image">
        /// The image on which the method computes the line profile.
        /// </param>
        /// <param name="line">
        /// The line along which the method computes the profile.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.LineProfileReport" crefType="Unqualified"/> object containing information
        /// about the line.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This example requires you to have a ListBox named List1 on the form.
        /// Dim line As New LineContour (New PointContour (10, 10), New PointContour (100, 100))
        /// 'Find the profile along the line in the image on Viewer1
        /// Dim Report As LineProfileReport = Algorithms.LineProfile (imageViewer1.Image, line)
        /// 'Display the pixel values in the listbox
        /// If Report.ProfileData.Count &gt; 0 Then
        ///     List1.Items.Clear ()
        ///     For Each Val As Double In Report.ProfileData
        ///         List1.Items.Add (val)
        ///     Next
        /// End If
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // This example requires you to have a ListBox named List1 on the form.
        /// LineContour line = new LineContour(new PointContour(10, 10), new PointContour(100, 100));
        /// // Find the profile along the line in the image on Viewer1
        /// LineProfileReport report = Algorithms.LineProfile(imageViewer1.Image, line);
        /// // Display the pixel values in the listbox
        /// if (report.ProfileData.Count) {
        ///     List1.Items.Clear();
        ///     foreach (double val in Report.ProfileData) {
        ///         List1.Items.Add(val);
        ///     }
        /// }
        ///  
        /// </code>
        /// </example>

        public static LineProfileReport LineProfile(VisionImage image, LineContour line)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (line == null) { throw new ArgumentNullException("line"); }
            CVI_Point cviStart = new CVI_Point();
            CVI_Point cviEnd = new CVI_Point();
            cviStart.ConvertFromExternal(line.Start);
            cviEnd.ConvertFromExternal(line.End);
            IntPtr result = VisionDll.imaqLineProfile(image._image, cviStart, cviEnd);
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToStructure<LineProfileReport, CVI_LineProfile>(result, true);
        }
        //==========================================================================================
        /// <summary>
        /// Finds curves in an image.
        /// </summary>
        /// <param name="image">
        /// The source image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CurveReport" crefType="Unqualified"/> objects that describe the curves in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<CurveReport> ExtractCurves(VisionImage image)
        {
            return ExtractCurves(image, new CurveOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Finds curves in an image.
        /// </summary>
        /// <param name="image">
        /// The source image.
        /// </param>
        /// <param name="options">
        /// The parameters used when finding curves in an image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CurveReport" crefType="Unqualified"/> objects that describe the curves in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<CurveReport> ExtractCurves(VisionImage image, CurveOptions options)
        {
            return ExtractCurves(image, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Finds curves in an image.
        /// </summary>
        /// <param name="image">
        /// The source image.
        /// </param>
        /// <param name="options">
        /// The parameters used when finding curves in an image.
        /// </param>
        /// <param name="roi">
        /// The ROI in which the method finds curves. Pass null or Nothing for this parameter to seach the entire image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.CurveReport" crefType="Unqualified"/> objects that describe the curves in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim options As New CurveOptions
        ///  
        /// ' Find the curves in the image in Viewer1
        /// Algorithms.ExtractCurves (imageViewer1.Image, options)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// CurveOptions options = new CurveOptions();
        ///     
        /// // Find the curves in the image in Viewer1
        /// Algorithms.ExtractCurves(imageViewer1.Image, options);
        ///  
        /// </code>
        /// </example>

        public static Collection<CurveReport> ExtractCurves(VisionImage image, CurveOptions options, Roi roi)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            Roi.ThrowIfNonNullAndDisposed(roi);
            CVI_CurveOptions cviOptions = new CVI_CurveOptions();
            cviOptions.ConvertFromExternal(options);
            UInt32 numCurves;
            IntPtr report = VisionDll.imaqExtractCurves(image._image, Roi.GetIntPtr(roi), ref cviOptions, out numCurves);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToCollection<CurveReport, CVI_Curve>(report, numCurves, true);
        }
        //==========================================================================================
        /// <summary>
        /// Computes the mean line profile of an image.
        /// </summary>
        /// <param name="image">
        /// The image on which to perform the linear average.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.LinearAveragesReport" crefType="Unqualified"/> object containing the
        /// linear averages of the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static LinearAveragesReport LinearAverages(VisionImage image)
        {
            return LinearAverages(image, LinearAveragesModes.AllLinearAverages, RectangleContour.None);    
        }
        //==========================================================================================
        /// <summary>
        /// Computes the mean line profile of an image.
        /// </summary>
        /// <param name="image">
        /// The image on which to perform the linear average.
        /// </param>
        /// <param name="mode">
        /// The types of linear averages the function should compute. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.LinearAveragesReport" crefType="Unqualified"/> object containing the
        /// linear averages of the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static LinearAveragesReport LinearAverages(VisionImage image, LinearAveragesModes mode)
        {
            return LinearAverages(image, mode, RectangleContour.None);    
        }
        //==========================================================================================
        /// <summary>
        /// Computes the mean line profile of an image.
        /// </summary>
        /// <param name="image">
        /// The image on which to perform the linear average.
        /// </param>
        /// <param name="mode">
        /// The types of linear averages the function should compute. 
        /// </param>
        /// <param name="rectangle">
        /// Sets the region of interest in which the function calculates the averages.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.LinearAveragesReport" crefType="Unqualified"/> object containing the
        /// linear averages of the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>
        /// <returns>
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Compute the linear averages on the image in viewer1.
        /// 'Use the ROI that's on the viewer.
        /// Dim Report As LinearAveragesReport = Algorithms.LinearAverages (imageViewer1.Image, 
        ///  
        /// LinearAveragesModes.AllLinearAverages, imageViewer1.Roi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Compute the linear averages on the image in viewer1.
        /// // Use the ROI that's on the viewer.
        /// LinearAveragesReport report = Algorithms.LinearAverages(imageViewer1.Image, 
        ///  
        /// LinearAveragesModes.AllLinearAverages, imageViewer1.Roi)
        ///  
        /// </code>
        /// </example>

        public static LinearAveragesReport LinearAverages(VisionImage image, LinearAveragesModes mode, Roi rectangle)
        {
            Roi.ThrowIfNonNullAndDisposed(rectangle);
            return LinearAverages(image, mode, Utilities.ConvertRoiToRectangle(rectangle));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the mean line profile of an image.
        /// </summary>
        /// <param name="image">
        /// The image on which to perform the linear average.
        /// </param>
        /// <param name="mode">
        /// The types of linear averages the function should compute. 
        /// </param>
        /// <param name="rectangle">
        /// Sets the rectangular area in which the function calculates the averages.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.LinearAveragesReport" crefType="Unqualified"/> object containing the
        /// linear averages of the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static LinearAveragesReport LinearAverages(VisionImage image, LinearAveragesModes mode, RectangleContour rectangle)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (rectangle == null) { throw new ArgumentNullException("rectangle"); }
            CVI_Rectangle cviRect = new CVI_Rectangle();
            cviRect.ConvertFromExternal(rectangle);
            IntPtr report = VisionDll.imaqLinearAverages2(image._image, mode, cviRect);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<LinearAveragesReport, CVI_LinearAverages>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Quantifies the contents of an image or the regions within an image. The region definition is performed with a 
        /// labeled image mask. Each region of the mask has a single unique value.
        /// </summary>
        /// <param name="image">
        /// The image to quantify.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.QuantifyReport" crefType="Unqualified"/> conintaining information about the regions in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static QuantifyReport Quantify(VisionImage image)
        {
            return Quantify(image, null);
        }
        //==========================================================================================
        /// <summary>
        /// Quantifies the contents of an image or the regions within an image. The region definition is performed with a 
        /// labeled image mask. Each region of the mask has a single unique value.
        /// </summary>
        /// <param name="image">
        /// The image to quantify.
        /// </param>
        /// <param name="mask">
        /// An 8-bit or 16-bit image specifying the regions to quantify in the image. Only the pixels in the original image 
        /// that correspond to an equivalent pixel in the mask different from 0 are used for the quantification. Each pixel in 
        /// this image (mask) indicates, by its value, which region belongs to the corresponding pixel in Image. Up to 255 
        /// regions for a U8, or 65,535 regions for an I16, can be quantified directly from the Image. Pass null or Nothing for 
        /// this parameter to quantify the entire image.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.QuantifyReport" crefType="Unqualified"/> conintaining information about the regions in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim MaskImage As New VisionImage
        /// Dim Report As QuantifyReport
        /// 'If there are regions on Viewer1
        /// If imageViewer1.Roi.Count &gt; 0 Then
        ///     'Create the mask from the regions you selected on Viewer1
        ///     Algorithms.RoiToMask (MaskImage, imageViewer1.Roi)
        ///     'Label the mask image
        ///     Algorithms.Label (maskImage, maskImage)
        ///     'Quantify the image based on the regions
        ///     Report = Algorithms.Quantify (imageViewer1.Image, maskImage)
        /// Else
        ///     'Quantify the entire image
        ///     Report = Algorithms.Quantify (imageViewer1.Image)
        /// End If
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage maskImage = new VisionImage();
        /// QuantifyReport report;
        /// // If there are regions on Viewer1
        /// if (imageViewer1.Roi.Count &gt; 0) {
        ///     // Create the mask from the regions you selected on Viewer1
        ///     Algorithms.RoiToMask(maskImage, imageViewer1.Roi);
        ///     // Label the mask image
        ///     Algorithms.Label(maskImage, maskImage);
        ///     // Quantify the image based on the regions
        ///     report = Algorithms.Quantify(imageViewer1.Image, maskImage);
        /// } else {
        ///     // Quantify the entire image
        ///     report = Algorithms.Quantify(imageViewer1.Image);
        /// }
        /// </code>
        /// </example>

        public static QuantifyReport Quantify(VisionImage image, VisionImage mask)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            IntPtr report = VisionDll.imaqQuantify(image._image, VisionImage.GetIntPtr(mask));
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<QuantifyReport, CVI_QuantifyReport>(report, true);

        }
        #endregion

        #region Threshold functions
        //==========================================================================================
        /// <summary>
        /// Thresholds an image. The method sets pixels values outside of the given range to 0. The method sets 
        /// pixel values within the range to a given value or leaves the values unchanged.
        /// </summary>
        /// <param name="source">
        /// The image to threshold.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is optimized for MMX.
        /// </remarks>

        public static void Threshold(VisionImage source, VisionImage destination)
        {
            Threshold(source, destination, new Range(128, 255), true, 1);
        }
        //==========================================================================================
        /// <summary>
        /// Thresholds an image. The method sets pixels values outside of the given range to 0. The method sets 
        /// pixel values within the range to a given value or leaves the values unchanged.
        /// </summary>
        /// <param name="source">
        /// The image to threshold.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="range">
        /// The range of pixel values to keep. The default is (128,255).
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is optimized for MMX.
        /// </remarks>

        public static void Threshold(VisionImage source, VisionImage destination, Range range)
        {
            Threshold(source, destination, range, true, 1);
        }
        //==========================================================================================
        /// <summary>
        /// Thresholds an image. The method sets pixels values outside of the given range to 0. The method sets 
        /// pixel values within the range to a given value or leaves the values unchanged.
        /// </summary>
        /// <param name="source">
        /// The image to threshold.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="range">
        /// The range of pixel values to keep. The default is (128,255).
        /// </param>
        /// <param name="replaceValue">
        /// Set this parameter to True to set the pixel values within the <format type="italics">range</format> 
        /// to 1. Set this parameter to False to leave the pixel values unchanged.
        /// This parameter has a default value of True.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is optimized for MMX.
        /// </remarks>

        public static void Threshold(VisionImage source, VisionImage destination, Range range, bool replaceValue)
        {
            Threshold(source, destination, range, replaceValue, 1);
        }
        //==========================================================================================
        /// <summary>
        /// Thresholds an image. The method sets pixels values outside of the given range to 0. The method sets 
        /// pixel values within the range to a given value or leaves the values unchanged.
        /// </summary>
        /// <param name="source">
        /// The image to threshold.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="range">
        /// The range of pixel values to keep. The default is (128,255).
        /// </param>
        /// <param name="replaceValue">
        /// Set this parameter to True to set the pixel values within the <format type="italics">range</format> 
        /// to the value specified in <format type="italics">newValue</format>. Set this parameter to False to 
        /// leave the pixel values unchanged. This parameter has a default value of True.
        /// </param>
        /// <param name="newValue">
        /// If <format type="italics">replaceValue</format> is set to True, 
        /// <format type="italics">newValue</format> is the replacement value for pixels within the 
        /// <format type="italics">range</format>. If <format type="italics">replaceValue</format> is set to 
        /// False, the pixel values are not changed. The default value is 1.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///  
        /// ' Threshold the image in Viewer1 and store the result in i.
        /// Algorithms.Threshold (imageViewer1.Image, i, New Range(128, 255), True, 255)
        ///  
        /// ' Threshold the image in Viewer1.
        /// ' Do the operation in-place.
        /// Algorithms.Threshold (imageViewer1.Image, imageViewer1.Image, New Range(128, 255), False)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///  
        /// // Threshold the image in Viewer1 and store the result in i.
        /// Algorithms.Threshold(imageViewer1.Image, i, new Range(128, 255), true, 255);
        ///  
        /// // Threshold the image in Viewer1.
        /// // Do the operation in-place.
        /// Algorithms.Threshold(imageViewer1.Image, imageViewer1.Image, new Range(128, 255), false);
        /// </code>
        /// </example>

        public static void Threshold(VisionImage source, VisionImage destination, Range range, bool replaceValue, double newValue)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (range == null) { throw new ArgumentNullException("range"); }
            Utilities.ThrowError(VisionDll.imaqThreshold(VisionImage.GetIntPtr(destination), source._image, (float)range.Minimum, (float)range.Maximum, replaceValue ? 1 : 0, (float)newValue));
        }
        //==========================================================================================
        /// <summary>
        /// Thresholds an image. The method sets pixels values outside of the given range to 0. The method sets 
        /// pixel values within the range to a given value or leaves the values unchanged.
        /// </summary>
        /// <param name="source">
        /// The image to threshold.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="thresholdData">
        /// The options to use to threshold the image.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is optimized for MMX.
        /// </remarks>

        public static void Threshold(VisionImage source, VisionImage destination, ThresholdData thresholdData)
        {
            if (thresholdData == null) { throw new ArgumentNullException("thresholdData"); }
            Threshold(source, destination, thresholdData.Range, thresholdData.UseNewValue, thresholdData.NewValue);
        }
        //==========================================================================================
        /// <summary>
        /// Thresholds an image into multiple classes. The method classifies each pixel into the last threshold 
        /// range of which it is a member. If a pixel is not a member of any of the given ranges, the method 
        /// sets it to 0.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="thresholdData">
        /// The data required to perform the multithreshold.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// <para>
        /// The threshold operations are performed in the order that the data is received from ThresholdData. 
        /// A pixel can be taken into account only once, even if the pixel is included in the threshold range 
        /// of two different thresholds.
        /// </para>
        /// 	<para>
        /// For example, consider the following ThresholdData input:
        /// </para>
        /// 	<list type="bullet">
        /// 		<item>
        /// 			<description>
        /// Item 1: ThresholdData.Range = (80, 150), UseNewValue = True, NewValue = 255	
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// Item 2: ThresholdData.Range = (120, 200), UseNewValue = False
        /// </description>
        /// 		</item>
        /// 	</list>
        /// 	<para>
        /// This example shows two threshold ranges with an overlap between 120 and 150. Therefore, the 
        /// pixels between 120 and 150 are treated only by the second threshold. The following results occur 
        /// after execution of this method:
        /// </para>
        /// 	<list type="bullet">
        /// 		<item>
        /// 			<description>
        /// Pixel values between 0 and 79 are replaced by 0.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// Pixel values between 80 and 120 are replaced by 255.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// Pixel values between 121 and 200 keep their original values.
        /// </description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// Pixel values greater than 200 are set to 0.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// Dim i As New VisionImage
        /// Dim thresholdData As Collection(Of ThresholdData)
        ///  
        /// 'AutoThreshold the image in Viewer1 into 3 classes
        /// 'Store the result in i
        /// thresholdData = Algorithms.AutoThreshold (imageViewer1.Image, i, 3)
        ///     
        /// 'Use ThresholdData result to MultiThreshold the Viewer inplace
        /// Algorithms.MultiThreshold (imageViewer1.Image, imageViewer1.Image, thresholdData)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// VisionImage i = new VisionImage();
        /// Collection&lt;ThresholdData&gt; thresholdData;
        ///     
        /// //AutoThreshold the image in Viewer1 into 3 classes
        /// //Store the result in i
        /// thresholdData = Algorithms.AutoThreshold(imageViewer1.Image, i, 3);
        ///   
        /// //Use ThresholdData result to MultiThreshold the Viewer inplace
        /// Algorithms.MultiThreshold(imageViewer1.Image, imageViewer1.Image, thresholdData);
        /// </code>
        /// </example>

        public static void MultiThreshold(VisionImage source, VisionImage destination, Collection<ThresholdData> thresholdData)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (thresholdData == null) { throw new ArgumentNullException("thresholdData"); }
            CVI_ThresholdData[] cviData = Utilities.ConvertCollectionToArray<ThresholdData, CVI_ThresholdData>(thresholdData); 
            Utilities.ThrowError(VisionDll.imaqMultithreshold(destination._image, source._image, cviData, cviData.Length));
        }
        //==========================================================================================
        /// <summary>
        /// Automatically thresholds an image into multiple classes.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting U8 image.
        /// </param>
        /// <returns>
        /// A collection of ThresholdData objects that specifies the threshold ranges. On failure, an exception is thrown.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<ThresholdData> AutoThreshold(VisionImage source, VisionImage destination)
        {
            return AutoThreshold(source, destination, 2, ThresholdMethod.Clustering, null);
        }
        //==========================================================================================
        /// <summary>
        /// Automatically thresholds an image into multiple classes.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting U8 image.
        /// </param>
        /// <param name="numberOfClasses">
        /// The number of classes into which to threshold the image. Valid values range from 2 to 256.
        /// </param>
        /// <returns>
        /// A collection of ThresholdData objects that specifies the threshold ranges. On failure, an exception is thrown.
        /// </returns>

        public static Collection<ThresholdData> AutoThreshold(VisionImage source, VisionImage destination, int numberOfClasses)
        {
            return AutoThreshold(source, destination, numberOfClasses, ThresholdMethod.Clustering, null);
        }
        //==========================================================================================
        /// <summary>
        /// Automatically thresholds an image into multiple classes.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting U8 image.
        /// </param>
        /// <param name="numberOfClasses">
        /// The number of classes into which to threshold the image. Valid values range from 2 to 256.
        /// </param>
        /// <param name="method">
        /// Specifies the threshold method to use if numberOfClasses is 2. The method ignores this parameter if numberOfClasses is not 2.
        /// </param>
        /// <returns>
        /// A collection of ThresholdData objects that specifies the threshold ranges. On failure, an exception is thrown.
        /// </returns>

        public static Collection<ThresholdData> AutoThreshold(VisionImage source, VisionImage destination, int numberOfClasses, ThresholdMethod method)
        {
            return AutoThreshold(source, destination, numberOfClasses, method, null);
        }
        //==========================================================================================
        /// <summary>
        /// Automatically thresholds an image into multiple classes.
        /// </summary>
        /// <param name="source">
        /// The image to process.
        /// </param>
        /// <param name="destination">
        /// The resulting U8 image.
        /// </param>
        /// <param name="numberOfClasses">
        /// The number of classes into which to threshold the image. Valid values range from 2 to 256.
        /// </param>
        /// <param name="method">
        /// A <see cref="NationalInstruments.Vision.Analysis.ThresholdMethod" crefType="Unqualified"/> value that specifies the threshold method to use if numberOfClasses is 2. The method ignores this parameter if numberOfClasses is not 2.
        /// </param>
        /// <param name="mask">
        /// Specifies the region in which the method computes the <format type="italics">method</format>.The method processes only those pixels in the image whose corresponding pixels in the mask are non-zero. Do not set this parameter if you want to calculate the <format type="italics">method</format> for the entire image.
        /// </param>
        /// <returns>
        /// A collection of ThresholdData objects that specifies the threshold ranges. On failure, an exception is thrown.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim ThresholdData(,) As Single 
        ///  
        /// 'AutoThreshold the image in Viewer1 into 3 classes 
        /// 'Store the result in i 
        /// Algorithms.AutoThreshold(imageViewer1.Image, i, 3, ThresholdMethod.Clustering, ThesholdData)
        ///  
        /// 'Use AutoThreshold result to MultiThreshold the Viewer inplace 
        /// Algorithms.MultiThreshold(imageViewer1.Image, imageViewer1.Image, ThresholdData)
        ///  
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// float[,] ThresholdData = null;
        ///     
        /// //AutoThreshold the image in Viewer1 into 3 classes
        /// //Store the result in i
        /// Algorithms.AutoThreshold(imageViewer1.Image, i, 3, ThresholdMethod.Clustering, ThesholdData);
        ///     
        /// //Use AutoThreshold result to MultiThreshold the Viewer inplace
        /// Algorithms.MultiThreshold(imageViewer1.Image, imageViewer1.Image, ThresholdData);
        /// </code>
        /// </example>

        public static Collection<ThresholdData> AutoThreshold(VisionImage source, VisionImage destination, int numberOfClasses, ThresholdMethod method, VisionImage mask)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            IntPtr data = VisionDll.imaqAutoThreshold2(destination._image, source._image, numberOfClasses, method, VisionImage.GetIntPtr(mask));
            Utilities.ThrowError(data);
            return Utilities.ConvertIntPtrToCollection<ThresholdData, CVI_ThresholdData>(data, numberOfClasses, true);
        }
        //==========================================================================================
        /// <summary>
        /// Automatically thresholds an image into a binary image based on the requested local adaptive thresholding method.
        /// </summary>
        /// <param name="source">
        /// The image to threshold.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 and I16 images. All of the images you pass to this method must be of the same type.
        /// </remarks>

        public static void LocalThreshold(VisionImage source, VisionImage destination)
        {
            LocalThreshold(source, destination, new LocalThresholdOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Automatically thresholds an image into a binary image based on the requested local adaptive thresholding method.
        /// </summary>
        /// <param name="source">
        /// The image to threshold.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="options">
        /// Specifies the options to use to perform the local threshold.
        /// </param>
        /// <remarks>
        /// Use this method with U8 and I16 images. All of the images you pass to this method must be of the same type.
        /// The window size you specify in <format type="italics">options</format> should be sized as large as 
        /// possible but small enough that each window contains pixels under similar lighting conditions. 
        /// This method will produce inconsistent results for windows that contain uniform pixel values.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Do a local threshold on the image in viewer1
        /// Algorithms.LocalThreshold (imageViewer1.Image, imageViewer1.Image)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// 'Do a local threshold on the image in viewer1
        /// Algorithms.LocalThreshold(imageViewer1.Image, imageViewer1.Image);
        /// </code>
        /// </example>

        public static void LocalThreshold(VisionImage source, VisionImage destination, LocalThresholdOptions options)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            Utilities.ThrowError(VisionDll.imaqLocalThreshold(destination._image, source._image, options.WindowWidth, options.WindowHeight, options.Method, options.DeviationWeight, options.ParticleType, (float)options.ReplaceValue));
        }
        //==========================================================================================
        /// <summary>
        /// Creates an image mask by extracting a region surrounding a reference pixel, called the origin, and 
        /// using a tolerance (+ or -) of intensity variations based on this reference pixel. Using this origin, 
        /// the method searches for its neighbors with an intensity that equals or falls within the tolerance 
        /// value of the point of reference. The resulting image is binary.
        /// </summary>
        /// <param name="source">
        /// The source image containing the particle to mask.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="point">
        /// The origin.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">destination</format> must be 
        /// a U8 image. If <format type="italics">source</format> and <format type="italics">destination</format> 
        /// are the same image, then <format type="italics">source</format> must be a U8 image.
        /// </remarks>

        public static void MagicWand(VisionImage source, VisionImage destination, PointContour point)
        {
            MagicWand(source, destination, point, 20, Connectivity.Connectivity8, 1.0);
        }
        //==========================================================================================
        /// <summary>
        /// Creates an image mask by extracting a region surrounding a reference pixel, called the origin, and 
        /// using a tolerance (+ or -) of intensity variations based on this reference pixel. Using this origin, 
        /// the method searches for its neighbors with an intensity that equals or falls within the tolerance 
        /// value of the point of reference. The resulting image is binary.
        /// </summary>
        /// <param name="source">
        /// The source image containing the particle to mask.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="point">
        /// The ROI containing the origin.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">destination</format> must be 
        /// a U8 image. If <format type="italics">source</format> and <format type="italics">destination</format> 
        /// are the same image, then <format type="italics">source</format> must be a U8 image.
        /// </remarks>

        public static void MagicWand(VisionImage source, VisionImage destination, Roi point)
        {
            if (point == null) { throw new ArgumentNullException("point"); }
            point.ThrowIfDisposed();
            Utilities.ThrowIfNotSinglePoint(point);
            MagicWand(source, destination, (PointContour)point[0].Shape, 20, Connectivity.Connectivity8, 1.0);
        }
        //==========================================================================================
        /// <summary>
        /// Creates an image mask by extracting a region surrounding a reference pixel, called the origin, and 
        /// using a tolerance (+ or -) of intensity variations based on this reference pixel. Using this origin, 
        /// the method searches for its neighbors with an intensity that equals or falls within the tolerance 
        /// value of the point of reference. The resulting image is binary.
        /// </summary>
        /// <param name="source">
        /// The source image containing the particle to mask.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="point">
        /// The origin.
        /// </param>
        /// <param name="tolerance">
        /// Specifies the pixel value tolerance that the method uses to determine whether neighbors of the reference 
        /// point are part of the particle. The default is 20.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">destination</format> must be 
        /// a U8 image. If <format type="italics">source</format> and <format type="italics">destination</format> 
        /// are the same image, then <format type="italics">source</format> must be a U8 image.
        /// </remarks>

        public static void MagicWand(VisionImage source, VisionImage destination, PointContour point, double tolerance)
        {
            MagicWand(source, destination, point, tolerance, Connectivity.Connectivity8, 1.0);
        }
        //==========================================================================================
        /// <summary>
        /// Creates an image mask by extracting a region surrounding a reference pixel, called the origin, and 
        /// using a tolerance (+ or -) of intensity variations based on this reference pixel. Using this origin, 
        /// the method searches for its neighbors with an intensity that equals or falls within the tolerance 
        /// value of the point of reference. The resulting image is binary.
        /// </summary>
        /// <param name="source">
        /// The source image containing the particle to mask.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="point">
        /// The ROI containing the origin.
        /// </param>
        /// <param name="tolerance">
        /// Specifies the pixel value tolerance that the method uses to determine whether neighbors of the reference 
        /// point are part of the particle. The default is 20.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">destination</format> must be 
        /// a U8 image. If <format type="italics">source</format> and <format type="italics">destination</format> 
        /// are the same image, then <format type="italics">source</format> must be a U8 image.
        /// </remarks>

        public static void MagicWand(VisionImage source, VisionImage destination, Roi point, double tolerance)
        {
            if (point == null) { throw new ArgumentNullException("point"); }
            point.ThrowIfDisposed();
            Utilities.ThrowIfNotSinglePoint(point);
            MagicWand(source, destination, (PointContour)point[0].Shape, tolerance, Connectivity.Connectivity8, 1.0);
        }
        //==========================================================================================
        /// <summary>
        /// Creates an image mask by extracting a region surrounding a reference pixel, called the origin, and 
        /// using a tolerance (+ or -) of intensity variations based on this reference pixel. Using this origin, 
        /// the method searches for its neighbors with an intensity that equals or falls within the tolerance 
        /// value of the point of reference. The resulting image is binary.
        /// </summary>
        /// <param name="source">
        /// The source image containing the particle to mask.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="point">
        /// The origin.
        /// </param>
        /// <param name="tolerance">
        /// Specifies the pixel value tolerance that the method uses to determine whether neighbors of the reference 
        /// point are part of the particle. The default is 20.
        /// </param>
        /// <param name="connectivity">Set this parameter to Connectivity8 to use connectivity-8 to determine 
        /// whether particles are touching. Set this parameter to Connectivity4 to use connectivity-4 to 
        /// determine whether particles are touching. The default is Connectivity8. Refer to the 
        /// <format type="italics">NI Vision Concepts Help</format> for more information about connectivity.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">destination</format> must be 
        /// a U8 image. If <format type="italics">source</format> and <format type="italics">destination</format> 
        /// are the same image, then <format type="italics">source</format> must be a U8 image.
        /// </remarks>

        public static void MagicWand(VisionImage source, VisionImage destination, PointContour point, double tolerance, Connectivity connectivity)
        {
            MagicWand(source, destination, point, tolerance, connectivity, 1.0);
        }
        //==========================================================================================
        /// <summary>
        /// Creates an image mask by extracting a region surrounding a reference pixel, called the origin, and 
        /// using a tolerance (+ or -) of intensity variations based on this reference pixel. Using this origin, 
        /// the method searches for its neighbors with an intensity that equals or falls within the tolerance 
        /// value of the point of reference. The resulting image is binary.
        /// </summary>
        /// <param name="source">
        /// The source image containing the particle to mask.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="point">
        /// The ROI containing the origin.
        /// </param>
        /// <param name="tolerance">
        /// Specifies the pixel value tolerance that the method uses to determine whether neighbors of the reference 
        /// point are part of the particle. The default is 20.
        /// </param>
        /// <param name="connectivity">Set this parameter to Connectivity8 to use connectivity-8 to determine 
        /// whether particles are touching. Set this parameter to Connectivity4 to use connectivity-4 to 
        /// determine whether particles are touching. The default is Connectivity8. Refer to the 
        /// <format type="italics">NI Vision Concepts Help</format> for more information about connectivity.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">destination</format> must be 
        /// a U8 image. If <format type="italics">source</format> and <format type="italics">destination</format> 
        /// are the same image, then <format type="italics">source</format> must be a U8 image.
        /// </remarks>

        public static void MagicWand(VisionImage source, VisionImage destination, Roi point, double tolerance, Connectivity connectivity)
        {
            if (point == null) { throw new ArgumentNullException("point"); }
            point.ThrowIfDisposed();
            Utilities.ThrowIfNotSinglePoint(point);
            MagicWand(source, destination, (PointContour)point[0].Shape, tolerance, connectivity, 1.0);
        }
        //==========================================================================================
        /// <summary>
        /// Creates an image mask by extracting a region surrounding a reference pixel, called the origin, and 
        /// using a tolerance (+ or -) of intensity variations based on this reference pixel. Using this origin, 
        /// the method searches for its neighbors with an intensity that equals or falls within the tolerance 
        /// value of the point of reference. The resulting image is binary.
        /// </summary>
        /// <param name="source">
        /// The source image containing the particle to mask.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="point">
        /// The ROI containing the origin.
        /// </param>
        /// <param name="tolerance">
        /// Specifies the pixel value tolerance that the method uses to determine whether neighbors of the reference 
        /// point are part of the particle. The default is 20.
        /// </param>
        /// <param name="connectivity">Set this parameter to Connectivity8 to use connectivity-8 to determine 
        /// whether particles are touching. Set this parameter to Connectivity4 to use connectivity-4 to 
        /// determine whether particles are touching. The default is Connectivity8. Refer to the 
        /// <format type="italics">NI Vision Concepts Help</format> for more information about connectivity.
        /// </param>
        /// <param name="replaceValue">
        /// The value to which the method sets pixels in the selected particle. The method sets pixels outside 
        /// the particle to 0. This parameter has a default value of 1.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">destination</format> must be 
        /// a U8 image. If <format type="italics">source</format> and <format type="italics">destination</format> 
        /// are the same image, then <format type="italics">source</format> must be a U8 image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Private Sub CWIMAQViewer1_RegionsChanged(Count As Variant)
        ///     If Count &gt;= 1 Then
        ///         If CWIMAQViewer1.Regions(1).Shape = cwimaqRegionPoint Then
        ///             CWIMAQVision1.MagicWand2 CWIMAQViewer1.Image, CWIMAQViewer2.Image, CWIMAQViewer1.Regions(1), , , 255
        ///         End If
        ///     End If
        /// End Sub
        /// </code>
        /// </example>

        public static void MagicWand(VisionImage source, VisionImage destination, Roi point, double tolerance, Connectivity connectivity, double replaceValue)
        {
            if (point == null) { throw new ArgumentNullException("point"); }
            point.ThrowIfDisposed();
            Utilities.ThrowIfNotSinglePoint(point);
            MagicWand(source, destination, (PointContour)point[0].Shape, tolerance, connectivity, replaceValue);
        }
        //==========================================================================================
        /// <summary>
        /// Creates an image mask by extracting a region surrounding a reference pixel, called the origin, and 
        /// using a tolerance (+ or -) of intensity variations based on this reference pixel. Using this origin, 
        /// the method searches for its neighbors with an intensity that equals or falls within the tolerance 
        /// value of the point of reference. The resulting image is binary.
        /// </summary>
        /// <param name="source">
        /// The source image containing the particle to mask.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="point">
        /// The origin.
        /// </param>
        /// <param name="tolerance">
        /// Specifies the pixel value tolerance that the method uses to determine whether neighbors of the reference 
        /// point are part of the particle. The default is 20.
        /// </param>
        /// <param name="connectivity">Set this parameter to Connectivity8 to use connectivity-8 to determine 
        /// whether particles are touching. Set this parameter to Connectivity4 to use connectivity-4 to 
        /// determine whether particles are touching. The default is Connectivity8. Refer to the 
        /// <format type="italics">NI Vision Concepts Help</format> for more information about connectivity.
        /// </param>
        /// <param name="replaceValue">
        /// The value to which the method sets pixels in the selected particle. The method sets pixels outside 
        /// the particle to 0. This parameter has a default value of 1.
        /// </param>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. <format type="italics">destination</format> must be 
        /// a U8 image. If <format type="italics">source</format> and <format type="italics">destination</format> 
        /// are the same image, then <format type="italics">source</format> must be a U8 image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Do a magic wand operation on the ROI in viewer1, if it's a point.
        /// If imageViewer1.Roi.Count = 1 AndAlso imageViewer1.Roi(0).Type = ContourType.Point Then
        ///     Algorithms.MagicWand (imageViewer1.Image, imageViewer2.Image, New PointContour (), 20, 
        /// Connectivity.Connectivity8, 255)
        /// End If
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Do a magic wand operation on the ROI in viewer1, if it's a point.
        /// if (imageViewer1.Roi.Count == 1 &amp;&amp; imageViewer1.Roi[0].Type == ContourType.Point) {
        ///    Algorithms.MagicWand(imageViewer1.Image, imageViewer2.Image, new PointContour(), 20, 
        /// Connectivity.Connectivity8, 255);
        /// }
        /// </code>
        /// </example>

        public static void MagicWand(VisionImage source, VisionImage destination, PointContour point, double tolerance, Connectivity connectivity, double replaceValue)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (point == null) { throw new ArgumentNullException("point"); }
            CVI_Point cviPoint = new CVI_Point();
            cviPoint.ConvertFromExternal(point);
            Utilities.ThrowError(VisionDll.imaqMagicWand(destination._image, source._image, cviPoint, (float)tolerance, connectivity, (float)replaceValue));
        }
        #endregion

        #region File I/O functions
        //==========================================================================================
        /// <summary>
        /// Returns information regarding the contents of an image file. You can retrieve information from the following 
        /// image file formats only: AIPD, BMP, JPEG, JPEG2000, PNG, and TIFF.
        /// </summary>
        /// <param name="fileName">
        /// The name of the file from which the method gets information.
        /// </param>
        /// <returns>
        /// Information about the file, such as calibration information, height, width, and image type.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim filename As String
        ///  
        /// 'Set the filename in the variable named filename
        ///  
        /// 'Get the file information
        /// Algorithms.GetFileInformation (filename)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// string filename;
        ///    
        /// //Set the filename in the variable named filename
        ///    
        /// //Get the file information
        /// Algorithms.GetFileInformation(filename);
        /// </code>
        /// </example>

        public static FileInformation GetFileInformation(string fileName)
        {
            return CommonAlgorithms.GetFileInformation(fileName);
        }
        #endregion

        #region Frequency Domain Analysis functions
        //==========================================================================================
        /// <summary>Attenuates the frequencies of a complex image.
        /// </summary>
        /// <param name="source">The source image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">destination</format> must be Complex images.
        /// </remarks>

        public static void ComplexAttenuate(VisionImage source, VisionImage destination)
        {
            ComplexAttenuate(source, destination, AttenuateMode.Low);
        }
        //==========================================================================================
        /// <summary>Attenuates the frequencies of a complex image.
        /// </summary>
        /// <param name="source">The source image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <param name="attenuateMode">Determines which frequencies to attenuate. Set this parameter to <see cref="NationalInstruments.Vision.Analysis.AttenuateMode.High" crefType="Unqualified"/> to attenuate the high frequencies or <see cref="NationalInstruments.Vision.Analysis.AttenuateMode.Low" crefType="Unqualified"/> to attenuate the low frequencies.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">destination</format> must be Complex images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Make i a Complex image
        /// Dim i As New VisionImage(ImageType.Complex)
        /// 'Attenuate the frequencies of the Complex image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.ComplexAttenuate (imageViewer1.Image, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Make i a Complex image
        /// VisionImage i = new VisionImage(ImageType.Complex);
        /// //Attenuate the frequencies of the Complex image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.ComplexAttenuate(imageViewer1.Image, i);
        /// </code>
        /// </example>

        public static void ComplexAttenuate(VisionImage source, VisionImage destination, AttenuateMode attenuateMode)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqAttenuate(destination._image, source._image, attenuateMode));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the conjugate of a complex image, converting the complex pixel data (z = a + ib) into (z = a - ib).
        /// </summary>
        /// <param name="source">The source image.
        /// </param>
        /// <param name="destination">The resulting image.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">destination</format> must be Complex images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Make i a Complex image
        /// Dim i As New VisionImage(ImageType.Complex)
        /// 'Complete the conjugate of the Complex image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.ComplexConjugate (imageViewer1.Image, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Make i a Complex image
        /// VisionImage i = new VisionImage(ImageType.Complex);
        /// //Complete the conjugate of the Complex image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.ComplexConjugate(imageViewer1.Image, i);
        /// </code>
        /// </example>

        public static void ComplexConjugate(VisionImage source, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqConjugate(destination._image, source._image));
        }
        //==========================================================================================
        /// <summary>
        /// Computes the Fourier transform of an image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The complex image containing the resulting FFT image.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> must be an U8, I16, Single, or Complex image. <format type="italics">destination</format> 
        /// must be a Complex image. The image can be any size, but the method works faster if the image dimensions are in powers of 2. 
        /// The <format type="italics">source</format> image and <format type="italics">destination</format> image must be different to 
        /// perform this operation. This method is optimized for MMX.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// ' Make i a complex image.
        /// Dim i As New VisionImage(ImageType.Complex)
        ///  
        /// ' Perform an FFT on the image in Viewer1 and store the resulting complex image in i.
        /// Algorithms.Fft (imageViewer1.Image, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Make i a complex image.
        /// VisionImage i = new VisionImage(ImageType.Complex);
        ///    
        /// // Perform an FFT on the image in Viewer1 and store the resulting complex image in i.
        /// Algorithms.Fft(imageViewer1.Image, i);
        /// </code>
        /// </example>

        public static void Fft(VisionImage source, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqFFT(destination._image, source._image));
        }
        //==========================================================================================
        /// <summary>
        /// Transposes the high and low frequencies of a complex image. The method flips the high and low frequency components of an FFT image to produce a central, symmetric representation of the spatial frequencies.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// All images must be Complex.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Make i a Complex image
        /// Dim i As New VisionImage(ImageType.Complex)
        /// 'Flip the frequencies of the Complex image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.ComplexFlipFrequency (imageViewer1.Image, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Make i a Complex image
        /// VisionImage i = new VisionImage(ImageType.Complex);
        /// //Flip the frequencies of the Complex image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.ComplexFlipFrequency(imageViewer1.Image, i);
        /// </code>
        /// </example>

        public static void ComplexFlipFrequency(VisionImage source, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqFlipFrequencies(destination._image, source._image));
        }
        //==========================================================================================
        /// <summary>
        /// Takes the inverse Fourier transform of an image.
        /// </summary>
        /// <param name="source">
        /// The image whose inverse Fourier transform the method takes. 
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> must be a Complex image. <format type="italics">destination</format>
        /// must be a U8, I16, SGL, or Complex image. <format type="italics">source</format> and 
        /// <format type="italics">destination</format> must be different to perform this operation.
        /// <para>
        /// This method uses a buffer equal to the size of the complex image. An 8-bit image with a resolution of 
        /// 256 x 256 pixels uses 64 KB of memory. The FFT associated with this image requires eight times the memory, 
        /// or 512 KB. The calculation of the inverse FFT also requires a temporary buffer of 512 KB. Therefore, the 
        /// total memory necessary for this operation is 1080 KB.</para>
        /// 	<para>
        /// This method is optimized for MMX.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This example assumes that the image in Viewer1 is a complex image.
        /// Dim i As New VisionImage
        ///  
        /// 'Perform an inverse FFT on the image in Viewer1.
        /// 'Store the result in i.
        /// Algorithms.InverseFFT (imageViewer1.Image, i)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //This example assumes that the image in Viewer1 is a complex image.
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Perform an inverse FFT on the image in Viewer1.
        /// //Store the result in i.
        /// Algorithms.InverseFFT(imageViewer1.Image, i);
        /// </code>
        /// </example>

        public static void InverseFft(VisionImage source, VisionImage destination)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqInverseFFT(destination._image, source._image));
        }
        //==========================================================================================
        /// <summary>
        /// Truncates the frequencies of a complex image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <remarks>
        /// All images must be Complex.
        /// </remarks>

        public static void ComplexTruncate(VisionImage source, VisionImage destination)
        {
            ComplexTruncate(source, destination, TruncateMode.Low, 10);
        }
        //==========================================================================================
        /// <summary>
        /// Truncates the frequencies of a complex image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="truncateMode">Determines which frequencies are truncated. Set this parameter to <see cref="NationalInstruments.Vision.Analysis.TruncateMode.High" crefType="Unqualified"/> to truncate the high frequencies or <see cref="NationalInstruments.Vision.Analysis.TruncateMode.Low" crefType="Unqualified"/> to truncate the low frequencies.
        /// The default is Low.
        /// </param>
        /// <remarks>
        /// All images must be Complex.
        /// </remarks>

        public static void ComplexTruncate(VisionImage source, VisionImage destination, TruncateMode truncateMode)
        {
            ComplexTruncate(source, destination, truncateMode, 10);
        }
        //==========================================================================================
        /// <summary>
        /// Truncates the frequencies of a complex image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image.
        /// </param>
        /// <param name="truncateMode">Determines which frequencies are truncated. Set this parameter to <see cref="NationalInstruments.Vision.Analysis.TruncateMode.High" crefType="Unqualified"/> to truncate the high frequencies or <see cref="NationalInstruments.Vision.Analysis.TruncateMode.Low" crefType="Unqualified"/> to truncate the low frequencies.
        /// The default is Low.</param>
        /// <param name="ratioToKeep">The percentage of the frequencies retained within a Fourier transformed (complex) image. The default value is 10 percent. The percentage works in conjunction with the length of the diagonal of the FFT image and the parameter <format type="italics">truncateMode</format>. For example, if you set <format type="italics">truncateMode</format> to <see cref="NationalInstruments.Vision.Analysis.TruncateMode.Low" crefType="Unqualified"/> and set <format type="italics">ratioToKeep</format> to 10 percent, the method retains 10 percent of the frequencies starting from the center (low frequencies). If you set <format type="italics">truncateMode</format> to <see cref="NationalInstruments.Vision.Analysis.TruncateMode.High" crefType="Unqualified"/> and set <format type="italics">ratioToKeep</format> to 10 percent, the method retains 10 percent of the frequencies starting from the outer periphery.
        /// The default is 10.0.
        /// </param>
        /// <remarks>
        /// All images must be Complex.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Make i a Complex image
        /// Dim i As New VisionImage(ImageType.Complex)
        /// 'Truncate the frequencies of the Complex image in Viewer1 at 25 percent.
        /// 'Store the result in i.
        /// Algorithms.ComplexTruncate (imageViewer1.Image, i, TruncateMode.Low, 25)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Make i a Complex image
        /// VisionImage i = new VisionImage(ImageType.Complex);
        /// //Truncate the frequencies of the Complex image in Viewer1 at 25 percent.
        /// //Store the result in i.
        /// Algorithms.ComplexTruncate(imageViewer1.Image, i, TruncateMode.Low, 25);
        /// </code>
        /// </example>

        public static void ComplexTruncate(VisionImage source, VisionImage destination, TruncateMode truncateMode, double ratioToKeep)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqTruncate(destination._image, source._image, truncateMode, (float)ratioToKeep));
        }
        #endregion

        #region Pixel Manipulation functions
        //==========================================================================================
        /// <summary>
        /// Extracts the three planes (RGB, HSL, HSV, or HSI) from an image. The type of planes that you extract does not need 
        /// to be the same as the image type. For example, you can extract the Hue, Saturation, and Luminance plane from an 
        /// Rgb32 image.
        /// </summary>
        /// <param name="image">
        /// The color image from which the method extracts the color planes.
        /// </param>
        /// <param name="colorMode">
        /// The color spaced used for the operation.
        /// </param>
        /// <param name="plane1">
        /// The image for the first extracted plane. It contains either the red plane (RGB) or the hue plane 
        /// (HSL, HSV, or HSI).
        /// </param>
        /// <param name="plane2">
        /// The image for the second extracted plane. It contains either the green plane (RGB) or the saturation plane 
        /// (HSL, HSV, or HSI).
        /// </param>
        /// <param name="plane3">
        /// The image for the third extracted plane. It contains either the blue plane (RGB), the luminance plane (HSL), 
        /// the value plane (HSV), or the Intensity plane (HSI).
        /// </param>
        /// <remarks>
        /// The source image must be Rgb32, Hsl32, or RgbU64 and the destination images must be U8 for Rgb32 and Hsl32 
        /// and I16 for RgbU64. Only the red, green, or blue plane can be extracted from RgbU64 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// ' This example assumes that the image in Viewer1 is an Rgb32 image.
        /// Dim red As New VisionImage
        /// Dim green As New VisionImage
        /// Dim blue As New VisionImage
        /// Dim hue As New VisionImage
        /// Dim saturation As New VisionImage
        /// Dim luminance As New VisionImage
        /// Dim value As New VisionImage
        /// Dim intensity As New VisionImage
        ///  
        /// ' Extract the red, green and blue planes from the image in Viewer1.
        /// Algorithms.ExtractColorPlanes (imageViewer1.Image, ColorMode.Rgb, red, green, blue)
        ///  
        /// ' Extract the hue, saturation and luminance planes from the image in Viewer1.
        /// Algorithms.ExtractColorPlanes (imageViewer1.Image, ColorMode.Hsl, hue, saturation, luminance)
        ///  
        /// ' Extract the hue, saturation, and intensity planes from the image in Viewer1.
        /// Algorithms.ExtractColorPlanes (imageViewer1.Image, ColorMode.Hsi, hue, saturation, intensity)
        ///  
        /// ' Extract the hue, saturation, and value planes from the image in Viewer1.
        /// Algorithms.ExtractColorPlanes (imageViewer1.Image, ColorMode.Hsv, hue, saturation, value)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // This example assumes that the image in Viewer1 is an Rgb32 image.
        ///  
        /// VisionImage red = new VisionImage();
        /// VisionImage green = new VisionImage();
        /// VisionImage blue = new VisionImage();
        /// VisionImage hue = new VisionImage();
        /// VisionImage saturation = new VisionImage();
        /// VisionImage luminance = new VisionImage();
        /// VisionImage value = new VisionImage();
        /// VisionImage intensity = new VisionImage();
        ///     
        /// // Extract the red, green and blue planes from the image in Viewer1.
        /// Algorithms.ExtractColorPlanes(imageViewer1.Image, ColorMode.Rgb, red, green, blue);
        ///     
        /// // Extract the hue, saturation and luminance planes from the image in Viewer1.
        /// Algorithms.ExtractColorPlanes(imageViewer1.Image, ColorMode.Hsl, hue, saturation, luminance);
        ///     
        /// // Extract the hue, saturation, and intensity planes from the image in Viewer1.
        /// Algorithms.ExtractColorPlanes(imageViewer1.Image, ColorMode.Hsi, hue, saturation, intensity);
        ///     
        /// // Extract the hue, saturation, and value planes from the image in Viewer1.
        /// Algorithms.ExtractColorPlanes(imageViewer1.Image, ColorMode.Hsv, hue, saturation, value);
        /// </code>
        /// </example>

        public static void ExtractColorPlanes(VisionImage image, ColorMode colorMode, VisionImage plane1, VisionImage plane2, VisionImage plane3)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(plane1);
            VisionImage.ThrowIfNonNullAndDisposed(plane2);
            VisionImage.ThrowIfNonNullAndDisposed(plane3);
            Utilities.ThrowError(VisionDll.imaqExtractColorPlanes(image._image, (Int32)colorMode, VisionImage.GetIntPtr(plane1), VisionImage.GetIntPtr(plane2), VisionImage.GetIntPtr(plane3)));
        }
        //==========================================================================================
        /// <summary>
        /// Replaces one or more image planes from a color image (RGB, HSL, HSV, or HSI). The method replaces 
        /// only the planes supplied as input. If all three planes are supplied, the source image is ignored, 
        /// and only the destination image is used. The image is resized to the dimensions of the planes passed 
        /// on input. Therefore, their sizes must be identical. If one or two planes are supplied, the planes 
        /// must have the same dimension as the source image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="colorMode">
        /// The color space in which the function replaces planes. 
        /// </param>
        /// <param name="plane1">
        /// The first plane of replacement data. For Rgb32 and Hsl32 images, the plane is a VisionImage object 
        /// of type U8 that contains either the red plane (RGB) or the hue plane (HSL, HSV, or HSI). For 
        /// RgbU64 images, the plane is a VisionImage of type I16 that contains the red plane (RGB). Pass null or Nothing for 
        /// this parameter if you do not want to replace the red or hue plane.
        /// </param>
        /// <param name="plane2">
        /// The second plane of replacement data. For Rgb32 and Hsl32 images, the plane is a VisionImage object 
        /// of type U8 that contains either the green plane (RGB) or the saturation plane (HSL, HSV, or HSI). 
        /// For RgbU64 images, the plane is a VisionImage of type I16 that contains the green plane (RGB). Pass null or Nothing for 
        /// this parameter if you do not want to replace the green or saturation plane.
        /// </param>
        /// <param name="plane3">
        /// The second plane of replacement data. For Rgb32 and Hsl32 images, the plane is a VisionImage object of type 
        /// U8 that contains either the blue plane (RGB), the luminance plane (HSL), the value plane (HSV), or the 
        /// intensity plane (HSI). For RgbU64 images, the plane is a VisionImage of type I16 that contains the blue 
        /// plane (RGB). Pass null or Nothing for this parameter if you do not want to replace the blue, light, 
        /// value, or intensity plane.
        /// </param>
        /// <remarks>
        /// Use this method with Rgb32, Hsl32, and RgbU64 images. Only RGB planes can be replaced in an RgbU64 image.
        /// All source images must be the same size.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstrument.Vision
        /// Imports NationalInstrument.Vision.Analysis
        ///  
        /// Dim redPlane As New VisionImage
        /// 'Only replace the red plane of the image in viewer1.
        /// Algorithms.ReplaceColorPlanes (imageViewer1.Image, imageViewer1.Image, ColorMode.Rgb, redPlane, Nothing, Nothing)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstrument.Vision.Analysis;
        ///  
        /// VisionImage redPlane = new VisionImage();
        /// // Only replace the red plane of the image in viewer1.
        /// Algorithms.ReplaceColorPlanes(imageViewer1.Image, imageViewer1.Image, ColorMode.Rgb, redPlane, null, null);
        /// </code>
        /// </example>

        public static void ReplaceColorPlanes(VisionImage source, VisionImage destination, ColorMode colorMode, VisionImage plane1, VisionImage plane2, VisionImage plane3)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(plane1);
            VisionImage.ThrowIfNonNullAndDisposed(plane2);
            VisionImage.ThrowIfNonNullAndDisposed(plane3);
            Utilities.ThrowError(VisionDll.imaqReplaceColorPlanes(destination._image, source._image, colorMode, VisionImage.GetIntPtr(plane1), VisionImage.GetIntPtr(plane2), VisionImage.GetIntPtr(plane3)));
        }
        //==========================================================================================
        /// <summary>
        /// Replaces a plane of a complex image with the given array of pixel values. 
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="newPixels">
        /// The 2D array of pixel values. This array must be the same size as source image.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">destination</format> must be complex images.
        /// </remarks>

        public static void ArrayToComplexPlane(VisionImage source, VisionImage destination, float[,] newPixels)
        {
            ArrayToComplexPlane(source, destination, newPixels, ComplexPlane.Real);
        }
        //==========================================================================================
        /// <summary>
        /// Replaces a plane of a complex image with the given array of pixel values. 
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="newPixels">
        /// The 2D array of pixel values. This array must be the same size as source image.
        /// </param>
        /// <param name="plane">
        /// The plane to replace. Set this parameter to <see cref="NationalInstruments.Vision.ComplexPlane.Real" crefType="Unqualified"/> to replace the real plane or <see cref="NationalInstruments.Vision.ComplexPlane.Imaginary" crefType="Unqualified"/> to replace the imaginary plane.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> and <format type="italics">destination</format> must be complex images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        /// Dim RealArray(,) As Single
        /// Dim ImaginaryArray(,) As Single
        ///  
        /// 'Do something here to populate RealArray and ImaginaryArray so that 
        /// 'they are the same size as the image in imageViewer1
        ///  
        /// 'Replace the imaginary plane of the image in imageViewer1 with the data
        /// 'stored in the array named ImaginaryArray.
        /// 'Store the results in image i.
        /// Algorithms.ArrayToComplexPlane(imageViewer1.Image, i, ImaginaryArray, ComplexPlane.Imaginary)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        /// float[,] realArray;
        /// float[,] imaginaryArray;
        ///  
        /// //Do something here to populate realArray and imaginaryArray so that 
        /// //they are the same size as the image in imageViewer1
        ///  
        /// //Replace the imaginary plane of the image in imageViewer1 with the data
        /// //stored in the array named imaginaryArray.
        /// //Store the results in image i.
        /// Algorithms.ArrayToComplexPlane(imageViewer1.Image, i, imaginaryArray, ComplexPlane.Imaginary);
        /// </code>
        /// </example>

        public static void ArrayToComplexPlane(VisionImage source, VisionImage destination, float[,] newPixels, ComplexPlane plane)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (newPixels == null) { throw new ArgumentNullException("newPixels"); }
            // We have to manually check the dimensions of the newPixels array.
            int numRows = newPixels.GetUpperBound(0) - newPixels.GetLowerBound(0) + 1;
            int numCols = newPixels.GetUpperBound(1) - newPixels.GetLowerBound(1) + 1;
            if (numRows != source.Height || numCols != source.Width)
            {
                throw new VisionException(ErrorCode.IncompSize);
            }
            Utilities.ThrowError(VisionDll.imaqArrayToComplexPlane(destination._image, source._image, newPixels, plane));
        }
        //==========================================================================================
        /// <summary>
        /// Extracts a plane from a complex image and places the plane into another image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image containing the extracted plane.
        /// </param>
        /// <remarks>
        /// Use this method with Complex images. The destination image must be a U8, I16, or Single image.
        /// </remarks>

        public static void ExtractComplexPlane(VisionImage source, VisionImage destination)
        {
            ExtractComplexPlane(source, destination, ComplexPlane.Real);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts a plane from a complex image and places the plane into another image.
        /// </summary>
        /// <param name="source">
        /// The source image.
        /// </param>
        /// <param name="destination">
        /// The resulting image containing the extracted plane.
        /// </param>
        /// <param name="plane">
        /// The plane to extract. The default is ComplexPlane.Real.
        /// </param>
        /// <remarks>
        /// Use this method with Complex images. The destination image must be a U8, I16, or Single image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This example assumes that the image in Viewer1 is a Complex image
        /// 'and the image in Viewer2 is a Complex image.
        ///  
        /// 'Set the plane types to Single
        /// Dim r As New VisionImage(ImageType.Single)
        /// Dim i as New VisionImage(ImageType.Single)
        ///     
        /// 'Extract the real plane on the image in Viewer1 into r.
        /// Algorithms.ExtractComplexPlane (imageViewer1.Image, r, ComplexPlane.Real)
        ///     
        /// 'Extract the imaginary plane on the image in Viewer1 into i.
        /// Algorithms.ExtractComplexPlane (imageViewer1.Image, i, ComplexPlane.Imaginary)
        ///     
        /// 'Replace the real plane of Viewer2 with the data in r.
        /// Algorithms.ReplaceComplexPlane (r, imageViewer2.Image, ComplexPlane.Real)
        ///     
        /// 'Replace the imaginary plane of Viewer2 with the data in i.
        /// Algorithms.ReplaceComplexPlane (i, imageViewer2.Image, ComplexPlane.Imaginary)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //This example assumes that the image in Viewer1 is a Complex image
        /// //and the image in Viewer2 is a Complex image.
        ///  
        /// // Set the plane type to Single
        /// VisionImage r = new VisionImage(ImageType.Single);
        /// VisionImage i = new VisionImage(ImageType.Single);
        ///     
        /// //Extract the real plane on the image in Viewer1 into r.
        /// Algorithms.ExtractComplexPlane(imageViewer1.Image, r, ComplexPlane.Real);
        ///     
        /// //Extract the imaginary plane on the image in Viewer1 into i.
        /// Algorithms.ExtractComplexPlane(imageViewer1.Image, i, ComplexPlane.Imaginary);
        ///     
        /// //Replace the real plane of Viewer2 with the data in r.
        /// Algorithms.ReplaceComplexPlane(r, imageViewer2.Image, ComplexPlane.Real);
        ///     
        /// //Replace the imaginary plane of Viewer2 with the data in i.
        /// Algorithms.ReplaceComplexPlane(i, imageViewer2.Image, ComplexPlane.Imaginary);
        /// </code>
        /// </example>

        public static void ExtractComplexPlane(VisionImage source, VisionImage destination, ComplexPlane plane)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqExtractComplexPlane(destination._image, source._image, plane));
        }
        //==========================================================================================
        /// <summary>
        /// Replaces a plane of a complex image with the pixel values from a given image
        /// </summary>
        /// <param name="source">
        /// The image whose data the method modifies.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> must be a U8, I16, Single, or Complex image. 
        /// <format type="italics">destination</format> must be a Complex image.
        /// </remarks>

        public static void ReplaceComplexPlane(VisionImage source, VisionImage destination)
        {
            ReplaceComplexPlane(source, destination, ComplexPlane.Real);
        }
        //==========================================================================================
        /// <summary>
        /// Replaces a plane of a complex image with the pixel values from a given image
        /// </summary>
        /// <param name="source">
        /// The image whose data the method modifies.
        /// </param>
        /// <param name="destination">
        /// The destination image.
        /// </param>
        /// <param name="plane">
        /// The complex image plane to replace. Set this value to ComplexPlane.Real or ComplexPlane.Imaginary. If 
        /// <format type="italics">source</format> is a Complex image, then this parameter also selects which plane 
        /// of the source image is used as the replacement.
        /// </param>
        /// <remarks>
        /// 	<format type="italics">source</format> must be a U8, I16, Single, or Complex image. 
        /// <format type="italics">destination</format> must be a Complex image.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'This example assumes that the image in Viewer1 is a Complex image
        /// 'and the image in Viewer2 is a Complex image.
        ///  
        /// Dim r As New VisionImage
        /// Dim i As New VisionImage
        ///     
        /// 'Set the plane types to Single
        /// r.Type = ImageType.Single
        /// i.Type = ImageType.Single
        ///     
        /// 'Extract the real plane on the image in Viewer1 into r.
        /// Algorithms.ExtractComplexPlane (imageViewer1.Image, r, ComplexPlane.Real)
        ///     
        /// 'Extract the imaginary plane on the image in Viewer1 into i.
        /// Algorithms.ExtractComplexPlane (imageViewer1.Image, i, ComplexPlane.Imaginary)
        ///     
        /// 'Replace the real plane of Viewer2 with the data in r.
        /// Algorithms.ReplaceComplexPlane (r, imageViewer2.Image, ComplexPlane.Real)
        ///   
        /// 'Replace the imaginary plane of Viewer2 with the data in i.
        /// Algorithms.ReplaceComplexPlane (i, imageViewer2.Image, ComplexPlane.Imaginary)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //This example assumes that the image in Viewer1 is a Complex image
        /// //and the image in Viewer2 is a Complex image.
        ///  
        /// VisionImage r = new VisionImage();
        /// VisionImage i = new VisionImage();
        ///     
        /// //Set the plane types to Single
        /// r.Type = ImageType.Single;
        /// i.Type = ImageType.Single;
        ///     
        /// //Extract the real plane on the image in Viewer1 into r.
        /// Algorithms.ExtractComplexPlane(imageViewer1.Image, r, ComplexPlane.Real);
        ///     
        /// //Extract the imaginary plane on the image in Viewer1 into i.
        /// Algorithms.ExtractComplexPlane(imageViewer1.Image, i, ComplexPlane.Imaginary);
        ///     
        /// //Replace the real plane of Viewer2 with the data in r.
        /// Algorithms.ReplaceComplexPlane(r, imageViewer2.Image, ComplexPlane.Real);
        ///     
        /// //Replace the imaginary plane of Viewer2 with the data in i.
        /// Algorithms.ReplaceComplexPlane(i, imageViewer2.Image, ComplexPlane.Imaginary);
        /// </code>
        /// </example>

        public static void ReplaceComplexPlane(VisionImage source, VisionImage destination, ComplexPlane plane)
        {
            if (source == null) { throw new ArgumentNullException("source"); }
            source.ThrowIfDisposed();
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            Utilities.ThrowError(VisionDll.imaqReplaceComplexPlane(destination._image, destination._image, source._image, plane));
        }
        #endregion

        #region LCD functions
        //==========================================================================================
        /// <summary>
        /// Calculates the region of interest of each digit from a rectangular area of interest around whole LCD and 
        /// electroluminescent indicators, which can contain multiple digits. Use this method first in a calibration phase. You can
        /// process the ROI with the <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadLcd" crefType="Unqualified"/> method.
        /// This method requires that all segments of the indicator are activated.
        /// </summary>
        /// <param name="image">
        /// The image containing the LCD. All segments of the LCD must be on.
        /// </param>
        /// <param name="rectangle">
        /// The rectangle containing the indicator.
        /// </param>
        /// <returns>
        /// A region of interest containing contours around each digit.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is designed for LCD and electroluminescent indicators. 
        /// It is resistant to light drift.
        /// </remarks>

        public static Roi FindLcdSegments(VisionImage image, RectangleContour rectangle)
        {
            if (rectangle == null) { throw new ArgumentNullException("rectangle"); }
            // Make a temporary Roi, but be sure to dispose it.
            using (Roi tempRoi = new Roi(new Shape[] { rectangle })) {
                return FindLcdSegments(image, tempRoi, new LcdOptions());
            }
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the region of interest of each digit from a rectangular area of interest around whole LCD and 
        /// electroluminescent indicators, which can contain multiple digits. Use this method first in a calibration phase. You can
        /// process the ROI with the <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadLcd" crefType="Unqualified"/> method.
        /// This method requires that all segments of the indicator are activated.
        /// </summary>
        /// <param name="image">
        /// The image containing the LCD. All segments of the LCD must be on.
        /// </param>
        /// <param name="rectangle">
        /// The rectangle containing the indicator.
        /// </param>
        /// <returns>
        /// A region of interest containing contours around each digit.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is designed for LCD and electroluminescent 
        /// indicators. It is resistant to light drift.
        /// </remarks>

        public static Roi FindLcdSegments(VisionImage image, Roi rectangle)
        {
            return FindLcdSegments(image, rectangle, new LcdOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the region of interest of each digit from a rectangular area of interest around whole LCD and 
        /// electroluminescent indicators, which can contain multiple digits. Use this method first in a calibration phase. You can
        /// process the ROI with the <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadLcd" crefType="Unqualified"/> method.
        /// This method requires that all segments of the indicator are activated.
        /// </summary>
        /// <param name="image">
        /// The image containing the LCD. All segments of the LCD must be on.
        /// </param>
        /// <param name="rectangle">
        /// The rectangle containing the indicator.
        /// </param>
        /// <param name="options">
        /// Controls how the method performs the search.
        /// </param>
        /// <returns>
        /// A region of interest containing contours around each digit.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is designed for LCD and electroluminescent 
        /// indicators. It is resistant to light drift.
        /// </remarks>

        public static Roi FindLcdSegments(VisionImage image, RectangleContour rectangle, LcdOptions options)
        {
            if (rectangle == null) { throw new ArgumentNullException("rectangle"); }
            // Make a temporary Roi, but be sure to dispose it.
            using (Roi tempRoi = new Roi(new Shape[] { rectangle })) {
                return FindLcdSegments(image, tempRoi, options);
            }
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the region of interest of each digit from a rectangular area of interest around whole LCD and 
        /// electroluminescent indicators, which can contain multiple digits. Use this method first in a calibration phase. You can
        /// process the ROI with the <see cref="NationalInstruments.Vision.Analysis.Algorithms.ReadLcd" crefType="Unqualified"/> method.
        /// This method requires that all segments of the indicator are activated.
        /// </summary>
        /// <param name="image">
        /// The image containing the LCD. All segments of the LCD must be on.
        /// </param>
        /// <param name="rectangle">
        /// The rectangle containing the indicator.
        /// </param>
        /// <param name="options">
        /// Controls how the method performs the search.
        /// </param>
        /// <returns>
        /// A region of interest containing contours around each digit.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images. This method is designed for LCD and electroluminescent 
        /// indicators. It is resistant to light drift.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// ' Assumes there is an image in Viewer1 with an ROI around the LCD.
        /// Dim LcdRoi As Roi = Algorithms.FindLcdSegments (imageViewer1.Image, imageViewer1.Roi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Assumes there is an image in Viewer1 with an ROI around the LCD.
        /// Roi lcdRoi = Algorithms.FindLcdSegments(imageViewer1.Image, imageViewer1.Roi);
        /// </code>
        /// </example>

        public static Roi FindLcdSegments(VisionImage image, Roi rectangle, LcdOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (rectangle == null) { throw new ArgumentNullException("rectangle"); }
            rectangle.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_LCDOptions cviOptions = new CVI_LCDOptions();
            cviOptions.ConvertFromExternal(options);
            Roi toReturn = new Roi(rectangle);
            Utilities.ThrowError(VisionDll.imaqFindLCDSegments(toReturn._roi, image._image, ref cviOptions));
            // The contours in the Roi have changed, so tell it to update and return it.
            toReturn.PointerUpdated();
            return toReturn;
        }
        //==========================================================================================
        /// <summary>
        /// Reads the numeric value of a seven-segment LCD. 
        /// </summary>
        /// <param name="image">
        /// The image containing the LCD to read. 
        /// </param>
        /// <param name="roi">
        /// A region of interest consisting of rectangles around each of the digits of the LCD. Generate this ROI by 
        /// calling the <see cref="NationalInstruments.Vision.Analysis.Algorithms.FindLcdSegments" crefType="Unqualified"/> method. 
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.LcdReport" crefType="Unqualified"/> object describing the state of the LCD.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>

        public static LcdReport ReadLcd(VisionImage image, Roi roi)
        {
            return ReadLcd(image, roi, new LcdOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Reads the numeric value of a seven-segment LCD. 
        /// </summary>
        /// <param name="image">
        /// The image containing the LCD to read. 
        /// </param>
        /// <param name="roi">
        /// A region of interest consisting of rectangles around each of the digits of the LCD. Generate this ROI by 
        /// calling the <see cref="NationalInstruments.Vision.Analysis.Algorithms.FindLcdSegments" crefType="Unqualified"/> method. 
        /// </param>
        /// <param name="options">
        /// Controls how the LCD is read.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.LcdReport" crefType="Unqualified"/> object describing the state of the LCD.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, and Single images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim LcdRoi As Roi
        ///  
        /// 'Assumes the LcdRoi has been populated with FindLcdSegments.
        /// Dim Report As LcdReport = Algorithms.ReadLcd (imageViewer1.Image, LcdRoi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// Roi lcdRoi;
        ///  
        /// // Assumes the lcdRoi has been populated with FindLcdSegments.
        /// LcdReport report = Algorithms.ReadLcd(imageViewer1.Image, lcdRoi);
        /// </code>
        /// </example>

        public static LcdReport ReadLcd(VisionImage image, Roi roi, LcdOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            CVI_LCDOptions cviOptions = new CVI_LCDOptions();
            cviOptions.ConvertFromExternal(options);
            IntPtr report = VisionDll.imaqReadLCD(image._image, roi._roi, ref cviOptions);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<LcdReport, CVI_LCDReport>(report, true);
        }
        #endregion

        #region Regions of Interest Manipulation functions
        //==========================================================================================
        /// <summary>
        /// Transforms a region of interest (ROI) into a mask image. 
        /// </summary>
        /// <param name="destination">
        /// The resulting mask image. 
        /// </param>
        /// <param name="roi">
        /// The ROI to transform.
        /// </param>
        /// <returns>
        /// 	<format type="bold">true</format> if the ROI is a true representation of the mask.
        /// </returns>
        /// <remarks>
        /// 	<format type="italics">destination</format> must be a U8 image.
        /// </remarks>

        public static bool RoiToMask(VisionImage destination, Roi roi)
        {
            return RoiToMask(destination, roi, new PixelValue(255), null);
        }
        //==========================================================================================
        /// <summary>
        /// Transforms a region of interest (ROI) into a mask image. 
        /// </summary>
        /// <param name="destination">
        /// The resulting mask image. 
        /// </param>
        /// <param name="roi">
        /// The ROI to transform.
        /// </param>
        /// <param name="fillValue">
        /// The pixel value of the mask. All pixels inside the region of interest take this value.
        /// The default is 255.
        /// </param>
        /// <returns>
        /// 	<format type="bold">true</format> if the ROI is a true representation of the mask.
        /// </returns>
        /// <remarks>
        /// 	<format type="italics">destination</format> must be a U8 image.
        /// </remarks>

        public static bool RoiToMask(VisionImage destination, Roi roi, PixelValue fillValue)
        {
            return RoiToMask(destination, roi, fillValue, null);
        }
        //==========================================================================================
        /// <summary>
        /// Transforms a region of interest (ROI) into a mask image. 
        /// </summary>
        /// <param name="destination">
        /// The resulting mask image. 
        /// </param>
        /// <param name="roi">
        /// The ROI to transform.
        /// </param>
        /// <param name="fillValue">
        /// The pixel value of the mask. All pixels inside the region of interest take this value.
        /// The default is 255.
        /// </param>
        /// <param name="imageModel">
        /// An optional template for the destination mask image. This parameter can be any image type that NI Vision 
        /// supports. If you supply an <format type="italics">imageModel</format>, the mask image is the size of the 
        /// model. If you set <format type="italics">imageModel</format> to null or Nothing, the size of mask is 
        /// equal to the size of the bounding rectangle of the ROI, which reduces the amount of memory used. 
        /// The method sets the offset of the mask image to reflect the real position of the ROI. 
        /// </param>
        /// <returns>
        /// 	<format type="bold">true</format> if the ROI is a true representation of the mask.
        /// </returns>
        /// <remarks>
        /// 	<format type="italics">destination</format> must be a U8 image.
        /// <para>
        /// You can use this method in two ways. The simplest technique is to supply the 
        /// <format type="italics">imageModel</format> parameter. In this case you can use the source image, in 
        /// which the ROI was drawn, as a template for the final destination image by supplying 
        /// <format type="italics">imageModel</format>. The output image automatically acquires the size of the 
        /// image and location of the ROI as found in the original source image.
        /// </para>
        /// 	<para>
        /// However, you do not have to supply the <format type="italics">imageModel</format> parameter. In this 
        /// case the ROI requires an offset that is determined automatically from the upper-left corner of the 
        /// bounding rectangle described by the ROI. These offset values are automatically set to the image mask.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim i As New VisionImage
        ///     
        /// 'Convert the ROI selected on Viewer1 into a mask and
        /// 'store the result in i.
        /// Algorithms.RoiToMask (imageViewer1.Image, imageViewer1.Roi)
        ///     
        /// 'Convert the ROI selected on Viewer1 into a mask using
        /// 'the image in Viewer1 as a model and store the resulting
        /// 'mask image in the image in Viewer2.
        /// Algorithms.RoiToMask (imageViewer2.Image, imageViewer1.Roi, New PixelValue (255), imageViewer1.Image)
        /// </code>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// VisionImage i = new VisionImage();
        ///     
        /// //Convert the ROI selected on Viewer1 into a mask and
        /// //store the result in i.
        /// Algorithms.RoiToMask(imageViewer1.Image, imageViewer1.Roi);
        ///     
        /// //Convert the ROI selected on Viewer1 into a mask using
        /// //the image in Viewer1 as a model and store the resulting
        /// //mask image in the image in Viewer2.
        /// Algorithms.RoiToMask(imageViewer2.Image, imageViewer1.Roi, new PixelValue(255), imageViewer1.Image);
        /// </code>
        /// </example>

        public static bool RoiToMask(VisionImage destination, Roi roi, PixelValue fillValue, VisionImage imageModel)
        {
            if (destination == null) { throw new ArgumentNullException("destination"); }
            destination.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(imageModel);
            int inSpace;
            Utilities.ThrowError(VisionDll.imaqROIToMask(destination._image, roi._roi, (Int32)fillValue.Grayscale , VisionImage.GetIntPtr(imageModel), out inSpace));
            return (inSpace != 0);
        }
        //==========================================================================================
        /// <summary>
        /// Transforms a mask image into a region of interest (ROI) descriptor. 
        /// </summary>
        /// <param name="mask">
        /// The mask image that the method transforms into an ROI. <format type="italics">mask</format> must be a U8 image.
        /// </param>
        /// <returns>
        /// A <see cref="NationalInstruments.Vision.Analysis.MaskToRoiReport" crefType="Unqualified"/> object that contains the
        /// ROI and indicates whether the ROI is a true representation of the mask.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Get an ROI from the mask image in viewer1.
        /// Dim roi As Roi = Algorithms.MaskToRoiReport (imageViewer1.Image).Roi
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Get an ROI from the mask image in viewer1.
        /// Roi roi = Algorithms.MaskToRoiReport(imageViewer1.Image).Roi;
        /// </code>
        /// </example>

        public static MaskToRoiReport MaskToRoi(VisionImage mask)
        {
            if (mask == null) { throw new ArgumentNullException("mask"); }
            mask.ThrowIfDisposed();
            int withinLimit;
            IntPtr roi = VisionDll.imaqMaskToROI(mask._image, out withinLimit);
            Utilities.ThrowError(roi);
            // Package up into a report, but be sure to dispose the Roi.
            MaskToRoiReport report = new MaskToRoiReport(new Roi(roi), withinLimit != 0);
            VisionDll.imaqDispose(roi);
            return report;
        }
        //==========================================================================================
        /// <summary>
        /// Calculates the profile of the pixels along the edge of each contour in a region of interest (ROI). 
        /// </summary>
        /// <param name="image">
        /// The image from which the method gets the profile. 
        /// </param>
        /// <param name="roi">
        /// The ROI describing the pixels about which the method gets information. 
        /// </param>
        /// <returns>
        /// An <see cref="NationalInstruments.Vision.Analysis.RoiProfileReport" crefType="Unqualified"/> object containing
        /// information about the points along the edge of each contour in the ROI.
        /// </returns>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Find the profile on the image in Viewer1 along
        /// 'the boundary of the regions selected on Viewer1.
        /// Dim Report As RoiProfileReport = Algorithms.RoiProfile (imageViewer1.Image, imageViewer1.Roi)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// //Find the profile on the image in Viewer1 along
        /// //the boundary of the regions selected on Viewer1.
        /// RoiProfileReport report = Algorithms.RoiProfile(imageViewer1.Image, imageViewer1.Roi);
        /// </code>
        /// </example>

        public static RoiProfileReport RoiProfile(VisionImage image, Roi roi)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            IntPtr profile = VisionDll.imaqROIProfile(image._image, roi._roi);
            Utilities.ThrowError(profile);
            return Utilities.ConvertIntPtrToStructure<RoiProfileReport, CVI_ROIProfile>(profile, true);
        }
        //==========================================================================================
        /// <summary>
        /// Rotates and translates a region of interest (ROI) from one coordinate system to another coordinate 
        /// system within an image. 
        /// </summary>
        /// <param name="roi">
        /// The ROI to transform.
        /// </param>
        /// <param name="transform">
        /// Specifies how to transform pixel coordinates based on the difference between the reference 
        /// coordinate system and the measurement coordinate system.
        /// </param>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// Dim coordinateSystem As New CoordinateTransform
        ///  
        /// ' Transform the regions on Viewer1 and store the result in the ROI.
        /// Dim roi As New Roi(imageViewer1.Roi)
        /// Algorithms.TransformRoi (roi, coordinateSystem)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// CoordinateTransform coordinateSystem = new CoordinateTransform();
        ///  
        /// // Transform the regions on Viewer1 and store the result in the ROI.
        /// Roi roi = new Roi(imageViewer1.Roi);
        /// Algorithms.TransformRoi(roi, coordinateSystem);
        /// </code>
        /// </example>

        public static void TransformRoi(Roi roi, CoordinateTransform transform)
        {
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (transform == null) { throw new ArgumentNullException("transform"); }
            CVI_CoordinateSystem cviBase = new CVI_CoordinateSystem();
            cviBase.ConvertFromExternal(transform.ReferenceSystem);
            CVI_CoordinateSystem cviNew = new CVI_CoordinateSystem();
            cviNew.ConvertFromExternal(transform.MeasurementSystem);
            Utilities.ThrowError(VisionDll.imaqTransformROI2(roi._roi, ref cviBase, ref cviNew));
            // Now the Roi has changed - tell it to update from the DLL's information.
            roi.PointerUpdated();
        }

        //==========================================================================================
        /// <summary>
        /// Converts a label image into an ROI. Multiple contours are added to an ROI if more than 
        /// one particle is present for the respective label.
        /// </summary>
        /// <param name="image">
        /// The image containing the image label that is transformed into a region of interest. 
        /// </param>
        /// <returns>
        /// 	<format type="bold">LabelToRoiReport</format> having collection of ROIs, labels and 
        /// 	TooManyVectors Array.
        /// </returns>

        public static LabelToROIReport LabelToRoi(VisionImage image)
        {
            return LabelToRoi(image, new Collection<UInt32>(), 2500, true);
        }
        //==========================================================================================
        /// <summary>
        /// Converts a label image into an ROI. Multiple contours are added to an ROI if more than 
        /// one particle is present for the respective label.
        /// </summary>
        /// <param name="image">
        /// The image containing the image label that is transformed into a region of interest. 
        /// </param>
        /// <param name="labelsIn">
        /// LabelsIn is a collection of label numbers. The label number must match the labels 
        /// in the label image.
        /// </param>
        /// <returns>
        /// 	<format type="bold">LabelToRoiReport</format> having collection of ROIs, labels and 
        /// 	TooManyVectors Array.
        /// </returns>

        public static LabelToROIReport LabelToRoi(VisionImage image, Collection<UInt32> labelsIn)
        {
            return LabelToRoi(image, labelsIn, 2500, true);
        }
        //==========================================================================================
        /// <summary>
        /// Converts a label image into an ROI. Multiple contours are added to an ROI if more than 
        /// one particle is present for the respective label.
        /// </summary>
        /// <param name="image">
        /// The image containing the image label that is transformed into a region of interest. 
        /// </param>
        /// <param name="labelsIn">
        /// LabelsIn is a collection of label numbers. The label number must match the labels 
        /// in the label image.
        /// </param>
        /// <param name="maxNumberOfVectors">
        /// Max number of vectors in ROI is the limit of points that define the contour of a 
        /// region of interest. The default is 2500.
        /// </param>
        /// <returns>
        /// 	<format type="bold">LabelToRoiReport</format> having collection of ROIs, labels and 
        /// 	TooManyVectors Array.
        /// </returns>

        public static LabelToROIReport LabelToRoi(VisionImage image, Collection<UInt32> labelsIn, int maxNumberOfVectors)
        {
            return LabelToRoi(image, labelsIn, maxNumberOfVectors, true);
        }
        //==========================================================================================
        /// <summary>
        /// Converts a label image into an ROI. Multiple contours are added to an ROI if more than 
        /// one particle is present for the respective label.
        /// </summary>
        /// <param name="image">
        /// The image containing the image label that is transformed into a region of interest. 
        /// </param>
        /// <param name="labelsIn">
        /// LabelsIn is a collection of label numbers. The label number must match the labels 
        /// in the label image.
        /// </param>
        /// <param name="maxNumberOfVectors">
        /// Max number of vectors in ROI is the limit of points that define the contour of a 
        /// region of interest. The default is 2500.
        /// </param>
        /// <param name="isExternalEdges">
        /// It specifies whether only the external edges are transformed. The default is TRUE. 
        /// </param>
        /// <returns>
        /// 	<format type="bold">LabelToRoiReport</format> having collection of ROIs, labels and 
        /// 	TooManyVectors Array. The TooManyVectors is a Collection of bools to indicate 
        /// 	specific cases where number of points in a contour exceeded the maximum limit
        /// </returns>

        public static LabelToROIReport LabelToRoi(VisionImage image, Collection<UInt32> labelsIn, int maxNumberOfVectors, bool isExternalEdges)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (labelsIn == null) { throw new ArgumentNullException("labelsIn"); }
            int labelsInSize = labelsIn.Count;
            IntPtr labelsInPtr = IntPtr.Zero;
            if (labelsInSize > 0)
            {
                labelsInPtr = Utilities.ConvertCollectionToIntPtr<UInt32>(labelsIn);
            }
            int externalEdges = isExternalEdges ? 1 : 0;
            IntPtr result = VisionDll.imaqLabelToROI(image._image, labelsInPtr, labelsInSize, maxNumberOfVectors, externalEdges);
            Utilities.ThrowError(result);
            LabelToROIReport toReturn = Utilities.ConvertIntPtrToStructure<LabelToROIReport, CVI_LabelToROIReport>(result, true);
            return toReturn;
        }
        #endregion

        #region Inspection functions
        //==========================================================================================
        /// <summary>
        /// Compares a template image to an inspection image at a specified alignment.
        /// </summary>
        /// <param name="image">The image to inspect.
        /// </param>
        /// <param name="goldenTemplate">The golden template to compare against the inspection image.
        /// </param>
        /// <param name="defects">A reference to the destination image for bright and dark defects.
        /// </param>
        /// <param name="alignment">The location within the inspection image where the template image is located.
        /// </param>
        /// <remarks>
        /// Use one of the following methods of configuring the template image you want to use with this method:
        /// <list type="bullet">
        /// 		<item>
        /// 			<description>
        /// Load a previously saved template image that you created using the NI Vision Template Editor.
        /// <note type="note">
        /// To use the NI Vision Template Editor to configure a template image, click <format type="bold">Start<entity value="#0187"/>All Programs<entity value="#0187"/>National Instruments<entity value="#0187"/>Vision<entity value="#0187"/>Template Editor</format>
        /// 				</note>
        /// 			</description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// Use a <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGoldenTemplate" crefType="Unqualified"/> method to learn the golden template image.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// 	<para>
        /// Use this method with U8 images.
        /// </para>
        /// </remarks>

        public static void CompareGoldenTemplate(VisionImage image, VisionImage goldenTemplate, VisionImage defects, InspectionAlignment alignment)
        {
            CompareGoldenTemplate(image, goldenTemplate, defects, defects, alignment, new InspectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Compares a template image to an inspection image at a specified alignment.
        /// </summary>
        /// <param name="image">The image to inspect.
        /// </param>
        /// <param name="goldenTemplate">The golden template to compare against the inspection image.
        /// </param>
        /// <param name="brightDefects">A reference to the destination image for bright defects.
        /// </param>
        /// <param name="darkDefects">A reference to the destination image for dark defects.
        /// </param>
        /// <param name="alignment">The location within the inspection image where the template image is located.
        /// </param>
        /// <remarks>
        /// Use one of the following methods of configuring the template image you want to use with this method:
        /// <list type="bullet">
        /// 		<item>
        /// 			<description>
        /// Load a previously saved template image that you created using the NI Vision Template Editor.
        /// <note type="note">
        /// To use the NI Vision Template Editor to configure a template image, click <format type="bold">Start<entity value="#0187"/>All Programs<entity value="#0187"/>National Instruments<entity value="#0187"/>Vision<entity value="#0187"/>Template Editor</format>
        /// 				</note>
        /// 			</description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// Use a <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGoldenTemplate" crefType="Unqualified"/> method to learn the golden template image.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// 	<para>
        /// Use this method with U8 images.
        /// </para>
        /// </remarks>

        public static void CompareGoldenTemplate(VisionImage image, VisionImage goldenTemplate, VisionImage brightDefects, VisionImage darkDefects, InspectionAlignment alignment)
        {
            CompareGoldenTemplate(image, goldenTemplate, brightDefects, darkDefects, alignment, new InspectionOptions());
        }
        //==========================================================================================
        /// <summary>
        /// Compares a template image to an inspection image at a specified alignment.
        /// </summary>
        /// <param name="image">The image to inspect.
        /// </param>
        /// <param name="goldenTemplate">The golden template to compare against the inspection image.
        /// </param>
        /// <param name="defects">The destination image for bright and dark defects.
        /// </param>
        /// <param name="alignment">The location within the inspection image where the template image is located.
        /// </param>
        /// <param name="options">An <see cref="NationalInstruments.Vision.Analysis.InspectionOptions" crefType="Unqualified"/> object that specifies the options to use for comparison.
        /// </param>
        /// <remarks>
        /// Use one of the following methods of configuring the template image you want to use with this method:
        /// <list type="bullet">
        /// 		<item>
        /// 			<description>
        /// Load a previously saved template image that you created using the NI Vision Template Editor.
        /// <note type="note">
        /// To use the NI Vision Template Editor to configure a template image, click <format type="bold">Start<entity value="#0187"/>All Programs<entity value="#0187"/>National Instruments<entity value="#0187"/>Vision<entity value="#0187"/>Template Editor</format>
        /// 				</note>
        /// 			</description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// Use a <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGoldenTemplate" crefType="Unqualified"/> method to learn the golden template image.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// 	<para>
        /// Use this method with U8 images.
        /// </para>
        /// </remarks>

        public static void CompareGoldenTemplate(VisionImage image, VisionImage goldenTemplate, VisionImage defects, InspectionAlignment alignment, InspectionOptions options)
        {
            CompareGoldenTemplate(image, goldenTemplate, defects, defects, alignment, options);
        }
        //==========================================================================================
        /// <summary>
        /// Compares a template image to an inspection image at a specified alignment.
        /// </summary>
        /// <param name="image">The image to inspect.
        /// </param>
        /// <param name="goldenTemplate">The golden template to compare against the inspection image.
        /// </param>
        /// <param name="brightDefects">A reference to the destination image for bright defects.
        /// </param>
        /// <param name="darkDefects">A reference to the destination image for dark defects.
        /// </param>
        /// <param name="alignment">The location within the inspection image where the template image is located.
        /// </param>
        /// <param name="options">An <see cref="NationalInstruments.Vision.Analysis.InspectionOptions" crefType="Unqualified"/> object that specifies the options to use for comparison.
        /// </param>
        /// <remarks>
        /// Use one of the following methods of configuring the template image you want to use with this method:
        /// <list type="bullet">
        /// 		<item>
        /// 			<description>
        /// Load a previously saved template image that you created using the NI Vision Template Editor.
        /// <note type="note">
        /// To use the NI Vision Template Editor to configure a template image, click <format type="bold">Start<entity value="#0187"/>All Programs<entity value="#0187"/>National Instruments<entity value="#0187"/>Vision<entity value="#0187"/>Template Editor</format>
        /// 				</note>
        /// 			</description>
        /// 		</item>
        /// 		<item>
        /// 			<description>
        /// Use a <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGoldenTemplate" crefType="Unqualified"/> method to learn the golden template image.
        /// </description>
        /// 		</item>
        /// 	</list>
        /// 	<para>
        /// Use this method with U8 images.
        /// </para>
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Learn the golden template in viewer1.
        /// Algorithms.LearnGoldenTemplate (imageViewer1.Image)
        /// 'Compare this template with the image in viewer2.
        /// Dim Defects As New VisionImage
        /// Algorithms.CompareGoldenTemplate (imageViewer2.Image, imageViewer1.Image, Defects, New InspectionAlignment ())
        /// 'Find the number of bad pixels.
        /// Dim HistogramReport As HistogramReport = Algorithms.Histogram (Defects)
        /// Dim NumberOfBadPixels As Integer = HistogramReport.NumberOfPixels - HistogramReport.Histogram(0)
        ///  
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Learn the golden template in viewer1.
        /// Algorithms.LearnGoldenTemplate(imageViewer1.Image);
        /// // Compare this template with the image in viewer2.
        /// VisionImage defects = new VisionImage();
        /// Algorithms.CompareGoldenTemplate(imageViewer2.Image, imageViewer1.Image, defects, new InspectionAlignment());
        /// // Find the number of bad pixels.
        /// HistogramReport histogramReport = Algorithms.Histogram(defects);
        /// int numberOfBadPixels = histogramReport.NumberOfPixels - histogramReport.Histogram[0];
        /// </code>
        /// </example>

        public static void CompareGoldenTemplate(VisionImage image, VisionImage goldenTemplate, VisionImage brightDefects, VisionImage darkDefects, InspectionAlignment alignment, InspectionOptions options)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (goldenTemplate == null) { throw new ArgumentNullException("goldenTemplate"); }
            goldenTemplate.ThrowIfDisposed();
            if (alignment == null) { throw new ArgumentNullException("alignment"); }
            if (options == null) { throw new ArgumentNullException("options"); }
            VisionImage.ThrowIfNonNullAndDisposed(brightDefects);
            VisionImage.ThrowIfNonNullAndDisposed(darkDefects);
            CVI_InspectionAlignment cviAlignment = new CVI_InspectionAlignment();
            cviAlignment.ConvertFromExternal(alignment);
            CVI_InspectionOptions cviOptions = new CVI_InspectionOptions();
            cviOptions.ConvertFromExternal(options);
            Utilities.ThrowError(VisionDll.imaqCompareGoldenTemplate(image._image, goldenTemplate._image, VisionImage.GetIntPtr(brightDefects), VisionImage.GetIntPtr(darkDefects), ref cviAlignment, ref cviOptions));
        }

        //==========================================================================================
        /// <summary>
        /// Creates a description of the golden template image you are going to compare against when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.CompareGoldenTemplate" crefType="Unqualified"/> 
        /// method. This description data is appended to the input template image.
        /// </summary>
        /// <param name="image">
        /// The golden template image that you want to compare against during inspection.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGoldenTemplate(VisionImage image)
        {
            LearnGoldenTemplate(image, new PointContour(0, 0), null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the golden template image you are going to compare against when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.CompareGoldenTemplate" crefType="Unqualified"/> 
        /// method. This description data is appended to the input template image.
        /// </summary>
        /// <param name="image">
        /// The golden template image that you want to compare against during inspection.
        /// </param>
        /// <param name="originOffset">
        /// Secifies the number of pixels the method shifts the origin of the template from the center of the 
        /// template image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGoldenTemplate(VisionImage image, PointContour originOffset)
        {
            LearnGoldenTemplate(image, originOffset, null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the golden template image you are going to compare against when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.CompareGoldenTemplate" crefType="Unqualified"/> 
        /// method. This description data is appended to the input template image.
        /// </summary>
        /// <param name="image">
        /// The golden template image that you want to compare against during inspection.
        /// </param>
        /// <param name="originOffset">
        /// The ROI containing the pixels the method shifts the origin of the template from the center of the 
        /// template image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGoldenTemplate(VisionImage image, Roi originOffset)
        {
            LearnGoldenTemplate(image, originOffset, null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the golden template image you are going to compare against when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.CompareGoldenTemplate" crefType="Unqualified"/> 
        /// method. This description data is appended to the input template image.
        /// </summary>
        /// <param name="image">
        /// The golden template image that you want to compare against during inspection.
        /// </param>
        /// <param name="originOffset">
        /// The ROI containing the pixels the method shifts the origin of the template from the center of the 
        /// template image.
        /// </param>
        /// <param name="mask">
        /// An 8-bit image the same size as the template that specifies what regions and edges to ignore in the template. 
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        ///  
        /// 'Learn the golden template in viewer1.
        /// Algorithms.LearnGoldenTemplate (imageViewer1.Image)
        /// 'Compare this template with the image in viewer2.
        /// Dim Defects As New VisionImage
        /// Algorithms.CompareGoldenTemplate (imageViewer2.Image, imageViewer1.Image, Defects, New InspectionAlignment ())
        /// 'Find the number of bad pixels.
        /// Dim HistogramReport As HistogramReport = Algorithms.Histogram (Defects)
        /// Dim NumberOfBadPixels As Integer = HistogramReport.NumberOfPixels - HistogramReport.Histogram(0)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        ///  
        /// // Learn the golden template in viewer1.
        /// Algorithms.LearnGoldenTemplate(imageViewer1.Image);
        /// // Compare this template with the image in viewer2.
        /// VisionImage defects = new VisionImage();
        /// Algorithms.CompareGoldenTemplate(imageViewer2.Image, imageViewer1.Image, defects, new InspectionAlignment());
        /// // Find the number of bad pixels.
        /// HistogramReport histogramReport = Algorithms.Histogram(defects);
        /// int numberOfBadPixels = histogramReport.NumberOfPixels - histogramReport.Histogram[0];
        /// </code>
        /// </example>

        public static void LearnGoldenTemplate(VisionImage image, Roi originOffset, VisionImage mask)
        {
            if (originOffset == null) { throw new ArgumentNullException("originOffset"); }
            originOffset.ThrowIfDisposed();
            Utilities.ThrowIfNotSinglePoint(originOffset);
            LearnGoldenTemplate(image, (PointContour)originOffset[0].Shape, mask);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the golden template image you are going to compare against when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.CompareGoldenTemplate" crefType="Unqualified"/> 
        /// method. This description data is appended to the input template image.
        /// </summary>
        /// <param name="image">
        /// The golden template image that you want to compare against during inspection.
        /// </param>
        /// <param name="originOffset">
        /// Secifies the number of pixels the method shifts the origin of the template from the center of the 
        /// template image.
        /// </param>
        /// <param name="mask">
        /// An 8-bit image the same size as the template that specifies what regions and edges to ignore in the template. 
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGoldenTemplate(VisionImage image, PointContour originOffset, VisionImage mask)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (originOffset == null) { throw new ArgumentNullException("originOffset"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            CVI_PointFloat cviOriginOffset = new CVI_PointFloat();
            cviOriginOffset.ConvertFromExternal(originOffset);
            Utilities.ThrowError(VisionDll.imaqLearnGoldenTemplate(image._image, cviOriginOffset, VisionImage.GetIntPtr(mask)));
        }
        #endregion

        #region Geometric Matching functions

        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternFeatureBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGeometricPatternFeatureBased(VisionImage template)
        {
            LearnGeometricPatternFeatureBased(template, new PointContour(), new CurveOptions(), new LearnGeometricPatternFeatureBasedAdvancedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternFeatureBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <param name="originOffset">
        /// Specifies the number of pixels the method shifts the origin of the template from the 
        /// center of the template image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGeometricPatternFeatureBased(VisionImage template, PointContour originOffset)
        {
            LearnGeometricPatternFeatureBased(template, originOffset, new CurveOptions(), new LearnGeometricPatternFeatureBasedAdvancedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternFeatureBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <param name="originOffset">
        /// Specifies the number of pixels the method shifts the origin of the template from the 
        /// center of the template image.
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to create the 
        /// template image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGeometricPatternFeatureBased(VisionImage template, PointContour originOffset, CurveOptions curveOptions)
        {
            LearnGeometricPatternFeatureBased(template, originOffset, curveOptions, new LearnGeometricPatternFeatureBasedAdvancedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternFeatureBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <param name="originOffset">
        /// Specifies the number of pixels the method shifts the origin of the template from the 
        /// center of the template image.
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to create the 
        /// template image.
        /// </param>
        /// <param name="options">
        /// Advanced options for determining the information the algorithm learns about the geometric pattern. 
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGeometricPatternFeatureBased(VisionImage template, PointContour originOffset, CurveOptions curveOptions, LearnGeometricPatternFeatureBasedAdvancedOptions options)
        {
            LearnGeometricPatternFeatureBased(template, originOffset, curveOptions, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternFeatureBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <param name="originOffset">
        /// Specifies the number of pixels the method shifts the origin of the template from the 
        /// center of the template image.
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to create the 
        /// template image.
        /// </param>
        /// <param name="options">
        /// Advanced options for determining the information the algorithm learns about the geometric pattern. 
        /// </param>
        /// <param name="mask">
        /// An image, which is the same size as the template, that specifies where to search for edges in the 
        /// template.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static unsafe void LearnGeometricPatternFeatureBased(VisionImage template, PointContour originOffset, CurveOptions curveOptions, LearnGeometricPatternFeatureBasedAdvancedOptions options, VisionImage mask)
        {
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            CVI_LearnGeometricPatternAdvancedOptions advancedOptions = new CVI_LearnGeometricPatternAdvancedOptions();
            advancedOptions.ConvertFromExternal(options);
            CVI_CurveOptions cviCurveOptions = new CVI_CurveOptions();
            cviCurveOptions.ConvertFromExternal(curveOptions);
            CVI_PointFloat cvioriginOffset = new CVI_PointFloat();
            cvioriginOffset.ConvertFromExternal(originOffset);
            Utilities.ThrowError(VisionDll.imaqLearnGeometricPattern(template._image, cvioriginOffset, ref cviCurveOptions, ref advancedOptions, VisionImage.GetIntPtr(mask)));
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternEdgeBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGeometricPatternEdgeBased(VisionImage template)
        {
            LearnGeometricPatternEdgeBased(template, new PointContour(), 0, new CurveOptions(), new LearnGeometricPatternEdgeBasedAdvancedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternEdgeBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <param name="originOffset">
        /// Specifies the number of pixels the method shifts the origin of the template from the 
        /// center of the template image.
        /// </param>
        /// <param name="angleOffset">
        /// Specifies the angle, in degrees, the method shifts the template from the center of the image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGeometricPatternEdgeBased(VisionImage template, PointContour originOffset, double angleOffset)
        {
            LearnGeometricPatternEdgeBased(template, originOffset, angleOffset, new CurveOptions(), new LearnGeometricPatternEdgeBasedAdvancedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternEdgeBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <param name="originOffset">
        /// Specifies the number of pixels the method shifts the origin of the template from the 
        /// center of the template image.
        /// </param>
        /// <param name="angleOffset">
        /// Specifies the angle, in degrees, the method shifts the template from the center of the image.
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to create the 
        /// template image.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGeometricPatternEdgeBased(VisionImage template, PointContour originOffset, double angleOffset, CurveOptions curveOptions)
        {
            LearnGeometricPatternEdgeBased(template, originOffset, angleOffset, curveOptions, new LearnGeometricPatternEdgeBasedAdvancedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternEdgeBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <param name="originOffset">
        /// Specifies the number of pixels the method shifts the origin of the template from the 
        /// center of the template image.
        /// </param>
        /// <param name="angleOffset">
        /// Specifies the angle, in degrees, the method shifts the template from the center of the image.
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to create the 
        /// template image.
        /// </param>
        /// <param name="options">
        /// Advanced options for determining the information the algorithm learns about the geometric pattern. 
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static void LearnGeometricPatternEdgeBased(VisionImage template, PointContour originOffset, double angleOffset, CurveOptions curveOptions, LearnGeometricPatternEdgeBasedAdvancedOptions options)
        {
            LearnGeometricPatternEdgeBased(template, originOffset, angleOffset, curveOptions, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Creates a description of the template image you are going to look for when using the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.MatchGeometricPatternEdgeBased" crefType="Unqualified"/>
        /// method. This description data is appended to the input template image. During the matching step, 
        /// the description data is extracted from the template image and used to search for the template image 
        /// in the match image.
        /// </summary>
        /// <param name="template">
        /// The image about which the method learns pattern matching information. The method appends the 
        /// pattern matching information to this image. 
        /// </param>
        /// <param name="originOffset">
        /// Specifies the number of pixels the method shifts the origin of the template from the 
        /// center of the template image.
        /// </param>
        /// <param name="angleOffset">
        /// Specifies the angle, in degrees, the method shifts the template from the center of the image.
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to create the 
        /// template image.
        /// </param>
        /// <param name="options">
        /// Advanced options for determining the information the algorithm learns about the geometric pattern. 
        /// </param>
        /// <param name="mask">
        /// An image, which is the same size as the template, that specifies where to search for edges in the 
        /// template.
        /// </param>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Learn the geometric template in viewer1.
        /// 'Set the center of the template to (5, 10).
        /// Algorithms.LearnGeometricPattern (imageViewer1.Image, New PointContour(5, 10), 0)
        /// 'Match the template in viewer2, searching for 4 matches that could be rotated.
        /// Dim MatchOptions As New MatchGeometricPatternEdgeBasedOptions (GeometricMatchModes.ShiftInvariant + GeometricMatchModes.RotationInvariant, 4)
        /// Dim Matches As Collection(Of GeometricEdgeBasedPatternMatch)
        /// Matches = Algorithms.MatchGeometricPatternEdgeBased (imageViewer2.Image, imageViewer1.Image, New CurveOptions (), MatchOptions)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Learn the geometric template in viewer1.
        /// // Set the center of the template to (5, 10).
        /// Algorithms.LearnGeometricPattern(imageViewer1.Image, new PointContour(5, 10), 0);
        /// // Match the template in viewer2, searching for 4 matches that could be rotated.
        /// MatchGeometricPatternEdgeBasedOptions matchOptions = new MatchGeometricPatternEdgeBasedOptions(GeometricMatchModes.ShiftInvariant + GeometricMatchModes.RotationInvariant, 4);
        /// Collection&lt;GeometricEdgeBasedPatternMatch&gt; matches;
        /// matches = Algorithms.MatchGeometricPatternEdgeBased(imageViewer2.Image, imageViewer1.Image, new CurveOptions(), matchOptions);
        /// </code>
        /// </example>

        public static unsafe void LearnGeometricPatternEdgeBased(VisionImage template, PointContour originOffset, double angleOffset, CurveOptions curveOptions, LearnGeometricPatternEdgeBasedAdvancedOptions options, VisionImage mask)
        {
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            VisionImage.ThrowIfNonNullAndDisposed(mask);
            CVI_LearnGeometricPatternAdvancedOptions2 advancedOptions = new CVI_LearnGeometricPatternAdvancedOptions2();
            advancedOptions.ConvertFromExternal(options);
            CVI_CurveOptions cviCurveOptions = new CVI_CurveOptions();
            cviCurveOptions.ConvertFromExternal(curveOptions);
            CVI_PointFloat cvioriginOffset = new CVI_PointFloat();
            cvioriginOffset.ConvertFromExternal(originOffset);
            Utilities.ThrowError(VisionDll.imaqLearnGeometricPattern2(template._image, cvioriginOffset, angleOffset, ref cviCurveOptions, ref advancedOptions, VisionImage.GetIntPtr(mask)));
        }

        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given geometric template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGeometricPatternFeatureBased" crefType="Unqualified"/> 
        /// method prior to using this method to ensure that the template image has been 
        /// configured for the geometric pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The template image to be located during the geometric matching process. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.GeometricFeatureBasedPatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<GeometricFeatureBasedPatternMatch> MatchGeometricPatternFeatureBased(VisionImage image, VisionImage template)
        {
            return MatchGeometricPatternFeatureBased(image, template, new CurveOptions(), new MatchGeometricPatternFeatureBasedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given geometric template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGeometricPatternFeatureBased" crefType="Unqualified"/> 
        /// method prior to using this method to ensure that the template image has been 
        /// configured for the geometric pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The template image to be located during the geometric matching process. 
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to match the template image. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.GeometricFeatureBasedPatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<GeometricFeatureBasedPatternMatch> MatchGeometricPatternFeatureBased(VisionImage image, VisionImage template, CurveOptions curveOptions)
        {
            return MatchGeometricPatternFeatureBased(image, template, curveOptions, new MatchGeometricPatternFeatureBasedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given geometric template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGeometricPatternFeatureBased" crefType="Unqualified"/> 
        /// method prior to using this method to ensure that the template image has been 
        /// configured for the geometric pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The template image to be located during the geometric matching process. 
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to match the template image. 
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.GeometricFeatureBasedPatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<GeometricFeatureBasedPatternMatch> MatchGeometricPatternFeatureBased(VisionImage image, VisionImage template, CurveOptions curveOptions, MatchGeometricPatternFeatureBasedOptions options)
        {
            return MatchGeometricPatternFeatureBased(image, template, curveOptions, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given geometric template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGeometricPatternFeatureBased" crefType="Unqualified"/> 
        /// method prior to using this method to ensure that the template image has been 
        /// configured for the geometric pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The template image to be located during the geometric matching process. 
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to match the template image. 
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image.
        /// </param>
        /// <param name="roi">
        /// Specifies the ROI within an image in which to search for the template image. Pass null or Nothing for this parameter to 
        /// search the entire image. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.GeometricFeatureBasedPatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<GeometricFeatureBasedPatternMatch> MatchGeometricPatternFeatureBased(VisionImage image, VisionImage template, CurveOptions curveOptions, MatchGeometricPatternFeatureBasedOptions options, Roi roi)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            if (curveOptions == null) { throw new ArgumentNullException("curveOptions"); }    
            Roi.ThrowIfNonNullAndDisposed(roi);
            CVI_CurveOptions cvicurveOptions = new CVI_CurveOptions();
            cvicurveOptions.ConvertFromExternal(curveOptions);         
            // Make sure we dispose the CVI_MatchGeometricPatternOptions object
            CVI_MatchGeometricPatternOptions cviOptions = new CVI_MatchGeometricPatternOptions();
            cviOptions.ConvertFromExternal(options);
            try
            {
                CVI_MatchGeometricPatternAdvancedOptions2 cviAdvancedOptions = new CVI_MatchGeometricPatternAdvancedOptions2();
                cviAdvancedOptions.ConvertFromExternal(options.Advanced);
                IntPtr numMatches;
                IntPtr report = VisionDll.imaqMatchGeometricPattern2(image._image, template._image, ref cvicurveOptions, ref cviOptions, ref cviAdvancedOptions, Roi.GetIntPtr(roi), out numMatches);
                Utilities.ThrowError(report);
                Collection<GeometricFeatureBasedPatternMatch> toReturn = Utilities.ConvertIntPtrToCollection<GeometricFeatureBasedPatternMatch, CVI_GeometricPatternMatch2>(report, numMatches, true);
                return toReturn;
            }
            finally
            {
                cviOptions.Dispose();
            }
        }

        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given geometric template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGeometricPatternEdgeBased" crefType="Unqualified"/> 
        /// method prior to using this method to ensure that the template image has been 
        /// configured for the geometric pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The template image to be located during the geometric matching process. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.GeometricEdgeBasedPatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<GeometricEdgeBasedPatternMatch> MatchGeometricPatternEdgeBased(VisionImage image, VisionImage template)
        {
            return MatchGeometricPatternEdgeBased(image, template, new CurveOptions(),  new MatchGeometricPatternEdgeBasedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given geometric template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGeometricPatternEdgeBased" crefType="Unqualified"/> 
        /// method prior to using this method to ensure that the template image has been 
        /// configured for the geometric pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The template image to be located during the geometric matching process. 
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to match the template image. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.GeometricEdgeBasedPatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<GeometricEdgeBasedPatternMatch> MatchGeometricPatternEdgeBased(VisionImage image, VisionImage template, CurveOptions curveOptions)
        {
            return MatchGeometricPatternEdgeBased(image, template, curveOptions, new MatchGeometricPatternEdgeBasedOptions(), null);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given geometric template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGeometricPatternEdgeBased" crefType="Unqualified"/> 
        /// method prior to using this method to ensure that the template image has been 
        /// configured for the geometric pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The template image to be located during the geometric matching process. 
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to match the template image. 
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image.
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.GeometricEdgeBasedPatternMatch" crefType="Unqualified"/> 
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static Collection<GeometricEdgeBasedPatternMatch> MatchGeometricPatternEdgeBased(VisionImage image, VisionImage template, CurveOptions curveOptions, MatchGeometricPatternEdgeBasedOptions options)
        {
            return MatchGeometricPatternEdgeBased(image, template, curveOptions, options, null);
        }
        //==========================================================================================
        /// <summary>
        /// Searches for areas in an image that match a given geometric template image. Use the 
        /// <see cref="NationalInstruments.Vision.Analysis.Algorithms.LearnGeometricPatternEdgeBased" crefType="Unqualified"/> 
        /// method prior to using this method to ensure that the template image has been 
        /// configured for the geometric pattern match stage.
        /// </summary>
        /// <param name="image">
        /// The image in which the method finds matches to the template image. 
        /// </param>
        /// <param name="template">
        /// The template image to be located during the geometric matching process. 
        /// </param>
        /// <param name="curveOptions">
        /// Describes how the method identifies the curves in the image the method will use to match the template image. 
        /// </param>
        /// <param name="options">
        /// Describes how to search for the template image.
        /// </param>
        /// <param name="roi">
        /// Specifies the ROI within an image in which to search for the template image. Pass null or Nothing for this parameter to 
        /// search the entire image. 
        /// </param>
        /// <returns>
        /// A collection of <see cref="NationalInstruments.Vision.Analysis.GeometricEdgeBasedPatternMatch" crefType="Unqualified"/>
        /// objects containing information about each match found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>
        /// <example>
        /// 	<code lang="VB.NET">
        /// Imports NationalInstruments.Vision
        /// Imports NationalInstruments.Vision.Analysis
        /// Imports System.Collections.ObjectModel
        ///  
        /// 'Learn the geometric template in viewer1.
        /// 'Set the center of the template to (5, 10).
        /// Algorithms.LearnGeometricPattern (imageViewer1.Image, New PointContour(5, 10), 0)
        /// 'Match the template in viewer2, searching for 4 matches that could be rotated.
        /// Dim MatchOptions As New MatchGeometricPatternEdgeBasedOptions (GeometricMatchModes.ShiftInvariant + GeometricMatchModes.RotationInvariant, 4)
        /// Dim Matches As Collection(Of GeometricEdgeBasedPatternMatch)
        /// Matches = Algorithms.MatchGeometricPatternEdgeBased (imageViewer2.Image, imageViewer1.Image, New CurveOptions (), MatchOptions)
        /// </code>
        /// </example>
        /// <example>
        /// 	<code lang="C#">
        /// using NationalInstruments.Vision;
        /// using NationalInstruments.Vision.Analysis;
        /// using System.Collections.ObjectModel;
        ///  
        /// // Learn the geometric template in viewer1.
        /// // Set the center of the template to (5, 10).
        /// Algorithms.LearnGeometricPattern(imageViewer1.Image, new PointContour(5, 10), 0);
        /// // Match the template in viewer2, searching for 4 matches that could be rotated.
        /// MatchGeometricPatternEdgeBasedOptions matchOptions = new MatchGeometricPatternEdgeBasedOptions(GeometricMatchModes.ShiftInvariant + GeometricMatchModes.RotationInvariant, 4);
        /// Collection&lt;GeometricEdgeBasedPatternMatch&gt; matches;
        /// matches = Algorithms.MatchGeometricPatternEdgeBased(imageViewer2.Image, imageViewer1.Image, new CurveOptions(), matchOptions);
        /// </code>
        /// </example>

        public static Collection<GeometricEdgeBasedPatternMatch> MatchGeometricPatternEdgeBased(VisionImage image, VisionImage template, CurveOptions curveOptions, MatchGeometricPatternEdgeBasedOptions options, Roi roi)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (template == null) { throw new ArgumentNullException("template"); }
            template.ThrowIfDisposed();
            if (options == null) { throw new ArgumentNullException("options"); }
            if (curveOptions == null) { throw new ArgumentNullException("curveOptions"); }    
            Roi.ThrowIfNonNullAndDisposed(roi);
            CVI_CurveOptions cvicurveOptions = new CVI_CurveOptions();
            cvicurveOptions.ConvertFromExternal(curveOptions);         
            // Make sure we dispose the CVI_MatchGeometricPatternOptions object
            CVI_MatchGeometricPatternOptions cviOptions = new CVI_MatchGeometricPatternOptions();
            cviOptions.ConvertFromExternal(options);
            try
            {
                CVI_MatchGeometricPatternAdvancedOptions3 cviAdvancedOptions = new CVI_MatchGeometricPatternAdvancedOptions3();
                cviAdvancedOptions.ConvertFromExternal(options.Advanced);
                IntPtr numMatches;
                IntPtr report = VisionDll.imaqMatchGeometricPattern3(image._image, template._image, ref cvicurveOptions, ref cviOptions, ref cviAdvancedOptions, Roi.GetIntPtr(roi), out numMatches);
                Utilities.ThrowError(report);
                Collection<GeometricEdgeBasedPatternMatch> toReturn = Utilities.ConvertIntPtrToCollection<GeometricEdgeBasedPatternMatch, CVI_GeometricPatternMatch3>(report, numMatches, true);
                return toReturn;
            }
            finally
            {
                cviOptions.Dispose();
            }
        }
        #endregion

        #region Texture Functions
        //==========================================================================================
        /// <summary>
        /// Extracts texture features from the image. Features are extracted by computing the wavelet 
        /// bands from the image and then computing co-occurrence Haralick features from the extracted bands. 
        /// The features are extracted within a specified window and the window is moved across the 
        /// image from the top-left to the right-bottom corner.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <returns>
        /// An instance of TextureFeaturesReport having wavelet Bands used and TextureFeatures extacted.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static TextureFeaturesReport ExtractTextureFeatures(VisionImage sourceImage)
        {
            return ExtractTextureFeatures(sourceImage, new Roi(), new WindowOptions(), new WaveletOptions(), new Collection<int>(), new CooccurrenceOptions(), true);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts texture features from the image. Features are extracted by computing the wavelet 
        /// bands from the image and then computing co-occurrence Haralick features from the extracted bands. 
        /// The features are extracted within a specified window and the window is moved across the 
        /// image from the top-left to the right-bottom corner.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the texture features.
        /// </param>
        /// <returns>
        /// An instance of TextureFeaturesReport having wavelet Bands used and TextureFeatures extacted.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static TextureFeaturesReport ExtractTextureFeatures(VisionImage sourceImage, Roi roi)
        {
            return ExtractTextureFeatures(sourceImage, roi, new WindowOptions(), new WaveletOptions(), new Collection<int>(), new CooccurrenceOptions(), true);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts texture features from the image. Features are extracted by computing the wavelet 
        /// bands from the image and then computing co-occurrence Haralick features from the extracted bands. 
        /// The features are extracted within a specified window and the window is moved across the 
        /// image from the top-left to the right-bottom corner.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the texture features.
        /// </param>
        /// <param name="windowOptions">
        /// Window Options specifies the options for the window within which features are extracted 
        /// and specifies how the window is moved across the image.
        /// </param>
        /// <returns>
        /// An instance of TextureFeaturesReport having wavelet Bands used and TextureFeatures extacted.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static TextureFeaturesReport ExtractTextureFeatures(VisionImage sourceImage, Roi roi, WindowOptions windowOptions)
        {
            return ExtractTextureFeatures(sourceImage, roi, windowOptions, new WaveletOptions(), new Collection<int>(), new CooccurrenceOptions(), true);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts texture features from the image. Features are extracted by computing the wavelet 
        /// bands from the image and then computing co-occurrence Haralick features from the extracted bands. 
        /// The features are extracted within a specified window and the window is moved across the 
        /// image from the top-left to the right-bottom corner.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the texture features.
        /// </param>
        /// <param name="windowOptions">
        /// Window Options specifies the options for the window within which features are extracted 
        /// and specifies how the window is moved across the image.
        /// </param>
        /// <param name="waveletOptions">
        /// Wavelet Options specifes how the VI extracts the wavelet bands that are used to compute texture features.
        /// </param>
        /// <returns>
        /// An instance of TextureFeaturesReport having wavelet Bands used and TextureFeatures extacted.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static TextureFeaturesReport ExtractTextureFeatures(VisionImage sourceImage, Roi roi, WindowOptions windowOptions, WaveletOptions waveletOptions)
        {
            return ExtractTextureFeatures(sourceImage, roi, windowOptions, waveletOptions, new Collection<int>(), new CooccurrenceOptions(), true);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts texture features from the image. Features are extracted by computing the wavelet 
        /// bands from the image and then computing co-occurrence Haralick features from the extracted bands. 
        /// The features are extracted within a specified window and the window is moved across the 
        /// image from the top-left to the right-bottom corner.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the texture features.
        /// </param>
        /// <param name="windowOptions">
        /// Window Options specifies the options for the window within which features are extracted 
        /// and specifies how the window is moved across the image.
        /// </param>
        /// <param name="waveletOptions">
        /// Wavelet Options specifes how the VI extracts the wavelet bands that are used to compute texture features.
        /// </param>
        /// <param name="waveletBands">
        /// Wavelet Bands collection specifes the wavelet bands that are used to compute texture features.
        /// </param>
        /// <returns>
        /// An instance of TextureFeaturesReport having wavelet Bands used and TextureFeatures extacted.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static TextureFeaturesReport ExtractTextureFeatures(VisionImage sourceImage, Roi roi, WindowOptions windowOptions, WaveletOptions waveletOptions, Collection<Int32> waveletBands)
        {
            return ExtractTextureFeatures(sourceImage, roi, windowOptions, waveletOptions, waveletBands, new CooccurrenceOptions(), true);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts texture features from the image. Features are extracted by computing the wavelet 
        /// bands from the image and then computing co-occurrence Haralick features from the extracted bands. 
        /// The features are extracted within a specified window and the window is moved across the 
        /// image from the top-left to the right-bottom corner.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the texture features.
        /// </param>
        /// <param name="windowOptions">
        /// Window Options specifies the options for the window within which features are extracted 
        /// and specifies how the window is moved across the image.
        /// </param>
        /// <param name="waveletOptions">
        /// Wavelet Options specifes how the VI extracts the wavelet bands that are used to compute texture features.
        /// </param>
        /// <param name="waveletBands">
        /// Wavelet Bands collection specifes the wavelet bands that are used to compute texture features.
        /// </param>
        /// <param name="CooccurrenceOptions">
        /// Co-occurrence Options specifies the options for computing the co-occurrence matrix of each window in the image. 
        /// The Haralick features are then extracted from the co-occurrence matrix.
        /// </param>
        /// <returns>
        /// An instance of TextureFeaturesReport having wavelet Bands used and TextureFeatures extacted.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static TextureFeaturesReport ExtractTextureFeatures(VisionImage sourceImage, Roi roi, WindowOptions windowOptions, WaveletOptions waveletOptions, Collection<Int32> waveletBands, CooccurrenceOptions cooccurrenceOptions)
        {
            return ExtractTextureFeatures(sourceImage, roi, windowOptions, waveletOptions, waveletBands, cooccurrenceOptions, true);
        }
        //==========================================================================================
        /// <summary>
        /// Extracts texture features from the image. Features are extracted by computing the wavelet 
        /// bands from the image and then computing co-occurrence Haralick features from the extracted bands. 
        /// The features are extracted within a specified window and the window is moved across the 
        /// image from the top-left to the right-bottom corner.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the texture features.
        /// </param>
        /// <param name="windowOptions">
        /// Window Options specifies the options for the window within which features are extracted 
        /// and specifies how the window is moved across the image.
        /// </param>
        /// <param name="waveletOptions">
        /// Wavelet Options specifes how the VI extracts the wavelet bands that are used to compute texture features.
        /// </param>
        /// <param name="waveletBands">
        /// Wavelet Bands collection specifes the wavelet bands that are used to compute texture features.
        /// </param>
        /// <param name="cooccurrenceOptions">
        /// Co-occurrence Options specifies the options for computing the co-occurrence matrix of each window in the image. 
        /// The Haralick features are then extracted from the co-occurrence matrix.
        /// </param>
        /// <param name="useWindow">
        /// Boolean to indicate wehether to use windows for extraction or not
        /// </param>
        /// <returns>
        /// An instance of TextureFeaturesReport having wavelet Bands used and TextureFeatures extacted.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static TextureFeaturesReport ExtractTextureFeatures(VisionImage sourceImage, Roi roi, WindowOptions windowOptions, WaveletOptions waveletOptions, Collection<Int32> waveletBands, CooccurrenceOptions cooccurrenceOptions, bool useWindow)
        {
            if (sourceImage == null) { throw new ArgumentNullException("sourceImage"); }
            sourceImage.ThrowIfDisposed();
            Roi.ThrowIfNonNullAndDisposed(roi);
            if (windowOptions == null) { throw new ArgumentNullException("windowOptions"); }
            if (waveletOptions == null) { throw new ArgumentNullException("waveletOptions"); }
            if (waveletBands == null) { throw new ArgumentNullException("waveletBands"); }
            if (cooccurrenceOptions == null) { throw new ArgumentNullException("cooccurrenceOptions"); }
            CVI_WindowOptions cviWindowOptions = new CVI_WindowOptions();
            cviWindowOptions.ConvertFromExternal(windowOptions);
            CVI_WaveletOptions cviWaveletOptions = new CVI_WaveletOptions();
            cviWaveletOptions.ConvertFromExternal(waveletOptions);
            CVI_CooccurrenceOptions cviCooccurrenceOptions = new CVI_CooccurrenceOptions();
            cviCooccurrenceOptions.ConvertFromExternal(cooccurrenceOptions);
            char useWindowOption = (char)(useWindow ? 1 : 0);
            Int32[] waveletBandsArray = new Int32[waveletBands.Count];
            Int32 numWaveletBands = 0;
            if (waveletBands.Count != 0)
            {
                foreach (Int32 val in waveletBands)
                {
                    waveletBandsArray.SetValue(val, numWaveletBands++);
                }
            }
            IntPtr waveletBandsPtr = Utilities.ConvertArrayToIntPtr<Int32>(waveletBandsArray);
            IntPtr result = VisionDll.imaqExtractTextureFeatures(sourceImage._image, Roi.GetIntPtr(roi), ref cviWindowOptions, ref cviWaveletOptions, waveletBandsPtr, numWaveletBands, ref cviCooccurrenceOptions, useWindowOption);
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToStructure<TextureFeaturesReport, CVI_TextureFeaturesReport>(result, true);
        }

        //==========================================================================================
        /// <summary>
        /// Extracts wavelet bands from the image.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <returns>
        /// Returns the Report having all the bands retrieved and the dimensions of the bands.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static WaveletBandsReport ExtractWaveletBands(VisionImage sourceImage)
        {
            return ExtractWaveletBands(sourceImage, new WaveletOptions(), new Collection<int>());
        }
        //==========================================================================================
        /// <summary>
        /// Extracts wavelet bands from the image.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="waveletOptions">
        /// Wavelet Options specifes how the VI extracts the wavelet bands that are used to compute texture features.
        /// </param>
        /// <returns>
        /// Returns the Report having all the bands retrieved and the dimensions of the bands.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static WaveletBandsReport ExtractWaveletBands(VisionImage sourceImage, WaveletOptions waveletOptions)
        {
            return ExtractWaveletBands(sourceImage, waveletOptions, new Collection<int>());
        }
        //==========================================================================================
        /// <summary>
        /// Extracts wavelet bands from the image.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="waveletOptions">
        /// Wavelet Options specifes how the VI extracts the wavelet bands that are used to compute texture features.
        /// </param>
        /// <param name="waveletBands">
        /// Wavelet Bands Used specifies the wavelet bands used to compute the features.
        /// </param>
        /// <returns>
        /// Returns the Report having all the bands retrieved and the dimensions of the bands.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static WaveletBandsReport ExtractWaveletBands(VisionImage sourceImage, WaveletOptions waveletOptions, Collection<int> waveletBands)
        {
            if (sourceImage == null) { throw new ArgumentNullException("sourceImage"); }
            sourceImage.ThrowIfDisposed();
            if (waveletOptions == null) { throw new ArgumentNullException("waveletOptions"); }
            Int32[] waveletBandsArray = new Int32[waveletBands.Count];
            Int32 numWaveletBands = 0;
            if (waveletBands.Count != 0)
            {
                foreach (Int32 val in waveletBands)
                {
                    waveletBandsArray.SetValue(val, numWaveletBands++);
                }
            }
            IntPtr waveletBandsPtr = Utilities.ConvertArrayToIntPtr<Int32>(waveletBandsArray);
            CVI_WaveletOptions cviWaveletOptions = new CVI_WaveletOptions();
            cviWaveletOptions.ConvertFromExternal(waveletOptions);
            IntPtr result = VisionDll.imaqExtractWaveletBands(sourceImage._image, ref cviWaveletOptions, waveletBandsPtr, numWaveletBands);
            Utilities.ThrowError(result);
            return Utilities.ConvertIntPtrToStructure<WaveletBandsReport, CVI_WaveletBandsReport>(result, true);
        }

        //==========================================================================================
        /// <summary>
        /// Performs the Cooccurrence Matrix Texture Analysis.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="levelPixel">
        /// Cooccurrence Level specifies the number of gray levels the image is quantized to before the cooccurrence
        /// matrix is computed. The larger this value, the longer the VI will take to compute the matrix.
        /// </param>
        /// <returns>
        /// Returns the Report having Cooccurrence Matrix and Feature Vector
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static CooccurrenceMatrixReport CooccurrenceMatrixAnalysis(VisionImage sourceImage, Int32 levelPixel)
        {
            return CooccurrenceMatrixAnalysis(sourceImage, levelPixel, new Roi(), new DisplacementVector(), new Collection<CooccurrenceFeature>());
        }
        //==========================================================================================
        /// <summary>
        /// Performs the Cooccurrence Matrix Texture Analysis.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="levelPixel">
        /// Cooccurrence Level specifies the number of gray levels the image is quantized to before the cooccurrence
        /// matrix is computed. The larger this value, the longer the VI will take to compute the matrix.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the cooccurrence matrix and features
        /// </param>
        /// <returns>
        /// Returns the Report having Cooccurrence Matrix and Feature Vector
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static CooccurrenceMatrixReport CooccurrenceMatrixAnalysis(VisionImage sourceImage, Int32 levelPixel, Roi roi)
        {
            return CooccurrenceMatrixAnalysis(sourceImage, levelPixel, roi, new DisplacementVector(), new Collection<CooccurrenceFeature>());
        }
        //==========================================================================================
        /// <summary>
        /// Performs the Cooccurrence Matrix Texture Analysis.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="levelPixel">
        /// Cooccurrence Level specifies the number of gray levels the image is quantized to before the cooccurrence
        /// matrix is computed. The larger this value, the longer the VI will take to compute the matrix.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the cooccurrence matrix and features
        /// </param>
        /// <param name="displacementVector">
        /// Displacement Vector specifies the distance of the neighboring pixels to consider when computing
        /// the co-occurrence matrix
        /// </param>
        /// <returns>
        /// Returns the Report having Cooccurrence Matrix and Feature Vector
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static CooccurrenceMatrixReport CooccurrenceMatrixAnalysis(VisionImage sourceImage, Int32 levelPixel, Roi roi, DisplacementVector displacementVector)
        {
            return CooccurrenceMatrixAnalysis(sourceImage, levelPixel, roi, displacementVector, new Collection<CooccurrenceFeature>());
        }
        //==========================================================================================
        /// <summary>
        /// Performs the Cooccurrence Matrix Texture Analysis.
        /// </summary>
        /// <param name="sourceImage">
        /// Image Src is a reference to the source image. 
        /// </param>
        /// <param name="levelPixel">
        /// Cooccurrence Level specifies the number of gray levels the image is quantized to before the cooccurrence
        /// matrix is computed. The larger this value, the longer the VI will take to compute the matrix.
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor specifies the region of the image in which to extract the cooccurrence matrix and features
        /// </param>
        /// <param name="displacementVector">
        /// Displacement Vector specifies the distance of the neighboring pixels to consider when computing
        /// the co-occurrence matrix
        /// </param>
        /// <param name="featureOption">
        /// Haralick Feature Type specifies the type of Haralick features to extract from the image.
        /// </param>
        /// <returns>
        /// Returns the Report having Cooccurrence Matrix and Feature Vector
        /// </returns>
        /// <remarks>
        /// Use this method with U8, U16 and I16 images.
        /// </remarks>

        public static CooccurrenceMatrixReport CooccurrenceMatrixAnalysis(VisionImage sourceImage, Int32 levelPixel, Roi roi, DisplacementVector displacementVector, Collection<CooccurrenceFeature> featureOption)
        {
            if (sourceImage == null) { throw new ArgumentNullException("sourceImage"); }
            sourceImage.ThrowIfDisposed();
            //Roi.ThrowIfNonNullAndDisposed(roi);
            if (displacementVector == null) { throw new ArgumentNullException("displacementVector"); }
            if (featureOption.Count > 7) { throw new ArgumentOutOfRangeException("featureOption"); }
            IntPtr featureOptionArray = (IntPtr)null;
            Int32 numFeatureOption = 0;
            Int32[] featureOptionsArray = new Int32[featureOption.Count]; ;
            if (featureOption.Count != 0)
            {
                
                foreach (CooccurrenceFeature val in featureOption)
                {
                    featureOptionsArray.SetValue((Int32)val, numFeatureOption++);
                }
                
            }
            featureOptionArray = Utilities.ConvertArrayToIntPtr<Int32>(featureOptionsArray);
            CVI_DisplacementVector cviDisplacementVector = new CVI_DisplacementVector();
            cviDisplacementVector.ConvertFromExternal(displacementVector);
            IntPtr cooccurrenceMatrixArray = (IntPtr)null; 
            IntPtr featureVectorArray  = (IntPtr)null;
            Int32 featureVectorArraySize = 0;
            Int32 rows  = 0, cols = 0;
            
            Utilities.ThrowError(VisionDll.imaqCooccurrenceMatrix(sourceImage._image, Roi.GetIntPtr(roi), levelPixel, ref cviDisplacementVector, featureOptionArray, numFeatureOption, ref cooccurrenceMatrixArray, ref rows, ref cols, ref featureVectorArray, ref featureVectorArraySize));
            double[,] cooccurrenceMatrix = Utilities.ConvertIntPtrIndirectTo2DArrayDouble(cooccurrenceMatrixArray, rows, cols, true);
            double[] featureVector = Utilities.ConvertIntPtrTo1DStructureArray<double>(featureVectorArray, (Int32)featureVectorArraySize, true);
            CooccurrenceMatrixReport toReturn = new CooccurrenceMatrixReport(cooccurrenceMatrix, featureVector);
            return toReturn;
        }
        #endregion

        #region Edge detection functions
        //==========================================================================================
        /// <summary>
        /// Detects circular edges inside an ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <returns>
        /// A FindCircularEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindCircularEdgeReport FindCircularEdge(VisionImage image, Roi roi)
        {
            return FindCircularEdge(image, roi, new FindCircularEdgeOptions(), new CircularEdgeFitOptions(), new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// Detects circular edges inside an ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <param name="findCircularEdgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel 
        /// analysis of the edges.
        /// </param>
        /// <returns>
        /// A FindCircularEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindCircularEdgeReport FindCircularEdge(VisionImage image, Roi roi, FindCircularEdgeOptions findCircularEdgeOptions)
        {
            return FindCircularEdge(image, roi, findCircularEdgeOptions, new CircularEdgeFitOptions(), new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// Detects circular edges inside an ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <param name="findCircularEdgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel 
        /// analysis of the edges.
        /// </param>
        /// <param name="circularEdgeFitOptions">
        /// Defines the Circle Fit options to use for Edge Detection.
        /// </param>
        /// <returns>
        /// A FindCircularEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindCircularEdgeReport FindCircularEdge(VisionImage image, Roi roi, FindCircularEdgeOptions findCircularEdgeOptions, CircularEdgeFitOptions circularEdgeFitOptions)
        {
            return FindCircularEdge(image, roi, findCircularEdgeOptions, circularEdgeFitOptions, new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// Detects circular edges inside an ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <param name="findCircularEdgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel 
        /// analysis of the edges.
        /// </param>
        /// <param name="circularEdgeFitOptions">
        /// Defines the Circle Fit options to use for Edge Detection.
        /// </param>
        /// <param name="transform">
        /// Specifies how to transform the location of the edge detection based on the difference between the reference 
        /// coordinate system and the measurement coordinate system.
        /// </param>
        /// <returns>
        /// A FindCircularEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindCircularEdgeReport FindCircularEdge(VisionImage image, Roi roi, FindCircularEdgeOptions findCircularEdgeOptions, CircularEdgeFitOptions circularEdgeFitOptions, CoordinateTransform transform)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (findCircularEdgeOptions == null) { throw new ArgumentNullException("findCircularEdgeOptions"); }
            if (findCircularEdgeOptions.EdgeOptions == null) { throw new ArgumentNullException("findCircularEdgeOptions.EdgeOptions"); }
            if (circularEdgeFitOptions == null) { throw new ArgumentNullException("circularEdgeFitOptions"); }
            if (transform == null) { throw new ArgumentNullException("transform"); }
            CVI_FindCircularEdgeOptions cviFindCircularEdgeOptions = new CVI_FindCircularEdgeOptions();
            cviFindCircularEdgeOptions.ConvertFromExternal(findCircularEdgeOptions);
            CVI_CoordinateSystem cviBaseSystem = new CVI_CoordinateSystem();
            cviBaseSystem.ConvertFromExternal(transform.ReferenceSystem);
            CVI_CoordinateSystem cviNewSystem = new CVI_CoordinateSystem();
            cviNewSystem.ConvertFromExternal(transform.MeasurementSystem);
            CVI_CircularEdgeFitOptions cviCircularFitOptions = new CVI_CircularEdgeFitOptions();
            cviCircularFitOptions.ConvertFromExternal(circularEdgeFitOptions);
            IntPtr report = VisionDll.imaqFindCircularEdge2(image._image, roi._roi, ref cviBaseSystem, ref cviNewSystem, ref cviFindCircularEdgeOptions, ref cviCircularFitOptions);
            Utilities.ThrowError(report);
            CVI_FindCircularEdgeReport toReturn = Utilities.ConvertIntPtrToStructure<CVI_FindCircularEdgeReport>(report, true);
            return toReturn.ConvertToExternal();
        }

        //==========================================================================================
        /// <summary>
        /// Detects straight edges inside an Annulus ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <returns>
        /// A FindConcentricEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindConcentricEdgeReport FindConcentricEdge(VisionImage image, Roi roi)
        {
            return FindConcentricEdge(image, roi, new FindConcentricEdgeOptions(), new ConcentricEdgeFitOptions(), new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// Detects straight edges inside an Annulus ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <param name="findConcentricEdgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel 
        /// analysis of the edges.
        /// </param>
        /// <returns>
        /// A FindConcentricEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindConcentricEdgeReport FindConcentricEdge(VisionImage image, Roi roi, FindConcentricEdgeOptions findConcentricEdgeOptions)
        {
            return FindConcentricEdge(image, roi, findConcentricEdgeOptions, new ConcentricEdgeFitOptions(), new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// Detects straight edges inside an Annulus ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <param name="findConcentricEdgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel 
        /// analysis of the edges.
        /// </param>
        /// <param name="concentricEdgeFitOptions">
        /// Defines the Circle Fit options to use for Edge Detection.
        /// </param>
        /// <returns>
        /// A FindConcentricEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindConcentricEdgeReport FindConcentricEdge(VisionImage image, Roi roi, FindConcentricEdgeOptions findConcentricEdgeOptions, ConcentricEdgeFitOptions concentricEdgeFitOptions)
        {
            return FindConcentricEdge(image, roi, findConcentricEdgeOptions, concentricEdgeFitOptions, new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// Detects straight edges inside an Annulus ROI, and optionally overlays the information used to search for the edges.
        /// </summary>
        /// <param name="image">
        /// The input image.
        /// </param>
        /// <param name="roi">
        /// The region of the image on which the method is performed. The first contour of <format type="italics">roi</format> must be 
        /// a RectangleContour or RotatedRectangleContour.
        /// </param>
        /// <param name="findConcentricEdgeOptions">
        /// Defines the characteristics the method uses to find the edges and the parameters it needs for subpixel 
        /// analysis of the edges.
        /// </param>
        /// <param name="concentricEdgeFitOptions">
        /// Defines the Circle Fit options to use for Edge Detection.
        /// </param>
        /// <param name="transform">
        /// Specifies how to transform the location of the edge detection based on the difference between the reference 
        /// coordinate system and the measurement coordinate system.
        /// </param>
        /// <returns>
        /// A FindConcentricEdgeReport containing the information about the detected edges.
        /// </returns>
        /// <remarks>
        /// Use this method with U8, I16, Single, Rgb32, Hsl32, RgbU64 images.
        /// </remarks>

        public static FindConcentricEdgeReport FindConcentricEdge(VisionImage image, Roi roi, FindConcentricEdgeOptions findConcentricEdgeOptions, ConcentricEdgeFitOptions concentricEdgeFitOptions, CoordinateTransform transform)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (findConcentricEdgeOptions == null) { throw new ArgumentNullException("findConcentricEdgeOptions"); }
            if (findConcentricEdgeOptions.EdgeOptions == null) { throw new ArgumentNullException("findConcentricEdgeOptions"); }
            if (concentricEdgeFitOptions == null) { throw new ArgumentNullException("concentricEdgeFitOptions"); }
            if (transform == null) { throw new ArgumentNullException("transform"); }
            CVI_FindConcentricEdgeOptions cviFindConcentricEdgeOptions = new CVI_FindConcentricEdgeOptions();
            cviFindConcentricEdgeOptions.ConvertFromExternal(findConcentricEdgeOptions);
            CVI_CoordinateSystem cviBaseSystem = new CVI_CoordinateSystem();
            cviBaseSystem.ConvertFromExternal(transform.ReferenceSystem);
            CVI_CoordinateSystem cviNewSystem = new CVI_CoordinateSystem();
            cviNewSystem.ConvertFromExternal(transform.MeasurementSystem);
            CVI_ConcentricEdgeFitOptions cviConcentricFitOptions = new CVI_ConcentricEdgeFitOptions();
            cviConcentricFitOptions.ConvertFromExternal(concentricEdgeFitOptions);
            IntPtr report = VisionDll.imaqFindConcentricEdge2(image._image, roi._roi, ref cviBaseSystem, ref cviNewSystem, ref cviFindConcentricEdgeOptions, ref cviConcentricFitOptions);
            Utilities.ThrowError(report);
            CVI_FindConcentricEdgeReport toReturn = Utilities.ConvertIntPtrToStructure<CVI_FindConcentricEdgeReport>(report, true);
            return toReturn.ConvertToExternal();
        }
        #endregion

        #region Geometric Matching Contour
        //==========================================================================================
        /// <summary>
        /// Extracts a single, best contour from an image. If the input image is calibrated, the contour will be learned with calibration.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor is a descriptor that defines the region within which to search for contours. 
        /// </param>
        /// <param name="direction">
        /// Specifies the direction in which the Region of Interest (ROI) is examined. 
        /// </param>                          
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ExtractContourReport" crefType="Unqualified"/>
        /// object containing contour points information in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ExtractContourReport ExtractContour(VisionImage image, Roi roi, ExtractContourDirection direction)
        {
            return ExtractContour(image, roi, direction, new CurveParameters(), new Collection<ConnectionConstraint>(), ExtractContourSelection.Closest, null);
        }

        //==========================================================================================
        /// <summary>
        /// Extracts a single, best contour from an image. If the input image is calibrated, the contour will be learned with calibration.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor is a descriptor that defines the region within which to search for contours. 
        /// </param>
        /// <param name="direction">
        /// Specifies the direction in which the Region of Interest (ROI) is examined. 
        /// </param>
        /// <param name="parameters">
        /// Describes  information about how curves are extracted from the image.
        /// </param>                     
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ExtractContourReport" crefType="Unqualified"/>
        /// object containing contour points information in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ExtractContourReport ExtractContour(VisionImage image, Roi roi, ExtractContourDirection direction, CurveParameters curveParameters)
        {
            return ExtractContour(image, roi, direction, curveParameters, new Collection<ConnectionConstraint>(), ExtractContourSelection.Closest, null);
        }

        //==========================================================================================
        /// <summary>
        /// Extracts a single, best contour from an image. If the input image is calibrated, the contour will be learned with calibration.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor is a descriptor that defines the region within which to search for contours. 
        /// </param>
        /// <param name="direction">
        /// Specifies the direction in which the Region of Interest (ROI) is examined. 
        /// </param>
        /// <param name="parameters">
        /// Describes  information about how curves are extracted from the image.
        /// </param>
        /// <param name="connectionConstraints">
        /// Specifies how to connect curves. Select a maximum of 1 constraint of each type. 
        /// </param>              
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ExtractContourReport" crefType="Unqualified"/>
        /// object containing contour points information in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ExtractContourReport ExtractContour(VisionImage image, Roi roi, ExtractContourDirection direction, CurveParameters curveParameters, Collection<ConnectionConstraint> connectionConstraints)
        {
            return ExtractContour(image, roi, direction, curveParameters, connectionConstraints, ExtractContourSelection.Closest, null);
        }

        //==========================================================================================
        /// <summary>
        /// Extracts a single, best contour from an image. If the input image is calibrated, the contour will be learned with calibration.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor is a descriptor that defines the region within which to search for contours. 
        /// </param>
        /// <param name="direction">
        /// Specifies the direction in which the Region of Interest (ROI) is examined. 
        /// </param>
        /// <param name="parameters">
        /// Describes  information about how curves are extracted from the image.
        /// </param>
        /// <param name="connectionConstraints">
        /// Specifies how to connect curves. Select a maximum of 1 constraint of each type. 
        /// </param>
        /// <param name="selection">
        /// Selects an extracted contour based on the property you choose. 
        /// </param>        
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ExtractContourReport" crefType="Unqualified"/>
        /// object containing contour points information in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ExtractContourReport ExtractContour(VisionImage image, Roi roi, ExtractContourDirection direction, CurveParameters curveParameters, Collection<ConnectionConstraint> connectionConstraints, ExtractContourSelection selection)
        {
            return ExtractContour(image, roi, direction, curveParameters, connectionConstraints, selection, null);
        }

        //==========================================================================================
        /// <summary>
        /// Extracts a single, best contour from an image. If the input image is calibrated, the contour will be learned with calibration.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the source image. 
        /// </param>
        /// <param name="roi">
        /// ROI Descriptor is a descriptor that defines the region within which to search for contours. 
        /// </param>
        /// <param name="direction">
        /// Specifies the direction in which the Region of Interest (ROI) is examined. 
        /// </param>
        /// <param name="curveParameters">
        /// Describes  information about how curves are extracted from the image.
        /// </param>
        /// <param name="connectionConstraints">
        /// Specifies how to connect curves. Select a maximum of 1 constraint of each type. 
        /// </param>
        /// <param name="selection">
        /// Selects an extracted contour based on the property you choose. 
        /// </param>
        /// <param name="contourImage">
        /// Contour Image is a reference to the destination image. If this input is connected, the Contour image will be the image extracted from the ROI bounding box.
        /// </param>
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ExtractContourReport" crefType="Unqualified"/>
        /// object containing contour points information in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ExtractContourReport ExtractContour(VisionImage image, Roi roi, ExtractContourDirection direction, CurveParameters curveParameters, Collection<ConnectionConstraint> connectionConstraints, ExtractContourSelection selection, VisionImage contourImage)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(contourImage);
            if (curveParameters == null) { throw new ArgumentNullException("curveParameters"); }
            // roi is compulsory in the extract contour. If the roi is not supplied then an error is returned.
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (connectionConstraints == null) { throw new ArgumentNullException("connectionConstraints"); }
            //converting the collection to the IntPtr
            IntPtr cviConnectionConstraint = Utilities.ConvertCollectionToIntPtr<ConnectionConstraint, CVI_ConnectionConstraint>(connectionConstraints);

            CVI_CurveParameters cviCurveParams = new CVI_CurveParameters();
            cviCurveParams.ConvertFromExternal(curveParameters);
            // imaqExtractContour in VDM internally throws an error for all images that are not of type U8.
            IntPtr report = VisionDll.imaqExtractContour(image._image, Roi.GetIntPtr(roi), direction, ref cviCurveParams, cviConnectionConstraint, connectionConstraints.Count, selection, VisionImage.GetIntPtr(contourImage));
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ExtractContourReport, CVI_ExtractContourReport>(report, true);
        }

        //==========================================================================================
        /// <summary>
        /// Returns all the information describing a contour.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the image containing a contour. 
        /// </param>        
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ContourInfoReport" crefType="Unqualified"/>
        /// object containing information about the contour found in the image.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ContourInfoReport ContourInfo(VisionImage image)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();

            IntPtr report = VisionDll.imaqContourInfo(image._image);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ContourInfoReport, CVI_ContourInfoReport>(report, true);
        }

        //==========================================================================================
        /// <summary>
        /// Overlays the Contour on the image.
        /// </summary>
        /// <param name="image">
        /// The image reference to the source image. 
        /// </param>                                      

        public static void ContourOverlay(VisionImage image)
        {
            ContourOverlaySettings defaultPointsSettings = new ContourOverlaySettings(false, new Rgb32Value(255, 0, 0), 1, true);
            ContourOverlaySettings defaultEquationSettings = new ContourOverlaySettings(false, new Rgb32Value(0, 0, 255), 1, true);
            ContourOverlay(image, image, defaultPointsSettings, defaultEquationSettings, "");
        }

        //==========================================================================================
        /// <summary>
        /// Overlays the Contour on the image.
        /// </summary>
        /// <param name="image">
        /// The image reference to the source image. 
        /// </param>
        /// <param name="contourImage">
        /// Contour Image is a reference to the image containing a contour.
        /// </param>                              

        public static void ContourOverlay(VisionImage image, VisionImage contourImage)
        {
            ContourOverlaySettings defaultPointsSettings = new ContourOverlaySettings(false, new Rgb32Value(255, 0, 0), 1, true);
            ContourOverlaySettings defaultEquationSettings = new ContourOverlaySettings(false, new Rgb32Value(0, 0, 255), 1, true);
            ContourOverlay(image, contourImage, defaultPointsSettings, defaultEquationSettings, "");
        }

        //==========================================================================================
        /// <summary>
        /// Overlays the Contour on the image.
        /// </summary>
        /// <param name="image">
        /// The image reference to the source image.  
        /// </param>
        /// <param name="contourImage">
        /// Contour Image is a reference to the image containing a contour.
        /// </param>
        /// <param name="pointsSettings">
        /// PointSettings specifies the settings for the contour points in the overlay.
        /// </param>                     

        public static void ContourOverlay(VisionImage image, VisionImage contourImage, ContourOverlaySettings pointsSettings)
        {
            ContourOverlaySettings defaultEquationSettings = new ContourOverlaySettings(false, new Rgb32Value(0, 0, 255), 1, true);
            ContourOverlay(image, contourImage, pointsSettings, defaultEquationSettings, "");
        }

        //==========================================================================================
        /// <summary>
        /// Overlays the Contour on the image.
        /// </summary>
        /// <param name="image">
        /// The image reference to the source image. 
        /// </param>
        /// <param name="contourImage">
        /// Contour Image is a reference to the image containing a contour.
        /// </param>
        /// <param name="pointsSettings">
        /// PointSettings specifies the settings for the contour points in the overlay.
        /// </param>
        /// <param name="equationSettings">
        /// Equation Settings specifies the settings for the contour equation in the overlay. 
        /// </param>             

        public static void ContourOverlay(VisionImage image, VisionImage contourImage, ContourOverlaySettings pointsSettings, ContourOverlaySettings equationSettings)
        {
            ContourOverlay(image, contourImage, pointsSettings, equationSettings, "");
        }

        //==========================================================================================
        /// <summary>
        /// Overlays the Contour on the image.
        /// </summary>
        /// <param name="image">
        /// The image reference to the source image. 
        /// </param>
        /// <param name="contourImage">
        /// Contour Image is a reference to the image containing a contour.
        /// </param>
        /// <param name="pointsSettings">
        /// PointSettings specifies the settings for the contour points in the overlay.
        /// </param>
        /// <param name="equationSettings">
        /// Equation Settings specifies the settings for the contour equation in the overlay. 
        /// </param>
        /// <param name="groupName">
        /// Specifies the group to which you want to add the overlay information.
        /// </param>       

        public static void ContourOverlay(VisionImage image, VisionImage contourImage, ContourOverlaySettings pointsSettings, ContourOverlaySettings equationSettings, String groupName)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (contourImage == null) { throw new ArgumentNullException("contourImage"); }
            contourImage.ThrowIfDisposed();
            VisionImage.ThrowIfNonNullAndDisposed(contourImage);
            if (pointsSettings == null) { throw new ArgumentNullException("pointsSettings"); }
            if (equationSettings == null) { throw new ArgumentNullException("equationSettings"); }
            if (groupName == null) { throw new ArgumentNullException("groupName"); }

            CVI_ContourOverlaySettings cviPointsSettings = new CVI_ContourOverlaySettings();
            cviPointsSettings.ConvertFromExternal(pointsSettings);

            CVI_ContourOverlaySettings cviEquationSettings = new CVI_ContourOverlaySettings();
            cviEquationSettings.ConvertFromExternal(equationSettings);

            Utilities.ThrowError(VisionDll.imaqContourOverlay(image._image, VisionImage.GetIntPtr(contourImage), ref cviPointsSettings, ref cviEquationSettings, groupName));
        }

        //==========================================================================================
        /// <summary>
        /// Computes the curvature profile along the contour.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the image containing a contour.
        /// </param>        
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ComputeCurvatureReport" crefType="Unqualified"/>
        /// object containing curvature profile information.
        /// </returns>     
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ComputeCurvatureReport ContourComputeCurvature(VisionImage image)
        {
            UInt32 defaultKernel = 5;
            return ContourComputeCurvature(image, defaultKernel);
        }

        //==========================================================================================
        /// <summary>
        /// Computes the curvature profile along the contour.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the image containing a contour. 
        /// </param>
        /// <param name="kernel">
        /// Kernel specifies the size of the kernel used to compute the curvature profile of the selected contour.
        /// </param>
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ComputeCurvatureReport" crefType="Unqualified"/>
        /// object containing curvature profile information.
        /// </returns>     
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ComputeCurvatureReport ContourComputeCurvature(VisionImage image, UInt32 kernel)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();

            IntPtr report = VisionDll.imaqContourComputeCurvature(image._image, kernel);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ComputeCurvatureReport, CVI_ComputeCurvatureReport>(report, true);
        }

        //==========================================================================================
        /// <summary>
        /// Classifies the contour located in the given image.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the image containing a contour. 
        /// </param>         
        /// <param name="curvatureClasses">
        /// Curvature classes a collection is with one element for every curvature class in the image. 
        /// </param>                 
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ClassifyCurvatureReport" crefType="Unqualified"/>
        /// object containing information of classified cureves.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ClassifyCurvatureReport ContourClassifyCurvature(VisionImage contourImage, Collection<RangeLabel> curvatureClasses)
        {
            UInt32 defaultKernel = 5;
            return ContourClassifyCurvature(contourImage, curvatureClasses, defaultKernel);
        }

        //==========================================================================================
        /// <summary>
        /// Classifies the contour located in the given image.
        /// </summary>
        /// <param name="image">
        /// The image is a reference to the image containing a contour.
        /// </param>         
        /// <param name="curvatureClasses">
        /// Curvature classes a collection is with one element for every curvature class in the image. 
        /// </param> 
        /// <param name="kernel">
        /// Kernel specifies the size of the kernel used to compute the curvature profile of the selected contour.
        /// </param>         
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ClassifyCurvatureReport" crefType="Unqualified"/>
        /// object containing information of classified cureves.
        /// </returns>
        /// <remarks>
        /// Use this method with U8 images.
        /// </remarks>

        public static ClassifyCurvatureReport ContourClassifyCurvature(VisionImage contourImage, Collection<RangeLabel> curvatureClasses, UInt32 kernel)
        {
            if (contourImage == null) { throw new ArgumentNullException("contourImage"); }
            contourImage.ThrowIfDisposed();
            if (curvatureClasses == null) { throw new ArgumentNullException("curvatureClasses"); }
            IntPtr cviCurvatureClasses = Utilities.ConvertCollectionToIntPtr<RangeLabel, CVI_RangeLabel>(curvatureClasses);

            IntPtr report = VisionDll.imaqContourClassifyCurvature(contourImage._image, kernel, cviCurvatureClasses, curvatureClasses.Count);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ClassifyCurvatureReport, CVI_ClassifyCurvatureReport>(report, true);
        }

        //==========================================================================================
        /// <summary>
        /// Compares a target contour with a template contour, and calculates the distance between the contours.
        /// </summary>
        /// <param name="targetImage">
        /// Target Image is a reference to the image containing the target contour.
        /// </param>         
        /// <param name="templateImage">
        /// Template Image is a reference to the template image.
        /// </param>                     
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ComputeDistanceReport" crefType="Unqualified"/>
        /// object containing information of distance between the contours.
        /// </returns>

        public static ComputeDistanceReport ContourComputeDistances(VisionImage targetImage, VisionImage templateImage)
        {
            return ContourComputeDistances(targetImage, templateImage, 0, new SetupMatchPatternData());
        }

        //==========================================================================================
        /// <summary>
        /// Compares a target contour with a template contour, and calculates the distance between the contours.
        /// </summary>
        /// <param name="targetImage">
        /// Target Image is a reference to the image containing the target contour.
        /// </param>         
        /// <param name="templateImage">
        /// Template Image is a reference to the template image.
        /// </param>             
        /// <param name="smoothingKernel">
        /// Smoothing Kernel specifies the size of the smoothing kernel.
        /// </param>
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ComputeDistanceReport" crefType="Unqualified"/>
        /// object containing information of distance between the contours.
        /// </returns>

        public static ComputeDistanceReport ContourComputeDistances(VisionImage targetImage, VisionImage templateImage, UInt32 smoothingKernel)
        {
            return ContourComputeDistances(targetImage, templateImage, smoothingKernel, new SetupMatchPatternData());
        }

        //==========================================================================================
        /// <summary>
        /// Compares a target contour with a template contour, and calculates the distance between the contours.
        /// </summary>
        /// <param name="targetImage">
        /// Target Image is a reference to the image containing the target contour.
        /// </param>         
        /// <param name="templateImage">
        /// Template Image is a reference to the template image.
        /// </param> 
        /// <param name="matchSetupData">
        /// Match Setup Data is a string that contains information from the SetupMatchContourPattern function.
        /// </param>     
        /// <param name="smoothingKernel">
        /// Smoothing Kernel specifies the size of the smoothing kernel.
        /// </param>
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ComputeDistanceReport" crefType="Unqualified"/>
        /// object containing information of distance between the contours.
        /// </returns>

        public static ComputeDistanceReport ContourComputeDistances(VisionImage targetImage, VisionImage templateImage, UInt32 smoothingKernel, SetupMatchPatternData matchSetupData)
        {
            if (targetImage == null) { throw new ArgumentNullException("targetImage"); }
            targetImage.ThrowIfDisposed();
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (matchSetupData == null) { throw new ArgumentNullException("matchSetupData"); }

            CVI_SetupMatchPatternData cviMatchSetupData = new CVI_SetupMatchPatternData();
            cviMatchSetupData.ConvertFromExternal(matchSetupData);

            IntPtr report = VisionDll.imaqContourComputeDistances(targetImage._image, templateImage._image, ref cviMatchSetupData, smoothingKernel);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ComputeDistanceReport, CVI_ComputeDistanceReport>(report, true);
        }

        //==========================================================================================
        /// <summary>
        /// Classifies the distance between a image containing a contour and a template image containing a contour.
        /// </summary>
        /// <param name="targetImage">
        /// Target Image is a reference to the image containing the target contour.
        /// </param>         
        /// <param name="templateImage">
        /// Template Image is a reference to the template image.
        /// </param> 
        /// <param name="distanceClasses">
        /// Distance Classes is an array with one element for every distance class. 
        /// </param>                     
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ClassifyDistancesReport" crefType="Unqualified"/>
        /// object containing information of classifeied distance between the contours.
        /// </returns>

        public static ClassifyDistancesReport ContourClassifyDistances(VisionImage targetImage, VisionImage templateImage, Collection<RangeLabel> distanceClasses)
        {
            return ContourClassifyDistances(targetImage, templateImage, distanceClasses, 0, new SetupMatchPatternData());
        }

        //==========================================================================================
        /// <summary>
        /// Classifies the distance between a image containing a contour and a template image containing a contour.
        /// </summary>
        /// <param name="targetImage">
        /// Target Image is a reference to the image containing the target contour.
        /// </param>         
        /// <param name="templateImage">
        /// Template Image is a reference to the template image.
        /// </param> 
        /// <param name="distanceClasses">
        /// Distance Classes is an array with one element for every distance class. 
        /// </param>             
        /// <param name="smoothingKernel">
        /// Smoothing Kernel specifies the size of the smoothing kernel.
        /// </param>
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ClassifyDistancesReport" crefType="Unqualified"/>
        /// object containing information of classifeied distance between the contours.
        /// </returns>

        public static ClassifyDistancesReport ContourClassifyDistances(VisionImage targetImage, VisionImage templateImage, Collection<RangeLabel> distanceClasses, UInt32 smoothingKernel)
        {
            return ContourClassifyDistances(targetImage, templateImage, distanceClasses, smoothingKernel, new SetupMatchPatternData());
        }
        //==========================================================================================
        /// <summary>
        /// Classifies the distance between a image containing a contour and a template image containing a contour.
        /// </summary>
        /// <param name="targetImage">
        /// Target Image is a reference to the image containing the target contour.
        /// </param>         
        /// <param name="templateImage">
        /// Template Image is a reference to the template image.
        /// </param> 
        /// <param name="distanceClasses">
        /// Distance Classes is an array with one element for every distance class. 
        /// </param> 
        /// <param name="matchSetupData">
        /// Match Setup Data is a string that contains information from the SetupMatchContourPattern function.
        /// </param>     
        /// <param name="smoothingKernel">
        /// Smoothing Kernel specifies the size of the smoothing kernel.
        /// </param>
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.ClassifyDistancesReport" crefType="Unqualified"/>
        /// object containing information of classifeied distance between the contours.
        /// </returns>

        public static ClassifyDistancesReport ContourClassifyDistances(VisionImage targetImage, VisionImage templateImage, Collection<RangeLabel> distanceClasses, UInt32 smoothingKernel, SetupMatchPatternData matchSetupData)
        {
            if (targetImage == null) { throw new ArgumentNullException("targetImage"); }
            targetImage.ThrowIfDisposed();
            if (templateImage == null) { throw new ArgumentNullException("templateImage"); }
            templateImage.ThrowIfDisposed();
            if (matchSetupData == null) { throw new ArgumentNullException("matchSetupData"); }

            if (distanceClasses == null) { throw new ArgumentNullException("distanceClasses"); }
            IntPtr cviDistanceClasses = Utilities.ConvertCollectionToIntPtr<RangeLabel, CVI_RangeLabel>(distanceClasses);

            CVI_SetupMatchPatternData cviMatchSetupData = new CVI_SetupMatchPatternData();
            cviMatchSetupData.ConvertFromExternal(matchSetupData);

            IntPtr report = VisionDll.imaqContourClassifyDistances(targetImage._image, templateImage._image, ref cviMatchSetupData, smoothingKernel, cviDistanceClasses, distanceClasses.Count);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ClassifyDistancesReport, CVI_ClassifyDistancesReport>(report, true);
        }

        //==========================================================================================
        /// <summary>
        /// Sets parameters that are used during the matching process.
        /// </summary>                          
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.SetupMatchPatternData" crefType="Unqualified"/>
        /// SetupMatchPatternData contains information about the setup parameters for the matching phase.
        /// </returns>

        public static SetupMatchPatternData ContourSetupMatchPattern()
        {
            return ContourSetupMatchPattern(new ContourMatchMode(), false, new CurveParameters(), true, new Collection<RangeSettings>());
        }

        //==========================================================================================
        /// <summary>
        /// Sets parameters that are used during the matching process.
        /// </summary>
        /// <param name="matchMode">
        /// Specifies conditions under which you want the function to find template matches.
        /// </param>                                 
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.SetupMatchPatternData" crefType="Unqualified"/>
        /// SetupMatchPatternData contains information about the setup parameters for the matching phase.
        /// </returns>

        public static SetupMatchPatternData ContourSetupMatchPattern(ContourMatchMode matchMode)
        {
            return ContourSetupMatchPattern(matchMode, false, new CurveParameters(), true, new Collection<RangeSettings>());
        }
        //==========================================================================================
        /// <summary>
        /// Sets parameters that are used during the matching process.
        /// </summary>
        /// <param name="matchMode">
        /// Specifies conditions under which you want the function to find template matches.
        /// </param>         
        /// <param name="enableAccuracy">
        /// Subpixel Accuracy determines whether to return the match results with subpixel accuracy.
        /// </param>                            
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.SetupMatchPatternData" crefType="Unqualified"/>
        /// SetupMatchPatternData contains information about the setup parameters for the matching phase.
        /// </returns> 

        public static SetupMatchPatternData ContourSetupMatchPattern(ContourMatchMode matchMode, bool enableAccuracy)
        {
            return ContourSetupMatchPattern(matchMode, enableAccuracy, new CurveParameters(), true, new Collection<RangeSettings>());
        }

        //==========================================================================================
        /// <summary>
        /// Sets parameters that are used during the matching process.
        /// </summary>
        /// <param name="matchMode">
        /// Specifies conditions under which you want the function to find template matches.
        /// </param>         
        /// <param name="enableAccuracy">
        /// Subpixel Accuracy determines whether to return the match results with subpixel accuracy.
        /// </param> 
        /// <param name="curveParameters">
        /// Specifies the information of how curves are extracted from the image. 
        /// </param>                   
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.SetupMatchPatternData" crefType="Unqualified"/>
        /// SetupMatchPatternData contains information about the setup parameters for the matching phase.
        /// </returns>

        public static SetupMatchPatternData ContourSetupMatchPattern(ContourMatchMode matchMode, bool enableAccuracy, CurveParameters curveParameters)
        {
            return ContourSetupMatchPattern(matchMode, enableAccuracy, curveParameters, true, new Collection<RangeSettings>());
        }

        //==========================================================================================
        /// <summary>
        /// Sets parameters that are used during the matching process.
        /// </summary>
        /// <param name="matchMode">
        /// Specifies conditions under which you want the function to find template matches.
        /// </param>         
        /// <param name="enableAccuracy">
        /// Subpixel Accuracy determines whether to return the match results with subpixel accuracy.
        /// </param> 
        /// <param name="curveParameters">
        /// Specifies the information of how curves are extracted from the image. 
        /// </param> 
        /// <param name="useLearnCurveParameters">
        /// Use Learn Curve Parameters specifies whether to use the Contour Learn Options parameters from the SetupLearnContourPattern function.
        /// </param>             
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.SetupMatchPatternData" crefType="Unqualified"/>
        /// SetupMatchPatternData contains information about the setup parameters for the matching phase.
        /// </returns>

        public static SetupMatchPatternData ContourSetupMatchPattern(ContourMatchMode matchMode, bool enableAccuracy, CurveParameters curveParameters, bool useLearnCurveParameters)
        {
            return ContourSetupMatchPattern(matchMode, enableAccuracy, curveParameters, useLearnCurveParameters, new Collection<RangeSettings>());
        }

        //==========================================================================================
        /// <summary>
        /// Sets parameters that are used during the matching process.
        /// </summary>
        /// <param name="matchMode">
        /// Specifies conditions under which you want the function to find template matches.
        /// </param>         
        /// <param name="enableAccuracy">
        /// Subpixel Accuracy determines whether to return the match results with subpixel accuracy.
        /// </param> 
        /// <param name="curveParameters">
        /// Specifies the information of how curves are extracted from the image. 
        /// </param> 
        /// <param name="useLearnCurveParameters">
        /// Use Learn Curve Parameters specifies whether to use the Contour Learn Options parameters from the SetupLearnContourPattern function.
        /// </param>     
        /// <param name="rangeSettings">
        /// Range Settings is a collection of the acceptable ranges for each Match Constraints option.
        /// </param>
        /// <returns>
        /// <see cref="NationalInstruments.Vision.Analysis.SetupMatchPatternData" crefType="Unqualified"/>
        /// SetupMatchPatternData contains information about the setup parameters for the matching phase.
        /// </returns>

        public static SetupMatchPatternData ContourSetupMatchPattern(ContourMatchMode matchMode, bool enableAccuracy, CurveParameters curveParameters, bool useLearnCurveParameters, Collection<RangeSettings> rangeSettings)
        {
            if (matchMode == null) { throw new ArgumentNullException("matchMode"); }
            if (curveParameters == null) { throw new ArgumentNullException("curveParameters"); }
            if (rangeSettings == null) { throw new ArgumentNullException("rangeSettings"); }

            CVI_ContourMatchMode cviMatchMode = new CVI_ContourMatchMode();
            cviMatchMode.ConvertFromExternal(matchMode);
            CVI_CurveParameters cviCurveParameters = new CVI_CurveParameters();
            cviCurveParameters.ConvertFromExternal(curveParameters);
            //converting the collection to the IntPtr
            IntPtr cviRangeSettings = Utilities.ConvertCollectionToIntPtr<RangeSettings, CVI_RangeSettings>(rangeSettings);

            IntPtr report = VisionDll.imaqContourSetupMatchPattern(ref cviMatchMode, (enableAccuracy ? 1 : 0), ref cviCurveParameters, (useLearnCurveParameters ? 1 : 0), cviRangeSettings, rangeSettings.Count);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<SetupMatchPatternData, CVI_SetupMatchPatternData>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Optimizes and fine-tunes advanced parameters used during the contour matching phase.
        /// </summary>
        /// <param name="setupMatchData">
        /// SetupMatchData is a string that contains information from the ContourSetupMatchPattern function
        /// </param>                        

        public static void ContourAdvancedSetupMatchPattern(SetupMatchPatternData setupMatchData)
        {
            ContourAdvancedSetupMatchPattern(setupMatchData, new Collection<GeometricAdvancedSetupDataOption>());
        }

        //==========================================================================================
        /// <summary>
        /// Optimizes and fine-tunes advanced parameters used during the contour matching phase.
        /// </summary>
        /// <param name="setupMatchData">
        /// SetupMatchData is a string that contains information from the ContourSetupMatchPattern function
        /// </param>         
        /// <param name="setupDataOptions">
        /// SetupDataOptions is an array of options to use during the matching phase.
        /// </param>                

        public static void ContourAdvancedSetupMatchPattern(SetupMatchPatternData setupMatchData, Collection<GeometricAdvancedSetupDataOption> setupDataOptions)
        {
            if (setupMatchData == null) { throw new ArgumentNullException("setupMatchData"); }
            if (setupDataOptions == null) { throw new ArgumentNullException("setupDataOptions"); }

            CVI_SetupMatchPatternData cviSetupMatchData = new CVI_SetupMatchPatternData();
            cviSetupMatchData.ConvertFromExternal(setupMatchData);

            //converting the collection to the IntPtr
            IntPtr cviSetupDataOptions = Utilities.ConvertCollectionToIntPtr<GeometricAdvancedSetupDataOption, CVI_GeometricAdvancedSetupDataOption>(setupDataOptions);
            Utilities.ThrowError(VisionDll.imaqContourAdvancedSetupMatchPattern(ref cviSetupMatchData, cviSetupDataOptions, setupDataOptions.Count));
            //copying the result back to the setupmatchData
            setupMatchData.MatchPatternData = cviSetupMatchData.ConvertToExternal().MatchPatternData;
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a line.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <returns>
        /// A ContourFitLineReport containing the information about the edge fitted.
        /// </returns>

        public static ContourFitLineReport ContourFitLine(VisionImage image)
        {
            return ContourFitLine(image, 3);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a line.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="pixelRadius">
        /// It specifies the neighborhood pixel relationship for the initial subset of points being used.
        /// </param>
        /// <returns>
        /// A ContourFitLineReport containing the information about the edge fitted.
        /// </returns>

        public static ContourFitLineReport ContourFitLine(VisionImage image, double pixelRadius)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            IntPtr report = VisionDll.imaqContourFitLine(image._image, pixelRadius);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ContourFitLineReport, CVI_ContourFitLineReport>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Circle.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <returns>
        /// A PartialCircle containing the information about the contour fitted.
        /// </returns>

        public static PartialCircle ContourFitCircle(VisionImage image)
        {
            return ContourFitCircle(image, 3);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Circle.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="pixelRadius">
        /// It specifies the neighborhood pixel relationship for the initial subset of points being used.
        /// </param>
        /// <returns>
        /// A PartialCircle containing the information about the contour fitted.
        /// </returns>

        public static PartialCircle ContourFitCircle(VisionImage image, double pixelRadius)
        {
            return ContourFitCircle(image, pixelRadius, false);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Circle.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="pixelRadius">
        /// It specifies the neighborhood pixel relationship for the initial subset of points being used.
        /// </param>
        /// <param name="rejectOutliers">
        /// It controls whether to use every given radial point or only a subset of the points to fit the circle.
        /// </param>
        /// <returns>
        /// A PartialCircle containing the information about the contour fitted.
        /// </returns>

        public static PartialCircle ContourFitCircle(VisionImage image, double pixelRadius, bool rejectOutliers)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            UInt32 isRejectOutliers = (UInt32)(rejectOutliers ? 1 : 0);
            IntPtr report = VisionDll.imaqContourFitCircle(image._image, pixelRadius, isRejectOutliers);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<PartialCircle, CVI_PartialCircle>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Ellipse.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <returns>
        /// A PartialEllipse containing the information about the contour fitted.
        /// </returns>

        public static PartialEllipse ContourFitEllipse(VisionImage image)
        {
            return ContourFitEllipse(image, 3);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Ellipse.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="pixelRadius">
        /// It specifies the neighborhood pixel relationship for the initial subset of points being used.
        /// </param>
        /// <returns>
        /// A PartialEllipse containing the information about the contour fitted.
        /// </returns>

        public static PartialEllipse ContourFitEllipse(VisionImage image, double pixelRadius)
        {
            return ContourFitEllipse(image, pixelRadius, false);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Ellipse.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="pixelRadius">
        /// It specifies the neighborhood pixel relationship for the initial subset of points being used.
        /// </param>
        /// <param name="rejectOutliers">
        /// It controls whether to use every given radial point or only a subset of the points to fit the ellipse.
        /// </param>
        /// <returns>
        /// A PartialEllipse containing the information about the contour fitted.
        /// </returns>

        public static PartialEllipse ContourFitEllipse(VisionImage image, double pixelRadius, bool rejectOutliers)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            UInt32 isRejectOutliers = (UInt32)(rejectOutliers ? 1 : 0);
            IntPtr report = VisionDll.imaqContourFitEllipse(image._image, pixelRadius, isRejectOutliers);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<PartialEllipse, CVI_PartialEllipse>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Spline.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <returns>
        /// A ContourFitSplineReport containing the information about the contour fitted.
        /// </returns>

        public static ContourFitSplineReport ContourFitSpline(VisionImage image)
        {
            return ContourFitSpline(image, 3);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Spline.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="degree">
        /// It specifies the order of polynomials that form the B-spline curve.
        /// </param>
        /// <returns>
        /// A ContourFitSplineReport containing the information about the contour fitted.
        /// </returns>

        public static ContourFitSplineReport ContourFitSpline(VisionImage image, UInt32 degree)
        {
            return ContourFitSpline(image, degree, 10);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a Spline.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="degree">
        /// It specifies the order of polynomials that form the B-spline curve.
        /// </param>
        /// <param name="nControlPoints">
        /// It specifies the number of control points on the B-spline curve.
        /// </param>
        /// <returns>
        /// A ContourFitSplineReport containing the information about the contour fitted.
        /// </returns>

        public static ContourFitSplineReport ContourFitSpline(VisionImage image, UInt32 degree, UInt32 nControlPoints)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            IntPtr report = VisionDll.imaqContourFitSpline(image._image, degree, nControlPoints);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ContourFitSplineReport, CVI_ContourFitSplineReport>(report, true);
        }
        //==========================================================================================
        /// <summary>
        /// Fits a contour with an equation. The fit will occur in corrected space if the contour 
        /// is calibrated. Fits a contour with a polynomial.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="order">
        /// It specifies the order of the polynomial equation. 
        /// </param>
        /// <returns>
        /// A ContourFitPolynomialReport containing the information about the contour fitted.
        /// </returns>

        public static ContourFitPolynomialReport ContourFitPolynomial(VisionImage image, UInt32 order)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            IntPtr report = VisionDll.imaqContourFitPolynomial(image._image, order);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ContourFitPolynomialReport, CVI_ContourFitPolynomialReport>(report, true);
        }
        #endregion

        #region Clamp functions
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax(VisionImage image, Roi roi)
        {
            return ClampMax(image, roi, new CurveOptions(false, ExtractionMode.NormalImage, EdgeFilterSize.Fine, false, 75, 10, 10, 5, 25));
        }
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>        
        /// <param name="curveParameters">
        /// Specifies what parameters will be used to extract curves from the inspection image.
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax(VisionImage image, Roi roi, CurveOptions curveParameters)
        {
            return ClampMax(image, roi, curveParameters, new ClampSettings());
        }
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>        
        /// <param name="curveParameters">
        /// Specifies what parameters will be used to extract curves from the inspection image.
        /// </param>
        /// <param name="clampSettings">
        /// Specifies clamp settings parameters to control the behavior of the clamp..
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax(VisionImage image, Roi roi, CurveOptions curveParameters, ClampSettings clampSettings)
        {
            return ClampMax(image, roi, curveParameters, clampSettings, new ClampOverlaySettings());
        }
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>        
        /// <param name="curveParameters">
        /// Specifies what parameters will be used to extract curves from the inspection image.
        /// </param>
        /// <param name="clampSettings">
        /// Specifies clamp settings parameters to control the behavior of the clamp..
        /// </param>
        /// <param name="clampOverlaySettings">
        /// Specifies the information that is to be overlaid on the result image.
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax(VisionImage image, Roi roi, CurveOptions curveParameters, ClampSettings clampSettings, ClampOverlaySettings clampOverlaySettings)
        {
            return ClampMax(image, roi, curveParameters, clampSettings, clampOverlaySettings, new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>        
        /// <param name="curveParameters">
        /// Specifies what parameters will be used to extract curves from the inspection image.
        /// </param>
        /// <param name="clampSettings">
        /// Specifies clamp settings parameters to control the behavior of the clamp..
        /// </param>
        /// <param name="clampOverlaySettings">
        /// Specifies the information that is to be overlaid on the result image.
        /// </param>
        /// <param name="transform">
        /// The coordinate system to which the ROI Descriptor is linked.
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax(VisionImage image, Roi roi, CurveOptions curveParameters, ClampSettings clampSettings, ClampOverlaySettings clampOverlaySettings, CoordinateTransform transform)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (transform == null) { throw new ArgumentNullException("transform"); }
            if (curveParameters == null) { throw new ArgumentNullException("curveParameters"); }
            if (clampSettings == null) { throw new ArgumentNullException("clampSettings"); }
            if (clampOverlaySettings == null) { throw new ArgumentNullException("clampOverlaySettings"); }
            CVI_CoordinateSystem baseSystem = new CVI_CoordinateSystem();
            baseSystem.ConvertFromExternal(transform.ReferenceSystem);
            CVI_CoordinateSystem newSystem = new CVI_CoordinateSystem();
            newSystem.ConvertFromExternal(transform.MeasurementSystem);
            CVI_CurveOptions cviCurveParameters = new CVI_CurveOptions();
            cviCurveParameters.ConvertFromExternal(curveParameters);
            CVI_ClampSettings cviClampSettings = new CVI_ClampSettings();
            cviClampSettings.ConvertFromExternal(clampSettings);
            CVI_ClampOverlaySettings cviClampOverlaySettings = new CVI_ClampOverlaySettings();
            cviClampOverlaySettings.ConvertFromExternal(clampOverlaySettings);

            IntPtr report = VisionDll.imaqClampMax2(image._image, roi._roi, ref baseSystem, ref newSystem, ref cviCurveParameters, ref cviClampSettings, ref cviClampOverlaySettings);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ClampMaxReport, CVI_ClampMaxReport>(report, true);
        }

        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax2(VisionImage image, Roi roi)
        {
            return ClampMax2(image, roi, new CurveOptions(false, ExtractionMode.NormalImage, EdgeFilterSize.Fine, false, 75, 10, 10, 5, 25));
        }
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>        
        /// <param name="curveParameters">
        /// Specifies what parameters will be used to extract curves from the inspection image.
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax2(VisionImage image, Roi roi, CurveOptions curveParameters)
        {
            return ClampMax2(image, roi, curveParameters, new ClampSettings());
        }
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>        
        /// <param name="curveParameters">
        /// Specifies what parameters will be used to extract curves from the inspection image.
        /// </param>
        /// <param name="clampSettings">
        /// Specifies clamp settings parameters to control the behavior of the clamp..
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax2(VisionImage image, Roi roi, CurveOptions curveParameters, ClampSettings clampSettings)
        {
            return ClampMax2(image, roi, curveParameters, clampSettings, new ClampOverlaySettings());
        }
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>        
        /// <param name="curveParameters">
        /// Specifies what parameters will be used to extract curves from the inspection image.
        /// </param>
        /// <param name="clampSettings">
        /// Specifies clamp settings parameters to control the behavior of the clamp..
        /// </param>
        /// <param name="clampOverlaySettings">
        /// Specifies the information that is to be overlaid on the result image.
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax2(VisionImage image, Roi roi, CurveOptions curveParameters, ClampSettings clampSettings, ClampOverlaySettings clampOverlaySettings)
        {
            return ClampMax2(image, roi, curveParameters, clampSettings, clampOverlaySettings, new CoordinateTransform());
        }
        //==========================================================================================
        /// <summary>
        /// ClampMax Measures a distance from outside-in by a simulating a clamping action. The clamping direction is governed by the ROI direction and can vary within a user-defined angle tolerance. 
        /// When valid calibration is present, the distance is computed in pixel as well as real-world units.
        /// </summary>
        /// <param name="image">
        /// The input image containing a contour.
        /// </param>
        /// <param name="roi">
        /// It specifies roi to find clampMax.
        /// </param>        
        /// <param name="curveParameters">
        /// Specifies what parameters will be used to extract curves from the inspection image.
        /// </param>
        /// <param name="clampSettings">
        /// Specifies clamp settings parameters to control the behavior of the clamp..
        /// </param>
        /// <param name="clampOverlaySettings">
        /// Specifies the information that is to be overlaid on the result image.
        /// </param>
        /// <param name="transform">
        /// The coordinate system to which the ROI Descriptor is linked.
        /// </param>
        /// <returns>
        /// A ClampMaxReport containing the information about the Max clamp.
        /// </returns>

        public static ClampMaxReport ClampMax2(VisionImage image, Roi roi, CurveOptions curveParameters, ClampSettings clampSettings, ClampOverlaySettings clampOverlaySettings, CoordinateTransform transform)
        {
            if (image == null) { throw new ArgumentNullException("image"); }
            image.ThrowIfDisposed();
            if (roi == null) { throw new ArgumentNullException("roi"); }
            roi.ThrowIfDisposed();
            if (transform == null) { throw new ArgumentNullException("transform"); }
            if (curveParameters == null) { throw new ArgumentNullException("curveParameters"); }
            if (clampSettings == null) { throw new ArgumentNullException("clampSettings"); }
            if (clampOverlaySettings == null) { throw new ArgumentNullException("clampOverlaySettings"); }
            CVI_CoordinateSystem baseSystem = new CVI_CoordinateSystem();
            baseSystem.ConvertFromExternal(transform.ReferenceSystem);
            CVI_CoordinateSystem newSystem = new CVI_CoordinateSystem();
            newSystem.ConvertFromExternal(transform.MeasurementSystem);
            CVI_CurveOptions cviCurveParameters = new CVI_CurveOptions();
            cviCurveParameters.ConvertFromExternal(curveParameters);
            CVI_ClampSettings cviClampSettings = new CVI_ClampSettings();
            cviClampSettings.ConvertFromExternal(clampSettings);
            CVI_ClampOverlaySettings cviClampOverlaySettings = new CVI_ClampOverlaySettings();
            cviClampOverlaySettings.ConvertFromExternal(clampOverlaySettings);

            IntPtr report = VisionDll.imaqClampMax3(image._image, roi._roi, ref baseSystem, ref newSystem, ref cviCurveParameters, ref cviClampSettings, ref cviClampOverlaySettings);
            Utilities.ThrowError(report);
            return Utilities.ConvertIntPtrToStructure<ClampMaxReport, CVI_ClampMaxReport>(report, true);
        }
        #endregion
}
}
